{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdb2bb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch,copy,argparse,csv\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9ee220",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09df5500",
   "metadata": {},
   "outputs": [],
   "source": [
    "velocity_coeff = np.genfromtxt('./data/POD_coeffs_3900_new_grid_221_42_Velocity_1.csv', delimiter=',')\n",
    "pressure_coeff = np.genfromtxt('./data/POD_coeffs_3900_new_grid_221_42_Pressure_1.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f442139",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_all = torch.from_numpy(np.append(velocity_coeff,pressure_coeff,axis=0)).type(torch.FloatTensor)\n",
    "data = copy.deepcopy(coeff_all[:,:-1].T)\n",
    "label = copy.deepcopy(coeff_all[:,1:].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1e2f6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data,label, test_size=0.3 , random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb3d013",
   "metadata": {},
   "source": [
    "## Compute residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dde356cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "basis_v = torch.from_numpy(np.genfromtxt('./data/Velocity_basis.csv', delimiter=',')).type(torch.FloatTensor)\n",
    "basis_p = torch.from_numpy(np.genfromtxt('./data/Pressure_basis.csv', delimiter=',')).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdfdb2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_p(coeff_p, basis_p):\n",
    "    length, number = coeff_p.shape\n",
    "    nx = 221\n",
    "    ny = 42\n",
    "    grid_p = torch.matmul(basis_p, coeff_p)\n",
    "    return grid_p.reshape(nx,ny,number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e47773e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_v(coeff_v, basis_v):\n",
    "    length, number = coeff_v.shape\n",
    "    nx = 221\n",
    "    ny = 42\n",
    "    grid_v = torch.matmul(basis_v, coeff_v)\n",
    "    grid_v1 = grid_v[0:nx*ny].reshape(nx,ny,number)\n",
    "    grid_v2 = grid_v[nx*ny:].reshape(nx,ny,number)\n",
    "    return grid_v1, grid_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52f9839c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(coeff, basis_v, basis_p):\n",
    "    nPOD = 10\n",
    "    coeff_v = coeff[:nPOD,:]\n",
    "    coeff_p = coeff[nPOD:,:]\n",
    "    v1, v2 = backward_v(coeff_v, basis_v)\n",
    "    p = backward_p(coeff_p, basis_p)\n",
    "    return v1, v2, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fc63b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_resids(coeff, coeff_pred, basis_v, basis_p):\n",
    "    u1_o1d, u2_o1d, p = backward(coeff, basis_v, basis_p)\n",
    "    u1, u2, p = backward(coeff_pred, basis_v, basis_p)\n",
    "    nx, ny = 221,42\n",
    "    dx, dy = 0.01, 0.01\n",
    "    dt = 1.\n",
    "    sigma = 0.\n",
    "    rho = 1.\n",
    "    mu = 1./300\n",
    "    # first order term of u1\n",
    "    u1x = (u1[2:,1:-1] - u1[:-2,1:-1])/(2.0*dx)\n",
    "    u1y = (u1[1:-1,2:] - u1[1:-1,:-2])/(2.0*dy)\n",
    "#     u1t = (u1_new[1:-1, 1:-1] - u1 [1:-1, 1:-1] )/dt \n",
    "    u1t = (u1[1:-1, 1:-1] - u1_o1d[1:-1, 1:-1])/dt \n",
    "    # second order term of u1 \n",
    "    u1xx = (u1[2:,1:-1] - 2. *u1[1:-1, 1:-1] + u1[0: -2, 1:-1])/(dx**2) \n",
    "    u1yy = (u1[1:-1,2:] - 2. *u1[1:-1, 1:-1] + u1[1:-1, 0:-2])/(dy**2) \n",
    "    # first order term of u2\n",
    "    u2x = (u2[2:,1:-1] - u2[0:-2, 1:-1])/(2.0*dx) \n",
    "    u2y = (u2[1:-1, 2: ] - u2 [1:-1, 0:-2])/(2.0*dy) \n",
    "#     u2t = (u2_new[1:-1, 1:-1] - u2 [1:-1, 1:-1] )/dt \n",
    "    u2t = (u2[1:-1, 1:-1] - u2_o1d[1:-1, 1:-1] )/dt \n",
    "    # second order term of u2 \n",
    "    u2xx = (u2[2:,1:-1] - 2.*u2[1:-1, 1:-1] + u2[0:-2, 1:-1])/(dx**2) \n",
    "    u2yy = (u2[1:-1,2:] - 2.*u2[1:-1, 1:-1] + u2[1:-1, 0:-2])/(dy**2) \n",
    "    # first order of p \n",
    "    px = (p[2:,1:-1] - p[0:-2,1:-1])/(2.* dx)\n",
    "    py = (p[1:-1,2:] - p[1:-1,0:-2])/(2.* dy)\n",
    "    r_cty = u1x + u2y \n",
    "    r_u1 = rho*u1t + sigma*u1 [1:-1, 1:-1]+ rho* (u1 [1:-1, 1:-1] *u1x+u2 [1:-1, 1:-1] *u1y) - mu* (u1xx+u1yy) + px\n",
    "    r_u2 = rho*u2t + sigma*u2 [1: -1, 1: -1]+ rho* (u1 [1:-1, 1:-1] *u2x+u2 [1:-1, 1:-1] *u2y) - mu* (u2xx+u2yy) + py\n",
    "    return r_cty, r_u1, r_u2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20171daf",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b95939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ========================模型验证======================\n",
    "def valid(model,data,criterion):\n",
    "    model.eval()\n",
    "    z = torch.from_numpy(np.random.randn(data.shape[0], 10)).float() # 随机噪声\n",
    "    with torch.no_grad():   # 不记录梯度信息\n",
    "        y_pred = model(torch.cat([z, data], dim=1))\n",
    "        loss = criterion(y_pred, y_test)\n",
    "        loss_val.append(loss.item())\n",
    "        print('val MSE loss:',loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1d59bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 网络G，用于预测下一时刻的特征向量\n",
    "G = torch.nn.Sequential(\n",
    "    torch.nn.Linear(20+10,30),   # 特征向量+噪声向量\n",
    "    torch.nn.ReLU(),   \n",
    "    torch.nn.Linear(30,15), \n",
    "    torch.nn.ReLU(),   \n",
    "    torch.nn.Linear(15,20),   \n",
    ")\n",
    "# 网络D，用于一组特征向量是网络G制造的还是真实标签，促使G的输出接近于真实标签\n",
    "D = torch.nn.Sequential(\n",
    "    torch.nn.Linear(20,10),   \n",
    "    torch.nn.ReLU(),   \n",
    "    torch.nn.Linear(10,5), \n",
    "    torch.nn.ReLU(),   \n",
    "    torch.nn.Linear(5,1),\n",
    "    nn.Sigmoid()   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dae7ea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func_bce = nn.BCELoss()    # 用于鉴别器模型D（对抗损失），D的任务判断是否为标签，属于二分类任务，因此用交叉熵损失\n",
    "loss_func_reg = nn.MSELoss()    # 用于生成器G，计算模型预测和标签的差异，因为是回归任务所以用均方根损失\n",
    "opt_g = torch.optim.Adam(G.parameters(), lr=3e-4)   \n",
    "opt_d = torch.optim.Adam(D.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9073166",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = []\n",
    "loss_val = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa147d7f",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c86370b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = Data.TensorDataset(x_train , y_train)\n",
    "batch_size = 60\n",
    "loader = Data.DataLoader(\n",
    "    dataset = dataset_train,\n",
    "    batch_size = batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers = 8\n",
    ")\n",
    "epochs = 10\n",
    "lambda_res = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47f16616",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "val MSE loss: 53.654083251953125\n",
      "val MSE loss: 53.57154083251953\n",
      "val MSE loss: 53.51240539550781\n",
      "train D loss: 1.4909698963165283 G loss: 53.200050354003906\n",
      "val MSE loss: 53.428165435791016\n",
      "val MSE loss: 53.369693756103516\n",
      "val MSE loss: 53.302188873291016\n",
      "train D loss: 1.5002529621124268 G loss: 51.79513931274414\n",
      "val MSE loss: 53.228206634521484\n",
      "val MSE loss: 53.164608001708984\n",
      "val MSE loss: 53.07018280029297\n",
      "train D loss: 1.4844386577606201 G loss: 54.24220657348633\n",
      "val MSE loss: 53.01261520385742\n",
      "val MSE loss: 52.964271545410156\n",
      "val MSE loss: 52.89380645751953\n",
      "train D loss: 1.4847276210784912 G loss: 52.608543395996094\n",
      "val MSE loss: 52.80446243286133\n",
      "val MSE loss: 52.756412506103516\n",
      "val MSE loss: 52.68563461303711\n",
      "train D loss: 1.485443115234375 G loss: 53.53581237792969\n",
      "val MSE loss: 52.63828659057617\n",
      "val MSE loss: 52.5682487487793\n",
      "val MSE loss: 52.49630355834961\n",
      "train D loss: 1.4797816276550293 G loss: 51.806339263916016\n",
      "val MSE loss: 52.44963455200195\n",
      "val MSE loss: 52.38234329223633\n",
      "val MSE loss: 52.33032989501953\n",
      "train D loss: 1.4902559518814087 G loss: 52.460205078125\n",
      "val MSE loss: 52.2445068359375\n",
      "val MSE loss: 52.195980072021484\n",
      "val MSE loss: 52.138771057128906\n",
      "train D loss: 1.4851857423782349 G loss: 52.433204650878906\n",
      "val MSE loss: 52.0804328918457\n",
      "val MSE loss: 52.01905059814453\n",
      "val MSE loss: 51.98408508300781\n",
      "train D loss: 1.4692306518554688 G loss: 51.25968933105469\n",
      "val MSE loss: 51.91875076293945\n",
      "val MSE loss: 51.84532165527344\n",
      "val MSE loss: 51.8073844909668\n",
      "train D loss: 1.4837369918823242 G loss: 51.676116943359375\n",
      "val MSE loss: 51.74325180053711\n",
      "val MSE loss: 51.694156646728516\n",
      "val MSE loss: 51.63361358642578\n",
      "train D loss: 1.4640297889709473 G loss: 51.44108963012695\n",
      "val MSE loss: 51.568931579589844\n",
      "val MSE loss: 51.51165771484375\n",
      "val MSE loss: 51.44976043701172\n",
      "train D loss: 1.4714150428771973 G loss: 51.83620071411133\n",
      "val MSE loss: 51.40670394897461\n",
      "val MSE loss: 51.35187530517578\n",
      "val MSE loss: 51.310951232910156\n",
      "train D loss: 1.459200143814087 G loss: 50.12110900878906\n",
      "val MSE loss: 51.239139556884766\n",
      "val MSE loss: 51.1751823425293\n",
      "val MSE loss: 51.13373947143555\n",
      "train D loss: 1.4611636400222778 G loss: 51.24337387084961\n",
      "val MSE loss: 51.0622444152832\n",
      "val MSE loss: 51.024078369140625\n",
      "val MSE loss: 50.9687385559082\n",
      "train D loss: 1.4568895101547241 G loss: 50.524452209472656\n",
      "val MSE loss: 50.913780212402344\n",
      "val MSE loss: 50.85092544555664\n",
      "val MSE loss: 50.80116653442383\n",
      "train D loss: 1.4637811183929443 G loss: 50.879051208496094\n",
      "val MSE loss: 50.73453140258789\n",
      "val MSE loss: 50.6786003112793\n",
      "val MSE loss: 50.60371780395508\n",
      "train D loss: 1.4688832759857178 G loss: 51.08856964111328\n",
      "val MSE loss: 50.56013488769531\n",
      "val MSE loss: 50.48404312133789\n",
      "val MSE loss: 50.44854736328125\n",
      "train D loss: 1.4599568843841553 G loss: 50.233856201171875\n",
      "val MSE loss: 50.38265609741211\n",
      "val MSE loss: 50.32529067993164\n",
      "val MSE loss: 50.265010833740234\n",
      "train D loss: 1.472442865371704 G loss: 52.144256591796875\n",
      "val MSE loss: 50.19683837890625\n",
      "val MSE loss: 50.129295349121094\n",
      "val MSE loss: 50.07182693481445\n",
      "train D loss: 1.457399606704712 G loss: 50.3238639831543\n",
      "val MSE loss: 50.01150131225586\n",
      "val MSE loss: 49.94036102294922\n",
      "val MSE loss: 49.87862014770508\n",
      "train D loss: 1.4635334014892578 G loss: 51.19229507446289\n",
      "val MSE loss: 49.796531677246094\n",
      "val MSE loss: 49.73072052001953\n",
      "val MSE loss: 49.65818405151367\n",
      "train D loss: 1.459427833557129 G loss: 49.101993560791016\n",
      "val MSE loss: 49.59587860107422\n",
      "val MSE loss: 49.51606369018555\n",
      "val MSE loss: 49.4415397644043\n",
      "train D loss: 1.4583232402801514 G loss: 49.83348846435547\n",
      "val MSE loss: 49.36696243286133\n",
      "val MSE loss: 49.286399841308594\n",
      "val MSE loss: 49.2181510925293\n",
      "train D loss: 1.4583382606506348 G loss: 50.20769119262695\n",
      "epoch: 1\n",
      "val MSE loss: 49.136451721191406\n",
      "val MSE loss: 49.0764045715332\n",
      "val MSE loss: 48.99345779418945\n",
      "train D loss: 1.439974069595337 G loss: 49.62464141845703\n",
      "val MSE loss: 48.90024948120117\n",
      "val MSE loss: 48.817081451416016\n",
      "val MSE loss: 48.7392692565918\n",
      "train D loss: 1.4547293186187744 G loss: 49.0397834777832\n",
      "val MSE loss: 48.66501998901367\n",
      "val MSE loss: 48.56989669799805\n",
      "val MSE loss: 48.51057434082031\n",
      "train D loss: 1.440782904624939 G loss: 48.50054931640625\n",
      "val MSE loss: 48.42534255981445\n",
      "val MSE loss: 48.34917068481445\n",
      "val MSE loss: 48.24553298950195\n",
      "train D loss: 1.4499495029449463 G loss: 47.98406219482422\n",
      "val MSE loss: 48.15810775756836\n",
      "val MSE loss: 48.070350646972656\n",
      "val MSE loss: 47.98934555053711\n",
      "train D loss: 1.4274790287017822 G loss: 47.004825592041016\n",
      "val MSE loss: 47.89767074584961\n",
      "val MSE loss: 47.80305099487305\n",
      "val MSE loss: 47.7101936340332\n",
      "train D loss: 1.4244885444641113 G loss: 48.137271881103516\n",
      "val MSE loss: 47.61332702636719\n",
      "val MSE loss: 47.52432632446289\n",
      "val MSE loss: 47.41759490966797\n",
      "train D loss: 1.4223849773406982 G loss: 46.776954650878906\n",
      "val MSE loss: 47.321739196777344\n",
      "val MSE loss: 47.240020751953125\n",
      "val MSE loss: 47.14141845703125\n",
      "train D loss: 1.4453654289245605 G loss: 46.14265060424805\n",
      "val MSE loss: 47.04766082763672\n",
      "val MSE loss: 46.94689178466797\n",
      "val MSE loss: 46.83376693725586\n",
      "train D loss: 1.4378361701965332 G loss: 46.59161376953125\n",
      "val MSE loss: 46.74706268310547\n",
      "val MSE loss: 46.63404083251953\n",
      "val MSE loss: 46.524574279785156\n",
      "train D loss: 1.4313864707946777 G loss: 46.35200500488281\n",
      "val MSE loss: 46.42833709716797\n",
      "val MSE loss: 46.346168518066406\n",
      "val MSE loss: 46.2197380065918\n",
      "train D loss: 1.442233681678772 G loss: 47.649234771728516\n",
      "val MSE loss: 46.099552154541016\n",
      "val MSE loss: 45.9908332824707\n",
      "val MSE loss: 45.88720703125\n",
      "train D loss: 1.4319475889205933 G loss: 44.759647369384766\n",
      "val MSE loss: 45.773895263671875\n",
      "val MSE loss: 45.6456298828125\n",
      "val MSE loss: 45.56592559814453\n",
      "train D loss: 1.4222698211669922 G loss: 45.29756546020508\n",
      "val MSE loss: 45.42825698852539\n",
      "val MSE loss: 45.327293395996094\n",
      "val MSE loss: 45.198123931884766\n",
      "train D loss: 1.4275875091552734 G loss: 45.10703659057617\n",
      "val MSE loss: 45.07807922363281\n",
      "val MSE loss: 44.975250244140625\n",
      "val MSE loss: 44.84999084472656\n",
      "train D loss: 1.4245202541351318 G loss: 46.16413879394531\n",
      "val MSE loss: 44.70674514770508\n",
      "val MSE loss: 44.5893440246582\n",
      "val MSE loss: 44.46994400024414\n",
      "train D loss: 1.4087059497833252 G loss: 45.38209533691406\n",
      "val MSE loss: 44.34703826904297\n",
      "val MSE loss: 44.233524322509766\n",
      "val MSE loss: 44.09191131591797\n",
      "train D loss: 1.416735291481018 G loss: 44.698123931884766\n",
      "val MSE loss: 43.954063415527344\n",
      "val MSE loss: 43.81207275390625\n",
      "val MSE loss: 43.69285583496094\n",
      "train D loss: 1.413647174835205 G loss: 44.75166320800781\n",
      "val MSE loss: 43.55476379394531\n",
      "val MSE loss: 43.432498931884766\n",
      "val MSE loss: 43.2881965637207\n",
      "train D loss: 1.4215943813323975 G loss: 44.65768814086914\n",
      "val MSE loss: 43.16679763793945\n",
      "val MSE loss: 42.989097595214844\n",
      "val MSE loss: 42.86834716796875\n",
      "train D loss: 1.4207046031951904 G loss: 43.06052017211914\n",
      "val MSE loss: 42.7192497253418\n",
      "val MSE loss: 42.577415466308594\n",
      "val MSE loss: 42.42400360107422\n",
      "train D loss: 1.4114344120025635 G loss: 42.37990951538086\n",
      "val MSE loss: 42.26817321777344\n",
      "val MSE loss: 42.1225471496582\n",
      "val MSE loss: 41.97700119018555\n",
      "train D loss: 1.4232752323150635 G loss: 42.62089157104492\n",
      "val MSE loss: 41.80051040649414\n",
      "val MSE loss: 41.65658187866211\n",
      "val MSE loss: 41.50633239746094\n",
      "train D loss: 1.4133069515228271 G loss: 42.227569580078125\n",
      "val MSE loss: 41.34733200073242\n",
      "val MSE loss: 41.200477600097656\n",
      "val MSE loss: 41.01882553100586\n",
      "train D loss: 1.4150822162628174 G loss: 39.336639404296875\n",
      "epoch: 2\n",
      "val MSE loss: 40.87016296386719\n",
      "val MSE loss: 40.705596923828125\n",
      "val MSE loss: 40.539310455322266\n",
      "train D loss: 1.4293110370635986 G loss: 42.19919967651367\n",
      "val MSE loss: 40.379615783691406\n",
      "val MSE loss: 40.20220947265625\n",
      "val MSE loss: 40.044334411621094\n",
      "train D loss: 1.3946499824523926 G loss: 40.55897903442383\n",
      "val MSE loss: 39.88778305053711\n",
      "val MSE loss: 39.695823669433594\n",
      "val MSE loss: 39.53704833984375\n",
      "train D loss: 1.3941428661346436 G loss: 39.92601776123047\n",
      "val MSE loss: 39.374366760253906\n",
      "val MSE loss: 39.191322326660156\n",
      "val MSE loss: 39.01183319091797\n",
      "train D loss: 1.3958075046539307 G loss: 39.70906066894531\n",
      "val MSE loss: 38.842979431152344\n",
      "val MSE loss: 38.663795471191406\n",
      "val MSE loss: 38.506683349609375\n",
      "train D loss: 1.3983244895935059 G loss: 39.51001739501953\n",
      "val MSE loss: 38.33183670043945\n",
      "val MSE loss: 38.1413688659668\n",
      "val MSE loss: 38.000755310058594\n",
      "train D loss: 1.4021058082580566 G loss: 37.8453369140625\n",
      "val MSE loss: 37.78675079345703\n",
      "val MSE loss: 37.62507629394531\n",
      "val MSE loss: 37.456443786621094\n",
      "train D loss: 1.3805968761444092 G loss: 37.760196685791016\n",
      "val MSE loss: 37.27048873901367\n",
      "val MSE loss: 37.08911895751953\n",
      "val MSE loss: 36.8897590637207\n",
      "train D loss: 1.3916268348693848 G loss: 36.870609283447266\n",
      "val MSE loss: 36.72193145751953\n",
      "val MSE loss: 36.54829788208008\n",
      "val MSE loss: 36.33169937133789\n",
      "train D loss: 1.3986940383911133 G loss: 35.9881477355957\n",
      "val MSE loss: 36.13494110107422\n",
      "val MSE loss: 35.97513198852539\n",
      "val MSE loss: 35.7801628112793\n",
      "train D loss: 1.3790693283081055 G loss: 34.864356994628906\n",
      "val MSE loss: 35.575347900390625\n",
      "val MSE loss: 35.3945198059082\n",
      "val MSE loss: 35.22412872314453\n",
      "train D loss: 1.373335361480713 G loss: 35.602943420410156\n",
      "val MSE loss: 35.05049133300781\n",
      "val MSE loss: 34.841522216796875\n",
      "val MSE loss: 34.6348876953125\n",
      "train D loss: 1.3980647325515747 G loss: 36.38736343383789\n",
      "val MSE loss: 34.48052215576172\n",
      "val MSE loss: 34.22502136230469\n",
      "val MSE loss: 34.08070373535156\n",
      "train D loss: 1.3660081624984741 G loss: 35.265289306640625\n",
      "val MSE loss: 33.87639617919922\n",
      "val MSE loss: 33.681800842285156\n",
      "val MSE loss: 33.51024627685547\n",
      "train D loss: 1.3808172941207886 G loss: 33.241214752197266\n",
      "val MSE loss: 33.28680419921875\n",
      "val MSE loss: 33.097965240478516\n",
      "val MSE loss: 32.90742492675781\n",
      "train D loss: 1.3539592027664185 G loss: 32.98139953613281\n",
      "val MSE loss: 32.7352294921875\n",
      "val MSE loss: 32.47443771362305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val MSE loss: 32.31267547607422\n",
      "train D loss: 1.3559777736663818 G loss: 32.57779312133789\n",
      "val MSE loss: 32.12884521484375\n",
      "val MSE loss: 31.927322387695312\n",
      "val MSE loss: 31.702661514282227\n",
      "train D loss: 1.3550890684127808 G loss: 31.486656188964844\n",
      "val MSE loss: 31.51173973083496\n",
      "val MSE loss: 31.2888126373291\n",
      "val MSE loss: 31.12147331237793\n",
      "train D loss: 1.357337474822998 G loss: 31.388578414916992\n",
      "val MSE loss: 30.8830623626709\n",
      "val MSE loss: 30.710651397705078\n",
      "val MSE loss: 30.48534393310547\n",
      "train D loss: 1.3637641668319702 G loss: 30.64800453186035\n",
      "val MSE loss: 30.288301467895508\n",
      "val MSE loss: 30.10608673095703\n",
      "val MSE loss: 29.88202667236328\n",
      "train D loss: 1.3669028282165527 G loss: 30.06639862060547\n",
      "val MSE loss: 29.682249069213867\n",
      "val MSE loss: 29.499835968017578\n",
      "val MSE loss: 29.265132904052734\n",
      "train D loss: 1.3467528820037842 G loss: 29.58690643310547\n",
      "val MSE loss: 29.07577896118164\n",
      "val MSE loss: 28.84905433654785\n",
      "val MSE loss: 28.659170150756836\n",
      "train D loss: 1.3550591468811035 G loss: 27.91069221496582\n",
      "val MSE loss: 28.445343017578125\n",
      "val MSE loss: 28.225353240966797\n",
      "val MSE loss: 27.99473762512207\n",
      "train D loss: 1.3560810089111328 G loss: 28.886516571044922\n",
      "val MSE loss: 27.784915924072266\n",
      "val MSE loss: 27.590412139892578\n",
      "val MSE loss: 27.385026931762695\n",
      "train D loss: 1.3498477935791016 G loss: 27.65484046936035\n",
      "epoch: 3\n",
      "val MSE loss: 27.172197341918945\n",
      "val MSE loss: 27.013628005981445\n",
      "val MSE loss: 26.76149559020996\n",
      "train D loss: 1.3666728734970093 G loss: 26.765111923217773\n",
      "val MSE loss: 26.537864685058594\n",
      "val MSE loss: 26.3354434967041\n",
      "val MSE loss: 26.124109268188477\n",
      "train D loss: 1.350721001625061 G loss: 27.02979278564453\n",
      "val MSE loss: 25.92772674560547\n",
      "val MSE loss: 25.689401626586914\n",
      "val MSE loss: 25.501934051513672\n",
      "train D loss: 1.3482463359832764 G loss: 25.52846336364746\n",
      "val MSE loss: 25.300987243652344\n",
      "val MSE loss: 25.086877822875977\n",
      "val MSE loss: 24.89624786376953\n",
      "train D loss: 1.3343095779418945 G loss: 25.01344108581543\n",
      "val MSE loss: 24.688377380371094\n",
      "val MSE loss: 24.500185012817383\n",
      "val MSE loss: 24.280094146728516\n",
      "train D loss: 1.3303864002227783 G loss: 24.487659454345703\n",
      "val MSE loss: 24.073183059692383\n",
      "val MSE loss: 23.860998153686523\n",
      "val MSE loss: 23.696447372436523\n",
      "train D loss: 1.3286992311477661 G loss: 24.13897705078125\n",
      "val MSE loss: 23.450176239013672\n",
      "val MSE loss: 23.271629333496094\n",
      "val MSE loss: 23.063262939453125\n",
      "train D loss: 1.3406195640563965 G loss: 23.347484588623047\n",
      "val MSE loss: 22.864755630493164\n",
      "val MSE loss: 22.646949768066406\n",
      "val MSE loss: 22.456287384033203\n",
      "train D loss: 1.3378441333770752 G loss: 22.81913948059082\n",
      "val MSE loss: 22.247827529907227\n",
      "val MSE loss: 22.04557228088379\n",
      "val MSE loss: 21.8721923828125\n",
      "train D loss: 1.3366326093673706 G loss: 22.71941375732422\n",
      "val MSE loss: 21.65358543395996\n",
      "val MSE loss: 21.44507598876953\n",
      "val MSE loss: 21.285593032836914\n",
      "train D loss: 1.333911657333374 G loss: 22.28047752380371\n",
      "val MSE loss: 21.100961685180664\n",
      "val MSE loss: 20.911365509033203\n",
      "val MSE loss: 20.711679458618164\n",
      "train D loss: 1.328876256942749 G loss: 20.704030990600586\n",
      "val MSE loss: 20.527565002441406\n",
      "val MSE loss: 20.34128189086914\n",
      "val MSE loss: 20.196796417236328\n",
      "train D loss: 1.3178911209106445 G loss: 20.608430862426758\n",
      "val MSE loss: 20.005939483642578\n",
      "val MSE loss: 19.783960342407227\n",
      "val MSE loss: 19.615503311157227\n",
      "train D loss: 1.3261653184890747 G loss: 20.268381118774414\n",
      "val MSE loss: 19.468276977539062\n",
      "val MSE loss: 19.245697021484375\n",
      "val MSE loss: 19.098569869995117\n",
      "train D loss: 1.3263094425201416 G loss: 19.458215713500977\n",
      "val MSE loss: 18.946298599243164\n",
      "val MSE loss: 18.747896194458008\n",
      "val MSE loss: 18.598724365234375\n",
      "train D loss: 1.315353512763977 G loss: 18.644947052001953\n",
      "val MSE loss: 18.41206169128418\n",
      "val MSE loss: 18.22768211364746\n",
      "val MSE loss: 18.080846786499023\n",
      "train D loss: 1.3225942850112915 G loss: 17.682157516479492\n",
      "val MSE loss: 17.9122371673584\n",
      "val MSE loss: 17.774255752563477\n",
      "val MSE loss: 17.585927963256836\n",
      "train D loss: 1.3294918537139893 G loss: 17.149770736694336\n",
      "val MSE loss: 17.460981369018555\n",
      "val MSE loss: 17.313798904418945\n",
      "val MSE loss: 17.141517639160156\n",
      "train D loss: 1.3223786354064941 G loss: 17.607994079589844\n",
      "val MSE loss: 17.00271224975586\n",
      "val MSE loss: 16.831552505493164\n",
      "val MSE loss: 16.68159294128418\n",
      "train D loss: 1.3181474208831787 G loss: 17.17849349975586\n",
      "val MSE loss: 16.54481315612793\n",
      "val MSE loss: 16.402427673339844\n",
      "val MSE loss: 16.227203369140625\n",
      "train D loss: 1.3105096817016602 G loss: 16.985219955444336\n",
      "val MSE loss: 16.087677001953125\n",
      "val MSE loss: 15.979802131652832\n",
      "val MSE loss: 15.830598831176758\n",
      "train D loss: 1.311721682548523 G loss: 16.123350143432617\n",
      "val MSE loss: 15.701167106628418\n",
      "val MSE loss: 15.58586311340332\n",
      "val MSE loss: 15.410968780517578\n",
      "train D loss: 1.3149676322937012 G loss: 15.04980754852295\n",
      "val MSE loss: 15.321439743041992\n",
      "val MSE loss: 15.186430931091309\n",
      "val MSE loss: 15.073955535888672\n",
      "train D loss: 1.3147459030151367 G loss: 15.024419784545898\n",
      "val MSE loss: 14.951399803161621\n",
      "val MSE loss: 14.833028793334961\n",
      "val MSE loss: 14.70612907409668\n",
      "train D loss: 1.3319218158721924 G loss: 14.018416404724121\n",
      "epoch: 4\n",
      "val MSE loss: 14.60976791381836\n",
      "val MSE loss: 14.464847564697266\n",
      "val MSE loss: 14.379015922546387\n",
      "train D loss: 1.2971570491790771 G loss: 14.727431297302246\n",
      "val MSE loss: 14.268147468566895\n",
      "val MSE loss: 14.157247543334961\n",
      "val MSE loss: 14.030497550964355\n",
      "train D loss: 1.3070193529129028 G loss: 14.146563529968262\n",
      "val MSE loss: 13.956192970275879\n",
      "val MSE loss: 13.827648162841797\n",
      "val MSE loss: 13.760061264038086\n",
      "train D loss: 1.2994718551635742 G loss: 13.85708236694336\n",
      "val MSE loss: 13.640870094299316\n",
      "val MSE loss: 13.558492660522461\n",
      "val MSE loss: 13.475382804870605\n",
      "train D loss: 1.2938477993011475 G loss: 14.012480735778809\n",
      "val MSE loss: 13.401520729064941\n",
      "val MSE loss: 13.311944961547852\n",
      "val MSE loss: 13.214021682739258\n",
      "train D loss: 1.3251585960388184 G loss: 12.536214828491211\n",
      "val MSE loss: 13.143682479858398\n",
      "val MSE loss: 13.04149341583252\n",
      "val MSE loss: 12.9898042678833\n",
      "train D loss: 1.2938878536224365 G loss: 13.302305221557617\n",
      "val MSE loss: 12.904211044311523\n",
      "val MSE loss: 12.812345504760742\n",
      "val MSE loss: 12.757674217224121\n",
      "train D loss: 1.2980139255523682 G loss: 13.048027992248535\n",
      "val MSE loss: 12.704440116882324\n",
      "val MSE loss: 12.592523574829102\n",
      "val MSE loss: 12.5322265625\n",
      "train D loss: 1.28729248046875 G loss: 12.651304244995117\n",
      "val MSE loss: 12.469695091247559\n",
      "val MSE loss: 12.401886940002441\n",
      "val MSE loss: 12.343822479248047\n",
      "train D loss: 1.2863327264785767 G loss: 13.266806602478027\n",
      "val MSE loss: 12.28221321105957\n",
      "val MSE loss: 12.186219215393066\n",
      "val MSE loss: 12.142224311828613\n",
      "train D loss: 1.2951555252075195 G loss: 12.21065616607666\n",
      "val MSE loss: 12.085851669311523\n",
      "val MSE loss: 12.015140533447266\n",
      "val MSE loss: 11.968012809753418\n",
      "train D loss: 1.3009138107299805 G loss: 12.174954414367676\n",
      "val MSE loss: 11.902237892150879\n",
      "val MSE loss: 11.843957901000977\n",
      "val MSE loss: 11.786453247070312\n",
      "train D loss: 1.2784990072250366 G loss: 13.382390975952148\n",
      "val MSE loss: 11.745210647583008\n",
      "val MSE loss: 11.682463645935059\n",
      "val MSE loss: 11.634961128234863\n",
      "train D loss: 1.2947449684143066 G loss: 12.18552017211914\n",
      "val MSE loss: 11.58836555480957\n",
      "val MSE loss: 11.545187950134277\n",
      "val MSE loss: 11.494842529296875\n",
      "train D loss: 1.2833901643753052 G loss: 11.90601634979248\n",
      "val MSE loss: 11.453659057617188\n",
      "val MSE loss: 11.41581916809082\n",
      "val MSE loss: 11.3711576461792\n",
      "train D loss: 1.2985411882400513 G loss: 11.33616828918457\n",
      "val MSE loss: 11.324920654296875\n",
      "val MSE loss: 11.269413948059082\n",
      "val MSE loss: 11.241669654846191\n",
      "train D loss: 1.2788171768188477 G loss: 11.313843727111816\n",
      "val MSE loss: 11.2054443359375\n",
      "val MSE loss: 11.167770385742188\n",
      "val MSE loss: 11.131621360778809\n",
      "train D loss: 1.297947645187378 G loss: 11.223041534423828\n",
      "val MSE loss: 11.0904541015625\n",
      "val MSE loss: 11.055207252502441\n",
      "val MSE loss: 11.030778884887695\n",
      "train D loss: 1.2786586284637451 G loss: 11.414959907531738\n",
      "val MSE loss: 10.992352485656738\n",
      "val MSE loss: 10.960587501525879\n",
      "val MSE loss: 10.93813419342041\n",
      "train D loss: 1.2915606498718262 G loss: 10.524908065795898\n",
      "val MSE loss: 10.90392780303955\n",
      "val MSE loss: 10.854541778564453\n",
      "val MSE loss: 10.834211349487305\n",
      "train D loss: 1.2811601161956787 G loss: 10.51231861114502\n",
      "val MSE loss: 10.815265655517578\n",
      "val MSE loss: 10.788217544555664\n",
      "val MSE loss: 10.773571968078613\n",
      "train D loss: 1.275416374206543 G loss: 11.214133262634277\n",
      "val MSE loss: 10.741656303405762\n",
      "val MSE loss: 10.720516204833984\n",
      "val MSE loss: 10.695326805114746\n",
      "train D loss: 1.264566421508789 G loss: 11.677596092224121\n",
      "val MSE loss: 10.663999557495117\n",
      "val MSE loss: 10.643446922302246\n",
      "val MSE loss: 10.617669105529785\n",
      "train D loss: 1.2717676162719727 G loss: 11.042157173156738\n",
      "val MSE loss: 10.595666885375977\n",
      "val MSE loss: 10.583571434020996\n",
      "val MSE loss: 10.555828094482422\n",
      "train D loss: 1.280907392501831 G loss: 10.19198989868164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5\n",
      "val MSE loss: 10.533575057983398\n",
      "val MSE loss: 10.508079528808594\n",
      "val MSE loss: 10.49455451965332\n",
      "train D loss: 1.295846939086914 G loss: 10.024094581604004\n",
      "val MSE loss: 10.467748641967773\n",
      "val MSE loss: 10.43653392791748\n",
      "val MSE loss: 10.406771659851074\n",
      "train D loss: 1.2795835733413696 G loss: 11.127325057983398\n",
      "val MSE loss: 10.390205383300781\n",
      "val MSE loss: 10.36113452911377\n",
      "val MSE loss: 10.330745697021484\n",
      "train D loss: 1.289215087890625 G loss: 9.894440650939941\n",
      "val MSE loss: 10.301661491394043\n",
      "val MSE loss: 10.279356956481934\n",
      "val MSE loss: 10.251638412475586\n",
      "train D loss: 1.274321436882019 G loss: 10.237245559692383\n",
      "val MSE loss: 10.23095417022705\n",
      "val MSE loss: 10.198833465576172\n",
      "val MSE loss: 10.181803703308105\n",
      "train D loss: 1.288909673690796 G loss: 10.057465553283691\n",
      "val MSE loss: 10.156983375549316\n",
      "val MSE loss: 10.139310836791992\n",
      "val MSE loss: 10.1161470413208\n",
      "train D loss: 1.28165602684021 G loss: 10.148804664611816\n",
      "val MSE loss: 10.090283393859863\n",
      "val MSE loss: 10.070700645446777\n",
      "val MSE loss: 10.058436393737793\n",
      "train D loss: 1.2627028226852417 G loss: 10.693232536315918\n",
      "val MSE loss: 10.033059120178223\n",
      "val MSE loss: 10.016999244689941\n",
      "val MSE loss: 9.985223770141602\n",
      "train D loss: 1.287126898765564 G loss: 9.601519584655762\n",
      "val MSE loss: 9.973943710327148\n",
      "val MSE loss: 9.952615737915039\n",
      "val MSE loss: 9.924795150756836\n",
      "train D loss: 1.2643001079559326 G loss: 10.284798622131348\n",
      "val MSE loss: 9.898934364318848\n",
      "val MSE loss: 9.880931854248047\n",
      "val MSE loss: 9.852995872497559\n",
      "train D loss: 1.278548240661621 G loss: 10.230414390563965\n",
      "val MSE loss: 9.83859920501709\n",
      "val MSE loss: 9.805632591247559\n",
      "val MSE loss: 9.782085418701172\n",
      "train D loss: 1.2772839069366455 G loss: 10.192558288574219\n",
      "val MSE loss: 9.762428283691406\n",
      "val MSE loss: 9.740065574645996\n",
      "val MSE loss: 9.713724136352539\n",
      "train D loss: 1.2769081592559814 G loss: 9.668259620666504\n",
      "val MSE loss: 9.691754341125488\n",
      "val MSE loss: 9.674954414367676\n",
      "val MSE loss: 9.657752990722656\n",
      "train D loss: 1.2752771377563477 G loss: 9.662914276123047\n",
      "val MSE loss: 9.632943153381348\n",
      "val MSE loss: 9.615023612976074\n",
      "val MSE loss: 9.592583656311035\n",
      "train D loss: 1.2821855545043945 G loss: 9.4303560256958\n",
      "val MSE loss: 9.57373332977295\n",
      "val MSE loss: 9.546833992004395\n",
      "val MSE loss: 9.544196128845215\n",
      "train D loss: 1.2615468502044678 G loss: 10.373601913452148\n",
      "val MSE loss: 9.511104583740234\n",
      "val MSE loss: 9.493181228637695\n",
      "val MSE loss: 9.475360870361328\n",
      "train D loss: 1.2768759727478027 G loss: 10.133447647094727\n",
      "val MSE loss: 9.46312141418457\n",
      "val MSE loss: 9.435029029846191\n",
      "val MSE loss: 9.418620109558105\n",
      "train D loss: 1.26081383228302 G loss: 10.534642219543457\n",
      "val MSE loss: 9.3901948928833\n",
      "val MSE loss: 9.36467170715332\n",
      "val MSE loss: 9.347949028015137\n",
      "train D loss: 1.2703208923339844 G loss: 10.161453247070312\n",
      "val MSE loss: 9.330694198608398\n",
      "val MSE loss: 9.313780784606934\n",
      "val MSE loss: 9.284248352050781\n",
      "train D loss: 1.2769910097122192 G loss: 9.785961151123047\n",
      "val MSE loss: 9.255728721618652\n",
      "val MSE loss: 9.250368118286133\n",
      "val MSE loss: 9.222606658935547\n",
      "train D loss: 1.285379409790039 G loss: 9.466089248657227\n",
      "val MSE loss: 9.193195343017578\n",
      "val MSE loss: 9.174250602722168\n",
      "val MSE loss: 9.153575897216797\n",
      "train D loss: 1.2563420534133911 G loss: 10.299701690673828\n",
      "val MSE loss: 9.133566856384277\n",
      "val MSE loss: 9.10754680633545\n",
      "val MSE loss: 9.095627784729004\n",
      "train D loss: 1.2731306552886963 G loss: 9.618572235107422\n",
      "val MSE loss: 9.066089630126953\n",
      "val MSE loss: 9.046590805053711\n",
      "val MSE loss: 9.021607398986816\n",
      "train D loss: 1.2886216640472412 G loss: 8.999885559082031\n",
      "val MSE loss: 9.0074462890625\n",
      "val MSE loss: 8.981689453125\n",
      "val MSE loss: 8.968852996826172\n",
      "train D loss: 1.2769900560379028 G loss: 8.326979637145996\n",
      "epoch: 6\n",
      "val MSE loss: 8.952496528625488\n",
      "val MSE loss: 8.9335298538208\n",
      "val MSE loss: 8.924128532409668\n",
      "train D loss: 1.2871711254119873 G loss: 8.977885246276855\n",
      "val MSE loss: 8.902615547180176\n",
      "val MSE loss: 8.884499549865723\n",
      "val MSE loss: 8.868176460266113\n",
      "train D loss: 1.265108346939087 G loss: 9.278329849243164\n",
      "val MSE loss: 8.8548583984375\n",
      "val MSE loss: 8.825502395629883\n",
      "val MSE loss: 8.807047843933105\n",
      "train D loss: 1.2749056816101074 G loss: 8.74187183380127\n",
      "val MSE loss: 8.796355247497559\n",
      "val MSE loss: 8.768077850341797\n",
      "val MSE loss: 8.761980056762695\n",
      "train D loss: 1.2731549739837646 G loss: 9.691301345825195\n",
      "val MSE loss: 8.729873657226562\n",
      "val MSE loss: 8.728404998779297\n",
      "val MSE loss: 8.705121994018555\n",
      "train D loss: 1.2849717140197754 G loss: 8.578536987304688\n",
      "val MSE loss: 8.67386531829834\n",
      "val MSE loss: 8.66508674621582\n",
      "val MSE loss: 8.643026351928711\n",
      "train D loss: 1.2861206531524658 G loss: 8.626205444335938\n",
      "val MSE loss: 8.631673812866211\n",
      "val MSE loss: 8.60777759552002\n",
      "val MSE loss: 8.595720291137695\n",
      "train D loss: 1.2954994440078735 G loss: 8.295010566711426\n",
      "val MSE loss: 8.571977615356445\n",
      "val MSE loss: 8.555428504943848\n",
      "val MSE loss: 8.536283493041992\n",
      "train D loss: 1.2767261266708374 G loss: 8.588303565979004\n",
      "val MSE loss: 8.513590812683105\n",
      "val MSE loss: 8.497897148132324\n",
      "val MSE loss: 8.478863716125488\n",
      "train D loss: 1.26631760597229 G loss: 9.496529579162598\n",
      "val MSE loss: 8.461220741271973\n",
      "val MSE loss: 8.438180923461914\n",
      "val MSE loss: 8.412857055664062\n",
      "train D loss: 1.282688856124878 G loss: 8.663755416870117\n",
      "val MSE loss: 8.396934509277344\n",
      "val MSE loss: 8.37873363494873\n",
      "val MSE loss: 8.362201690673828\n",
      "train D loss: 1.2792046070098877 G loss: 8.071527481079102\n",
      "val MSE loss: 8.34188175201416\n",
      "val MSE loss: 8.318399429321289\n",
      "val MSE loss: 8.305900573730469\n",
      "train D loss: 1.2887635231018066 G loss: 8.186964988708496\n",
      "val MSE loss: 8.289338111877441\n",
      "val MSE loss: 8.26608943939209\n",
      "val MSE loss: 8.251317977905273\n",
      "train D loss: 1.2891759872436523 G loss: 8.720744132995605\n",
      "val MSE loss: 8.231417655944824\n",
      "val MSE loss: 8.206793785095215\n",
      "val MSE loss: 8.18813705444336\n",
      "train D loss: 1.2841651439666748 G loss: 8.128961563110352\n",
      "val MSE loss: 8.162489891052246\n",
      "val MSE loss: 8.14509105682373\n",
      "val MSE loss: 8.125619888305664\n",
      "train D loss: 1.2894673347473145 G loss: 8.142693519592285\n",
      "val MSE loss: 8.103626251220703\n",
      "val MSE loss: 8.085756301879883\n",
      "val MSE loss: 8.067037582397461\n",
      "train D loss: 1.2889617681503296 G loss: 8.746546745300293\n",
      "val MSE loss: 8.04625415802002\n",
      "val MSE loss: 8.025946617126465\n",
      "val MSE loss: 8.011530876159668\n",
      "train D loss: 1.273786187171936 G loss: 8.453523635864258\n",
      "val MSE loss: 7.992110252380371\n",
      "val MSE loss: 7.96940279006958\n",
      "val MSE loss: 7.947791576385498\n",
      "train D loss: 1.2593274116516113 G loss: 9.70240306854248\n",
      "val MSE loss: 7.933422088623047\n",
      "val MSE loss: 7.903078556060791\n",
      "val MSE loss: 7.892576217651367\n",
      "train D loss: 1.2773075103759766 G loss: 7.779615879058838\n",
      "val MSE loss: 7.8717169761657715\n",
      "val MSE loss: 7.850364685058594\n",
      "val MSE loss: 7.833767414093018\n",
      "train D loss: 1.2821452617645264 G loss: 7.919681549072266\n",
      "val MSE loss: 7.812079429626465\n",
      "val MSE loss: 7.793013572692871\n",
      "val MSE loss: 7.77266263961792\n",
      "train D loss: 1.2876098155975342 G loss: 8.023158073425293\n",
      "val MSE loss: 7.752826690673828\n",
      "val MSE loss: 7.738614559173584\n",
      "val MSE loss: 7.714909076690674\n",
      "train D loss: 1.2808887958526611 G loss: 8.524088859558105\n",
      "val MSE loss: 7.700867176055908\n",
      "val MSE loss: 7.685981273651123\n",
      "val MSE loss: 7.6732683181762695\n",
      "train D loss: 1.2970054149627686 G loss: 8.229519844055176\n",
      "val MSE loss: 7.645267009735107\n",
      "val MSE loss: 7.6339545249938965\n",
      "val MSE loss: 7.6090240478515625\n",
      "train D loss: 1.2980509996414185 G loss: 7.507726192474365\n",
      "epoch: 7\n",
      "val MSE loss: 7.584372520446777\n",
      "val MSE loss: 7.573847770690918\n",
      "val MSE loss: 7.555202484130859\n",
      "train D loss: 1.2925422191619873 G loss: 7.619790077209473\n",
      "val MSE loss: 7.535170078277588\n",
      "val MSE loss: 7.510743618011475\n",
      "val MSE loss: 7.4870219230651855\n",
      "train D loss: 1.265904426574707 G loss: 8.31273365020752\n",
      "val MSE loss: 7.4743828773498535\n",
      "val MSE loss: 7.449019432067871\n",
      "val MSE loss: 7.431654453277588\n",
      "train D loss: 1.2885105609893799 G loss: 7.937928199768066\n",
      "val MSE loss: 7.412386894226074\n",
      "val MSE loss: 7.387477874755859\n",
      "val MSE loss: 7.363023281097412\n",
      "train D loss: 1.2790918350219727 G loss: 8.132608413696289\n",
      "val MSE loss: 7.3472089767456055\n",
      "val MSE loss: 7.324637413024902\n",
      "val MSE loss: 7.310362815856934\n",
      "train D loss: 1.2975006103515625 G loss: 7.159019470214844\n",
      "val MSE loss: 7.286454200744629\n",
      "val MSE loss: 7.271640777587891\n",
      "val MSE loss: 7.242842674255371\n",
      "train D loss: 1.2938610315322876 G loss: 7.747122764587402\n",
      "val MSE loss: 7.230525493621826\n",
      "val MSE loss: 7.208977699279785\n",
      "val MSE loss: 7.192889213562012\n",
      "train D loss: 1.2998777627944946 G loss: 7.083832740783691\n",
      "val MSE loss: 7.170808792114258\n",
      "val MSE loss: 7.141184329986572\n",
      "val MSE loss: 7.123558521270752\n",
      "train D loss: 1.2977192401885986 G loss: 7.264824867248535\n",
      "val MSE loss: 7.103769302368164\n",
      "val MSE loss: 7.088888645172119\n",
      "val MSE loss: 7.060459613800049\n",
      "train D loss: 1.30332612991333 G loss: 6.68271541595459\n",
      "val MSE loss: 7.045848369598389\n",
      "val MSE loss: 7.037380695343018\n",
      "val MSE loss: 7.011948108673096\n",
      "train D loss: 1.2865651845932007 G loss: 7.066123008728027\n",
      "val MSE loss: 6.997838020324707\n",
      "val MSE loss: 6.977726459503174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val MSE loss: 6.956114768981934\n",
      "train D loss: 1.2972341775894165 G loss: 7.477616310119629\n",
      "val MSE loss: 6.941550254821777\n",
      "val MSE loss: 6.926775932312012\n",
      "val MSE loss: 6.907219886779785\n",
      "train D loss: 1.2883284091949463 G loss: 6.960317611694336\n",
      "val MSE loss: 6.896113395690918\n",
      "val MSE loss: 6.877568244934082\n",
      "val MSE loss: 6.868427753448486\n",
      "train D loss: 1.2902922630310059 G loss: 7.518331527709961\n",
      "val MSE loss: 6.854240894317627\n",
      "val MSE loss: 6.834964275360107\n",
      "val MSE loss: 6.823108196258545\n",
      "train D loss: 1.2934234142303467 G loss: 6.978027820587158\n",
      "val MSE loss: 6.801543712615967\n",
      "val MSE loss: 6.780981540679932\n",
      "val MSE loss: 6.766584873199463\n",
      "train D loss: 1.3111002445220947 G loss: 6.385170936584473\n",
      "val MSE loss: 6.751660346984863\n",
      "val MSE loss: 6.720277309417725\n",
      "val MSE loss: 6.706488132476807\n",
      "train D loss: 1.3076705932617188 G loss: 7.35164213180542\n",
      "val MSE loss: 6.688843727111816\n",
      "val MSE loss: 6.66713285446167\n",
      "val MSE loss: 6.653197288513184\n",
      "train D loss: 1.3108255863189697 G loss: 6.105069637298584\n",
      "val MSE loss: 6.631734848022461\n",
      "val MSE loss: 6.599483013153076\n",
      "val MSE loss: 6.585085868835449\n",
      "train D loss: 1.3027547597885132 G loss: 7.585666656494141\n",
      "val MSE loss: 6.57269287109375\n",
      "val MSE loss: 6.548920154571533\n",
      "val MSE loss: 6.527088642120361\n",
      "train D loss: 1.291579008102417 G loss: 6.7087249755859375\n",
      "val MSE loss: 6.516758918762207\n",
      "val MSE loss: 6.4903059005737305\n",
      "val MSE loss: 6.467598915100098\n",
      "train D loss: 1.2844295501708984 G loss: 7.3217620849609375\n",
      "val MSE loss: 6.449230670928955\n",
      "val MSE loss: 6.428894996643066\n",
      "val MSE loss: 6.417839050292969\n",
      "train D loss: 1.2969141006469727 G loss: 6.837072849273682\n",
      "val MSE loss: 6.398597717285156\n",
      "val MSE loss: 6.377901077270508\n",
      "val MSE loss: 6.367190837860107\n",
      "train D loss: 1.3037574291229248 G loss: 6.5382537841796875\n",
      "val MSE loss: 6.349430561065674\n",
      "val MSE loss: 6.332777500152588\n",
      "val MSE loss: 6.312061309814453\n",
      "train D loss: 1.3069100379943848 G loss: 6.771921157836914\n",
      "val MSE loss: 6.297969818115234\n",
      "val MSE loss: 6.278491020202637\n",
      "val MSE loss: 6.262937068939209\n",
      "train D loss: 1.2890443801879883 G loss: 7.465550899505615\n",
      "epoch: 8\n",
      "val MSE loss: 6.253522872924805\n",
      "val MSE loss: 6.233011245727539\n",
      "val MSE loss: 6.216780662536621\n",
      "train D loss: 1.2928026914596558 G loss: 6.4459404945373535\n",
      "val MSE loss: 6.206794261932373\n",
      "val MSE loss: 6.1839518547058105\n",
      "val MSE loss: 6.176690578460693\n",
      "train D loss: 1.2985255718231201 G loss: 6.5313639640808105\n",
      "val MSE loss: 6.163388729095459\n",
      "val MSE loss: 6.145249843597412\n",
      "val MSE loss: 6.131109237670898\n",
      "train D loss: 1.295158863067627 G loss: 6.691266059875488\n",
      "val MSE loss: 6.115957260131836\n",
      "val MSE loss: 6.103897571563721\n",
      "val MSE loss: 6.091899871826172\n",
      "train D loss: 1.321058988571167 G loss: 5.544804573059082\n",
      "val MSE loss: 6.080113887786865\n",
      "val MSE loss: 6.053133487701416\n",
      "val MSE loss: 6.051016807556152\n",
      "train D loss: 1.2972166538238525 G loss: 5.943981647491455\n",
      "val MSE loss: 6.037519454956055\n",
      "val MSE loss: 6.025274753570557\n",
      "val MSE loss: 6.006085395812988\n",
      "train D loss: 1.303877592086792 G loss: 7.3825554847717285\n",
      "val MSE loss: 5.99285364151001\n",
      "val MSE loss: 5.988792896270752\n",
      "val MSE loss: 5.973276138305664\n",
      "train D loss: 1.3139674663543701 G loss: 6.770204544067383\n",
      "val MSE loss: 5.961156368255615\n",
      "val MSE loss: 5.94599723815918\n",
      "val MSE loss: 5.932400226593018\n",
      "train D loss: 1.2966437339782715 G loss: 6.802789211273193\n",
      "val MSE loss: 5.913255214691162\n",
      "val MSE loss: 5.896752834320068\n",
      "val MSE loss: 5.877274036407471\n",
      "train D loss: 1.303309440612793 G loss: 6.4617815017700195\n",
      "val MSE loss: 5.868990898132324\n",
      "val MSE loss: 5.848803520202637\n",
      "val MSE loss: 5.8342814445495605\n",
      "train D loss: 1.3138149976730347 G loss: 6.221996784210205\n",
      "val MSE loss: 5.811033725738525\n",
      "val MSE loss: 5.798614025115967\n",
      "val MSE loss: 5.7909440994262695\n",
      "train D loss: 1.3093228340148926 G loss: 5.656651496887207\n",
      "val MSE loss: 5.773995876312256\n",
      "val MSE loss: 5.754380226135254\n",
      "val MSE loss: 5.740476131439209\n",
      "train D loss: 1.317194938659668 G loss: 6.228363990783691\n",
      "val MSE loss: 5.7245683670043945\n",
      "val MSE loss: 5.710120677947998\n",
      "val MSE loss: 5.692539215087891\n",
      "train D loss: 1.3122941255569458 G loss: 6.405914306640625\n",
      "val MSE loss: 5.6765456199646\n",
      "val MSE loss: 5.673065185546875\n",
      "val MSE loss: 5.65425968170166\n",
      "train D loss: 1.3091895580291748 G loss: 5.682309627532959\n",
      "val MSE loss: 5.642752647399902\n",
      "val MSE loss: 5.623382091522217\n",
      "val MSE loss: 5.616192817687988\n",
      "train D loss: 1.2945716381072998 G loss: 6.284662246704102\n",
      "val MSE loss: 5.604907512664795\n",
      "val MSE loss: 5.593418121337891\n",
      "val MSE loss: 5.573883533477783\n",
      "train D loss: 1.3102483749389648 G loss: 5.775359630584717\n",
      "val MSE loss: 5.5610527992248535\n",
      "val MSE loss: 5.549813270568848\n",
      "val MSE loss: 5.525805473327637\n",
      "train D loss: 1.3026140928268433 G loss: 5.706058025360107\n",
      "val MSE loss: 5.507421016693115\n",
      "val MSE loss: 5.491543769836426\n",
      "val MSE loss: 5.478997230529785\n",
      "train D loss: 1.3245501518249512 G loss: 5.393742084503174\n",
      "val MSE loss: 5.458455562591553\n",
      "val MSE loss: 5.452908039093018\n",
      "val MSE loss: 5.430084705352783\n",
      "train D loss: 1.3138904571533203 G loss: 5.495141983032227\n",
      "val MSE loss: 5.4241743087768555\n",
      "val MSE loss: 5.403874397277832\n",
      "val MSE loss: 5.393198013305664\n",
      "train D loss: 1.3280137777328491 G loss: 5.114190578460693\n",
      "val MSE loss: 5.380960464477539\n",
      "val MSE loss: 5.361452102661133\n",
      "val MSE loss: 5.348876476287842\n",
      "train D loss: 1.3125908374786377 G loss: 5.810579776763916\n",
      "val MSE loss: 5.3354363441467285\n",
      "val MSE loss: 5.325102806091309\n",
      "val MSE loss: 5.311953544616699\n",
      "train D loss: 1.3149693012237549 G loss: 5.180228233337402\n",
      "val MSE loss: 5.301126480102539\n",
      "val MSE loss: 5.285518646240234\n",
      "val MSE loss: 5.275866508483887\n",
      "train D loss: 1.323739767074585 G loss: 5.313060760498047\n",
      "val MSE loss: 5.261016368865967\n",
      "val MSE loss: 5.256721019744873\n",
      "val MSE loss: 5.240713119506836\n",
      "train D loss: 1.3470131158828735 G loss: 4.743679523468018\n",
      "epoch: 9\n",
      "val MSE loss: 5.231481552124023\n",
      "val MSE loss: 5.216893672943115\n",
      "val MSE loss: 5.210156440734863\n",
      "train D loss: 1.3115532398223877 G loss: 5.228210926055908\n",
      "val MSE loss: 5.197841167449951\n",
      "val MSE loss: 5.190098762512207\n",
      "val MSE loss: 5.173661231994629\n",
      "train D loss: 1.3033545017242432 G loss: 5.431787490844727\n",
      "val MSE loss: 5.161787033081055\n",
      "val MSE loss: 5.152656555175781\n",
      "val MSE loss: 5.138794898986816\n",
      "train D loss: 1.2990816831588745 G loss: 5.349452495574951\n",
      "val MSE loss: 5.129289150238037\n",
      "val MSE loss: 5.123410701751709\n",
      "val MSE loss: 5.110479831695557\n",
      "train D loss: 1.3288328647613525 G loss: 4.962033748626709\n",
      "val MSE loss: 5.096444606781006\n",
      "val MSE loss: 5.081836700439453\n",
      "val MSE loss: 5.0713982582092285\n",
      "train D loss: 1.2971773147583008 G loss: 5.931252956390381\n",
      "val MSE loss: 5.067650318145752\n",
      "val MSE loss: 5.049532890319824\n",
      "val MSE loss: 5.042314529418945\n",
      "train D loss: 1.3210631608963013 G loss: 5.667526721954346\n",
      "val MSE loss: 5.027818202972412\n",
      "val MSE loss: 5.016566753387451\n",
      "val MSE loss: 5.003164291381836\n",
      "train D loss: 1.3071590662002563 G loss: 5.930551528930664\n",
      "val MSE loss: 4.996772289276123\n",
      "val MSE loss: 4.9822797775268555\n",
      "val MSE loss: 4.968876838684082\n",
      "train D loss: 1.3174192905426025 G loss: 4.383768081665039\n",
      "val MSE loss: 4.958975791931152\n",
      "val MSE loss: 4.946503162384033\n",
      "val MSE loss: 4.932821273803711\n",
      "train D loss: 1.3163615465164185 G loss: 5.10676383972168\n",
      "val MSE loss: 4.922267436981201\n",
      "val MSE loss: 4.90484619140625\n",
      "val MSE loss: 4.894863128662109\n",
      "train D loss: 1.3133931159973145 G loss: 5.54240608215332\n",
      "val MSE loss: 4.878959655761719\n",
      "val MSE loss: 4.867219924926758\n",
      "val MSE loss: 4.8539347648620605\n",
      "train D loss: 1.3122011423110962 G loss: 4.961016654968262\n",
      "val MSE loss: 4.842060565948486\n",
      "val MSE loss: 4.825810432434082\n",
      "val MSE loss: 4.815037727355957\n",
      "train D loss: 1.3211065530776978 G loss: 4.804440021514893\n",
      "val MSE loss: 4.802644729614258\n",
      "val MSE loss: 4.797868728637695\n",
      "val MSE loss: 4.786397457122803\n",
      "train D loss: 1.3150064945220947 G loss: 5.010833263397217\n",
      "val MSE loss: 4.779755115509033\n",
      "val MSE loss: 4.764561653137207\n",
      "val MSE loss: 4.758338451385498\n",
      "train D loss: 1.3430798053741455 G loss: 4.677089214324951\n",
      "val MSE loss: 4.747776985168457\n",
      "val MSE loss: 4.738614082336426\n",
      "val MSE loss: 4.72761869430542\n",
      "train D loss: 1.3154640197753906 G loss: 4.618215560913086\n",
      "val MSE loss: 4.7181315422058105\n",
      "val MSE loss: 4.712043285369873\n",
      "val MSE loss: 4.699345588684082\n",
      "train D loss: 1.3145325183868408 G loss: 5.338389873504639\n",
      "val MSE loss: 4.6910014152526855\n",
      "val MSE loss: 4.682998180389404\n",
      "val MSE loss: 4.67576265335083\n",
      "train D loss: 1.3082871437072754 G loss: 5.202981472015381\n",
      "val MSE loss: 4.6623077392578125\n",
      "val MSE loss: 4.6493821144104\n",
      "val MSE loss: 4.637444496154785\n",
      "train D loss: 1.3297224044799805 G loss: 4.574000835418701\n",
      "val MSE loss: 4.629634857177734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val MSE loss: 4.61967134475708\n",
      "val MSE loss: 4.609200954437256\n",
      "train D loss: 1.3096771240234375 G loss: 4.94633150100708\n",
      "val MSE loss: 4.6002984046936035\n",
      "val MSE loss: 4.590822696685791\n",
      "val MSE loss: 4.5800065994262695\n",
      "train D loss: 1.2988412380218506 G loss: 5.79888391494751\n",
      "val MSE loss: 4.571438312530518\n",
      "val MSE loss: 4.563554763793945\n",
      "val MSE loss: 4.560601234436035\n",
      "train D loss: 1.3143959045410156 G loss: 5.542464256286621\n",
      "val MSE loss: 4.551673412322998\n",
      "val MSE loss: 4.5482401847839355\n",
      "val MSE loss: 4.54475736618042\n",
      "train D loss: 1.3200645446777344 G loss: 4.709235668182373\n",
      "val MSE loss: 4.5379509925842285\n",
      "val MSE loss: 4.530390739440918\n",
      "val MSE loss: 4.529071807861328\n",
      "train D loss: 1.3112890720367432 G loss: 4.646711349487305\n",
      "val MSE loss: 4.516599178314209\n",
      "val MSE loss: 4.511559963226318\n",
      "val MSE loss: 4.507720947265625\n",
      "train D loss: 1.3115673065185547 G loss: 3.93868088722229\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print('epoch:',epoch)\n",
    "    for step , (batch_x,batch_y) in enumerate(loader):\n",
    "        # 训练判别器D，目的是能够区分G的输出和真实标签\n",
    "        for d in range(1):\n",
    "            D.train()\n",
    "            G.eval()\n",
    "            # 前向传播\n",
    "            z=torch.from_numpy(np.random.randn(batch_x.shape[0], 10)).float()   # 随机噪声\n",
    "            d_real = D(batch_y)     #  （标签）输入判别器的结果\n",
    "            batch_y_pred = G(torch.cat([z, batch_x], dim=1))      #  （噪声+x）输入生成器，得到预测结果\n",
    "            d_gen = D(batch_y_pred)        #  （预测结果）输入判别器的结果\n",
    "            # 计算损失\n",
    "            Dloss_real = loss_func_bce(d_real, torch.ones((batch_x.shape[0],1))) # 对于（标签+x）组，判别器输出应趋向于全1\n",
    "            Dloss_gen = loss_func_bce(d_gen, torch.zeros((batch_x.shape[0],1)))  # 对于（预测结果+x）组，判别器输出应趋向于全0\n",
    "            Dloss = Dloss_real + Dloss_gen\n",
    "            # 反向传播（只对判别器参数进行更新）\n",
    "            Dloss.backward()\n",
    "            opt_d.step()\n",
    "            opt_d.zero_grad()\n",
    "            opt_g.zero_grad()\n",
    "        # 训练生成器G，目的是G的输出能够欺骗D，让D以为G的输出就是真实标签\n",
    "        for g in range(3):\n",
    "            D.eval()\n",
    "            G.train()\n",
    "            # 前向传播\n",
    "            z = torch.from_numpy(np.random.randn(batch_x.shape[0], 10)).float() # 随机噪声\n",
    "            batch_y_pred = G(torch.cat([z, batch_x], dim=1))\n",
    "            d_gen = D(batch_y_pred)  \n",
    "            # 计算损失函数\n",
    "            Gloss_adventure = 0.3 * loss_func_bce(d_gen, torch.ones((batch_x.shape[0],1)))   # G的目的是，让D以为它的输出就是真实标签，因此G趋向于让d_gen等于1\n",
    "            Gloss_regression = loss_func_reg(batch_y_pred,batch_y)\n",
    "            Gloss = Gloss_regression + Gloss_adventure\n",
    "            # 反向传播\n",
    "            Gloss.backward()\n",
    "            opt_g.step()\n",
    "            opt_g.zero_grad()\n",
    "            opt_d.zero_grad()\n",
    "            loss_train.append(Gloss_regression.item())\n",
    "            valid(G,x_test,loss_func_reg)\n",
    "        D.eval()\n",
    "        G.eval()\n",
    "        print('train D loss:', Dloss.detach().item(), 'G loss:', Gloss.detach().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce2b730f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAN_with_noise——测试集均方误差： 4.507720947265625\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_iter = 3*epochs*(x_train.shape[0]//batch_size + 1)\n",
    "x = [i for i in range(n_iter)]    # 每一轮G都训练了3次，所以乘3\n",
    "plt.plot(x,loss_train, label='train')\n",
    "plt.plot(x,loss_val, label='val')\n",
    "plt.title('GAN_with_noise')\n",
    "plt.legend()\n",
    "plt.savefig(fname=\"results/GAN_with_noise.png\")\n",
    "np.save('results/GAN_with_noise.npy',loss_train)   # 保存为npy文件,供不同方法对比\n",
    "print('GAN_with_noise——测试集均方误差：',loss_val[-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a608e94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.eval()\n",
    "z = torch.from_numpy(np.random.randn(data.shape[0], 10)).float() # 随机噪声\n",
    "coeff_pred = G(torch.cat([z, data], dim=1))\n",
    "np.savetxt('./results/prediction_res_GAN.csv', coeff_pred.detach().numpy(), delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
