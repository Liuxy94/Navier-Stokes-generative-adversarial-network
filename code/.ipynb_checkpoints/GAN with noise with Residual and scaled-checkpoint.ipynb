{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdb2bb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch,copy,argparse,csv\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9ee220",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09df5500",
   "metadata": {},
   "outputs": [],
   "source": [
    "velocity_coeff = np.genfromtxt('./data/POD_coeffs_3900_new_grid_221_42_Velocity_1.csv', delimiter=',')\n",
    "pressure_coeff = np.genfromtxt('./data/POD_coeffs_3900_new_grid_221_42_Pressure_1.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f442139",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_all = torch.from_numpy(np.append(velocity_coeff,pressure_coeff,axis=0)).type(torch.FloatTensor)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(coeff_all.T)\n",
    "coeff_all_scaled = torch.from_numpy(scaler.transform(coeff_all.T)).type(torch.FloatTensor)\n",
    "data = copy.deepcopy(coeff_all_scaled[:-1,:])\n",
    "label = copy.deepcopy(coeff_all_scaled[1:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1e2f6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data,label, test_size=0.3 , random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85ebd0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_inverse(data_scaled, scaler):\n",
    "    data = (data_scaled - torch.tensor(scaler.min_).type(torch.FloatTensor))/torch.tensor(scaler.scale_).type(torch.FloatTensor)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb3d013",
   "metadata": {},
   "source": [
    "## Compute residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dde356cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "basis_v = torch.from_numpy(np.genfromtxt('./data/Velocity_basis.csv', delimiter=',')).type(torch.FloatTensor)\n",
    "basis_p = torch.from_numpy(np.genfromtxt('./data/Pressure_basis.csv', delimiter=',')).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdfdb2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_p(coeff_p, basis_p):\n",
    "    length, number = coeff_p.shape\n",
    "    nx = 221\n",
    "    ny = 42\n",
    "    grid_p = torch.matmul(basis_p, coeff_p)\n",
    "    return grid_p.reshape(nx,ny,number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e47773e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_v(coeff_v, basis_v):\n",
    "    length, number = coeff_v.shape\n",
    "    nx = 221\n",
    "    ny = 42\n",
    "    grid_v = torch.matmul(basis_v, coeff_v)\n",
    "    grid_v1 = grid_v[0:nx*ny].reshape(nx,ny,number)\n",
    "    grid_v2 = grid_v[nx*ny:].reshape(nx,ny,number)\n",
    "    return grid_v1, grid_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52f9839c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(coeff, basis_v, basis_p):\n",
    "    nPOD = 10\n",
    "    coeff_v = coeff[:nPOD,:]\n",
    "    coeff_p = coeff[nPOD:,:]\n",
    "    v1, v2 = backward_v(coeff_v, basis_v)\n",
    "    p = backward_p(coeff_p, basis_p)\n",
    "    return v1, v2, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fc63b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_resids(coeff_scaled, coeff_pred_scaled, basis_v, basis_p, scaler):\n",
    "#     coeff = scaler.inverse_transform(coeff_scaled)\n",
    "    coeff =  norm_inverse(coeff_scaled, scaler)\n",
    "    coeff_pred = norm_inverse(coeff_pred_scaled, scaler)\n",
    "    u1_o1d, u2_o1d, p = backward(coeff.T, basis_v, basis_p)\n",
    "    u1, u2, p = backward(coeff_pred.T, basis_v, basis_p)\n",
    "    nx, ny = 221,42\n",
    "    dx, dy = 0.01, 0.01\n",
    "    dt = 1.\n",
    "    sigma = 0.\n",
    "    rho = 1.\n",
    "    mu = 1./300\n",
    "    # first order term of u1\n",
    "    u1x = (u1[2:,1:-1] - u1[:-2,1:-1])/(2.0*dx)\n",
    "    u1y = (u1[1:-1,2:] - u1[1:-1,:-2])/(2.0*dy)\n",
    "#     u1t = (u1_new[1:-1, 1:-1] - u1 [1:-1, 1:-1] )/dt \n",
    "    u1t = (u1[1:-1, 1:-1] - u1_o1d[1:-1, 1:-1])/dt \n",
    "    # second order term of u1 \n",
    "    u1xx = (u1[2:,1:-1] - 2. *u1[1:-1, 1:-1] + u1[0: -2, 1:-1])/(dx**2) \n",
    "    u1yy = (u1[1:-1,2:] - 2. *u1[1:-1, 1:-1] + u1[1:-1, 0:-2])/(dy**2) \n",
    "    # first order term of u2\n",
    "    u2x = (u2[2:,1:-1] - u2[0:-2, 1:-1])/(2.0*dx) \n",
    "    u2y = (u2[1:-1, 2: ] - u2 [1:-1, 0:-2])/(2.0*dy) \n",
    "#     u2t = (u2_new[1:-1, 1:-1] - u2 [1:-1, 1:-1] )/dt \n",
    "    u2t = (u2[1:-1, 1:-1] - u2_o1d[1:-1, 1:-1] )/dt \n",
    "    # second order term of u2 \n",
    "    u2xx = (u2[2:,1:-1] - 2.*u2[1:-1, 1:-1] + u2[0:-2, 1:-1])/(dx**2) \n",
    "    u2yy = (u2[1:-1,2:] - 2.*u2[1:-1, 1:-1] + u2[1:-1, 0:-2])/(dy**2) \n",
    "    # first order of p \n",
    "    px = (p[2:,1:-1] - p[0:-2,1:-1])/(2.* dx)\n",
    "    py = (p[1:-1,2:] - p[1:-1,0:-2])/(2.* dy)\n",
    "    r_cty = u1x + u2y \n",
    "    r_u1 = rho*u1t + sigma*u1 [1:-1, 1:-1]+ rho* (u1 [1:-1, 1:-1] *u1x+u2 [1:-1, 1:-1] *u1y) - mu* (u1xx+u1yy) + px\n",
    "    r_u2 = rho*u2t + sigma*u2 [1: -1, 1: -1]+ rho* (u1 [1:-1, 1:-1] *u2x+u2 [1:-1, 1:-1] *u2y) - mu* (u2xx+u2yy) + py\n",
    "    return r_cty, r_u1, r_u2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20171daf",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b95939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ========================模型验证======================\n",
    "def valid(model,data,criterion):\n",
    "    model.eval()\n",
    "    z = torch.from_numpy(np.random.randn(data.shape[0], 10)).float() # 随机噪声\n",
    "    with torch.no_grad():   # 不记录梯度信息\n",
    "        y_pred = model(torch.cat([z, data], dim=1))\n",
    "        loss = criterion(y_pred, y_test)\n",
    "        loss_val.append(loss.item())\n",
    "        print('val MSE loss:',loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1d59bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 网络G，用于预测下一时刻的特征向量\n",
    "G = torch.nn.Sequential(\n",
    "    torch.nn.Linear(20+10,30),   # 特征向量+噪声向量\n",
    "    torch.nn.ReLU(),   \n",
    "    torch.nn.Linear(30,15), \n",
    "    torch.nn.ReLU(),   \n",
    "    torch.nn.Linear(15,20),   \n",
    ")\n",
    "# 网络D，用于一组特征向量是网络G制造的还是真实标签，促使G的输出接近于真实标签\n",
    "D = torch.nn.Sequential(\n",
    "    torch.nn.Linear(20,10),   \n",
    "    torch.nn.ReLU(),   \n",
    "    torch.nn.Linear(10,5), \n",
    "    torch.nn.ReLU(),   \n",
    "    torch.nn.Linear(5,1),\n",
    "    nn.Sigmoid()   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dae7ea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func_bce = nn.BCELoss()    # 用于鉴别器模型D（对抗损失），D的任务判断是否为标签，属于二分类任务，因此用交叉熵损失\n",
    "loss_func_reg = nn.MSELoss()    # 用于生成器G，计算模型预测和标签的差异，因为是回归任务所以用均方根损失\n",
    "opt_g = torch.optim.Adam(G.parameters(), lr=3e-4)   \n",
    "opt_d = torch.optim.Adam(D.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9073166",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = []\n",
    "loss_val = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa147d7f",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c86370b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = Data.TensorDataset(x_train , y_train)\n",
    "batch_size = 60\n",
    "loader = Data.DataLoader(\n",
    "    dataset = dataset_train,\n",
    "    batch_size = batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers = 8\n",
    ")\n",
    "epochs = 10\n",
    "lambda_res = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47f16616",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "val MSE loss: 0.2793005108833313\n",
      "val MSE loss: 0.27761226892471313\n",
      "val MSE loss: 0.2768770754337311\n",
      "train D loss: 1.418960690498352 G loss: 143.44589233398438\n",
      "val MSE loss: 0.27434420585632324\n",
      "val MSE loss: 0.27432015538215637\n",
      "val MSE loss: 0.2729452848434448\n",
      "train D loss: 1.4174729585647583 G loss: 133.49781799316406\n",
      "val MSE loss: 0.27200090885162354\n",
      "val MSE loss: 0.26969555020332336\n",
      "val MSE loss: 0.26904240250587463\n",
      "train D loss: 1.4158588647842407 G loss: 126.17159271240234\n",
      "val MSE loss: 0.2674330770969391\n",
      "val MSE loss: 0.2665795385837555\n",
      "val MSE loss: 0.26484057307243347\n",
      "train D loss: 1.4155926704406738 G loss: 125.72518920898438\n",
      "val MSE loss: 0.2635948359966278\n",
      "val MSE loss: 0.26298749446868896\n",
      "val MSE loss: 0.26187118887901306\n",
      "train D loss: 1.4169732332229614 G loss: 124.88496398925781\n",
      "val MSE loss: 0.2611386775970459\n",
      "val MSE loss: 0.25888916850090027\n",
      "val MSE loss: 0.2580789029598236\n",
      "train D loss: 1.415212869644165 G loss: 123.2328109741211\n",
      "val MSE loss: 0.2561611235141754\n",
      "val MSE loss: 0.2551148533821106\n",
      "val MSE loss: 0.25406643748283386\n",
      "train D loss: 1.4125992059707642 G loss: 117.413818359375\n",
      "val MSE loss: 0.25179192423820496\n",
      "val MSE loss: 0.2511983811855316\n",
      "val MSE loss: 0.2501494288444519\n",
      "train D loss: 1.41415536403656 G loss: 120.2822494506836\n",
      "val MSE loss: 0.24936023354530334\n",
      "val MSE loss: 0.24721166491508484\n",
      "val MSE loss: 0.24574542045593262\n",
      "train D loss: 1.4131824970245361 G loss: 119.41741943359375\n",
      "val MSE loss: 0.24451039731502533\n",
      "val MSE loss: 0.24295678734779358\n",
      "val MSE loss: 0.24171586334705353\n",
      "train D loss: 1.414517879486084 G loss: 115.29767608642578\n",
      "val MSE loss: 0.24121561646461487\n",
      "val MSE loss: 0.23904858529567719\n",
      "val MSE loss: 0.23772595822811127\n",
      "train D loss: 1.4103786945343018 G loss: 116.66419982910156\n",
      "val MSE loss: 0.23538857698440552\n",
      "val MSE loss: 0.2343599498271942\n",
      "val MSE loss: 0.23201000690460205\n",
      "train D loss: 1.41153883934021 G loss: 109.01907348632812\n",
      "val MSE loss: 0.2299678772687912\n",
      "val MSE loss: 0.23041601479053497\n",
      "val MSE loss: 0.2285202294588089\n",
      "train D loss: 1.4152969121932983 G loss: 108.19532012939453\n",
      "val MSE loss: 0.22658634185791016\n",
      "val MSE loss: 0.2233300358057022\n",
      "val MSE loss: 0.223453089594841\n",
      "train D loss: 1.4150134325027466 G loss: 110.60078430175781\n",
      "val MSE loss: 0.2219359129667282\n",
      "val MSE loss: 0.22043131291866302\n",
      "val MSE loss: 0.21908311545848846\n",
      "train D loss: 1.4129990339279175 G loss: 95.26874542236328\n",
      "val MSE loss: 0.21664413809776306\n",
      "val MSE loss: 0.21569065749645233\n",
      "val MSE loss: 0.21371370553970337\n",
      "train D loss: 1.4126003980636597 G loss: 93.6624984741211\n",
      "val MSE loss: 0.21280428767204285\n",
      "val MSE loss: 0.21072892844676971\n",
      "val MSE loss: 0.20878928899765015\n",
      "train D loss: 1.414064884185791 G loss: 101.2111587524414\n",
      "val MSE loss: 0.20731955766677856\n",
      "val MSE loss: 0.2051205188035965\n",
      "val MSE loss: 0.20534135401248932\n",
      "train D loss: 1.4125889539718628 G loss: 94.08317565917969\n",
      "val MSE loss: 0.20328356325626373\n",
      "val MSE loss: 0.20108264684677124\n",
      "val MSE loss: 0.19945363700389862\n",
      "train D loss: 1.4133425951004028 G loss: 87.02110290527344\n",
      "val MSE loss: 0.1976529061794281\n",
      "val MSE loss: 0.1967335194349289\n",
      "val MSE loss: 0.1949385553598404\n",
      "train D loss: 1.4139587879180908 G loss: 87.72161102294922\n",
      "val MSE loss: 0.1925986260175705\n",
      "val MSE loss: 0.19198542833328247\n",
      "val MSE loss: 0.1893618106842041\n",
      "train D loss: 1.4137115478515625 G loss: 87.98385620117188\n",
      "val MSE loss: 0.18850046396255493\n",
      "val MSE loss: 0.18713438510894775\n",
      "val MSE loss: 0.1848045438528061\n",
      "train D loss: 1.4145848751068115 G loss: 87.96635437011719\n",
      "val MSE loss: 0.18417449295520782\n",
      "val MSE loss: 0.18302665650844574\n",
      "val MSE loss: 0.18235348165035248\n",
      "train D loss: 1.414066195487976 G loss: 78.43905639648438\n",
      "val MSE loss: 0.17921987175941467\n",
      "val MSE loss: 0.1792798638343811\n",
      "val MSE loss: 0.17780649662017822\n",
      "train D loss: 1.4132049083709717 G loss: 46.11705017089844\n",
      "epoch: 1\n",
      "val MSE loss: 0.17739078402519226\n",
      "val MSE loss: 0.17506125569343567\n",
      "val MSE loss: 0.173801988363266\n",
      "train D loss: 1.4141979217529297 G loss: 79.33383178710938\n",
      "val MSE loss: 0.17213495075702667\n",
      "val MSE loss: 0.17125730216503143\n",
      "val MSE loss: 0.1706981658935547\n",
      "train D loss: 1.414244532585144 G loss: 79.62483978271484\n",
      "val MSE loss: 0.17001619935035706\n",
      "val MSE loss: 0.16816633939743042\n",
      "val MSE loss: 0.16766197979450226\n",
      "train D loss: 1.415771484375 G loss: 79.21014404296875\n",
      "val MSE loss: 0.16624704003334045\n",
      "val MSE loss: 0.16580800712108612\n",
      "val MSE loss: 0.16416028141975403\n",
      "train D loss: 1.4135218858718872 G loss: 73.88499450683594\n",
      "val MSE loss: 0.16307489573955536\n",
      "val MSE loss: 0.16275568306446075\n",
      "val MSE loss: 0.16153720021247864\n",
      "train D loss: 1.4132286310195923 G loss: 72.31473541259766\n",
      "val MSE loss: 0.16135302186012268\n",
      "val MSE loss: 0.159650057554245\n",
      "val MSE loss: 0.15935847163200378\n",
      "train D loss: 1.4129446744918823 G loss: 72.99745178222656\n",
      "val MSE loss: 0.15850625932216644\n",
      "val MSE loss: 0.15836173295974731\n",
      "val MSE loss: 0.15752169489860535\n",
      "train D loss: 1.4138898849487305 G loss: 69.55928039550781\n",
      "val MSE loss: 0.15626609325408936\n",
      "val MSE loss: 0.15515927970409393\n",
      "val MSE loss: 0.15495702624320984\n",
      "train D loss: 1.4132295846939087 G loss: 67.1812973022461\n",
      "val MSE loss: 0.15357090532779694\n",
      "val MSE loss: 0.15348154306411743\n",
      "val MSE loss: 0.15229277312755585\n",
      "train D loss: 1.4153399467468262 G loss: 72.72510528564453\n",
      "val MSE loss: 0.15203239023685455\n",
      "val MSE loss: 0.15168724954128265\n",
      "val MSE loss: 0.1508541852235794\n",
      "train D loss: 1.4113266468048096 G loss: 71.04081726074219\n",
      "val MSE loss: 0.15086530148983002\n",
      "val MSE loss: 0.1503414362668991\n",
      "val MSE loss: 0.1492949277162552\n",
      "train D loss: 1.4133977890014648 G loss: 71.36061096191406\n",
      "val MSE loss: 0.14868532121181488\n",
      "val MSE loss: 0.1482672542333603\n",
      "val MSE loss: 0.1479945033788681\n",
      "train D loss: 1.4129949808120728 G loss: 69.90098571777344\n",
      "val MSE loss: 0.14666245877742767\n",
      "val MSE loss: 0.14722809195518494\n",
      "val MSE loss: 0.14681866765022278\n",
      "train D loss: 1.4133617877960205 G loss: 67.97124481201172\n",
      "val MSE loss: 0.146332785487175\n",
      "val MSE loss: 0.14350192248821259\n",
      "val MSE loss: 0.14461076259613037\n",
      "train D loss: 1.412797451019287 G loss: 64.25395965576172\n",
      "val MSE loss: 0.1437644511461258\n",
      "val MSE loss: 0.14297953248023987\n",
      "val MSE loss: 0.14312976598739624\n",
      "train D loss: 1.412502408027649 G loss: 68.62590026855469\n",
      "val MSE loss: 0.14163166284561157\n",
      "val MSE loss: 0.14221327006816864\n",
      "val MSE loss: 0.14070364832878113\n",
      "train D loss: 1.4137699604034424 G loss: 67.5904769897461\n",
      "val MSE loss: 0.14044466614723206\n",
      "val MSE loss: 0.1404629647731781\n",
      "val MSE loss: 0.1396854668855667\n",
      "train D loss: 1.4144535064697266 G loss: 63.99702453613281\n",
      "val MSE loss: 0.13974237442016602\n",
      "val MSE loss: 0.1384507566690445\n",
      "val MSE loss: 0.13812464475631714\n",
      "train D loss: 1.4132353067398071 G loss: 66.80781555175781\n",
      "val MSE loss: 0.13795845210552216\n",
      "val MSE loss: 0.13722558319568634\n",
      "val MSE loss: 0.13747623562812805\n",
      "train D loss: 1.4125458002090454 G loss: 67.0260009765625\n",
      "val MSE loss: 0.1365155428647995\n",
      "val MSE loss: 0.13635967671871185\n",
      "val MSE loss: 0.1357889026403427\n",
      "train D loss: 1.411815881729126 G loss: 67.4627685546875\n",
      "val MSE loss: 0.13529786467552185\n",
      "val MSE loss: 0.1352263242006302\n",
      "val MSE loss: 0.13483014702796936\n",
      "train D loss: 1.4138267040252686 G loss: 66.86829376220703\n",
      "val MSE loss: 0.1343681812286377\n",
      "val MSE loss: 0.13280482590198517\n",
      "val MSE loss: 0.13335803151130676\n",
      "train D loss: 1.4112627506256104 G loss: 65.15699005126953\n",
      "val MSE loss: 0.1330784410238266\n",
      "val MSE loss: 0.13215310871601105\n",
      "val MSE loss: 0.1322198212146759\n",
      "train D loss: 1.4157902002334595 G loss: 64.3232192993164\n",
      "val MSE loss: 0.13218440115451813\n",
      "val MSE loss: 0.13137799501419067\n",
      "val MSE loss: 0.13095372915267944\n",
      "train D loss: 1.4138809442520142 G loss: 34.94813537597656\n",
      "epoch: 2\n",
      "val MSE loss: 0.1305699199438095\n",
      "val MSE loss: 0.13004447519779205\n",
      "val MSE loss: 0.12892769277095795\n",
      "train D loss: 1.4127819538116455 G loss: 65.05905151367188\n",
      "val MSE loss: 0.1289304941892624\n",
      "val MSE loss: 0.12835533916950226\n",
      "val MSE loss: 0.1287754476070404\n",
      "train D loss: 1.4131561517715454 G loss: 64.57564544677734\n",
      "val MSE loss: 0.12850289046764374\n",
      "val MSE loss: 0.12672674655914307\n",
      "val MSE loss: 0.12716464698314667\n",
      "train D loss: 1.410834789276123 G loss: 64.09930419921875\n",
      "val MSE loss: 0.1262684166431427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val MSE loss: 0.12581747770309448\n",
      "val MSE loss: 0.12530983984470367\n",
      "train D loss: 1.4111721515655518 G loss: 62.04731369018555\n",
      "val MSE loss: 0.1254795640707016\n",
      "val MSE loss: 0.12400101870298386\n",
      "val MSE loss: 0.12455303966999054\n",
      "train D loss: 1.4127888679504395 G loss: 61.46040344238281\n",
      "val MSE loss: 0.12269020825624466\n",
      "val MSE loss: 0.12332387268543243\n",
      "val MSE loss: 0.12340070307254791\n",
      "train D loss: 1.4124114513397217 G loss: 64.65586853027344\n",
      "val MSE loss: 0.12258543074131012\n",
      "val MSE loss: 0.12136173248291016\n",
      "val MSE loss: 0.1218225285410881\n",
      "train D loss: 1.4123907089233398 G loss: 61.09112548828125\n",
      "val MSE loss: 0.1204843744635582\n",
      "val MSE loss: 0.12026676535606384\n",
      "val MSE loss: 0.11963029205799103\n",
      "train D loss: 1.4116592407226562 G loss: 63.25872802734375\n",
      "val MSE loss: 0.11947114765644073\n",
      "val MSE loss: 0.11926993727684021\n",
      "val MSE loss: 0.11832230538129807\n",
      "train D loss: 1.4149783849716187 G loss: 62.7800407409668\n",
      "val MSE loss: 0.11735239624977112\n",
      "val MSE loss: 0.11746913939714432\n",
      "val MSE loss: 0.11667728424072266\n",
      "train D loss: 1.4140692949295044 G loss: 62.41782760620117\n",
      "val MSE loss: 0.11634159088134766\n",
      "val MSE loss: 0.11654990911483765\n",
      "val MSE loss: 0.11689218878746033\n",
      "train D loss: 1.414430856704712 G loss: 62.692291259765625\n",
      "val MSE loss: 0.11532150954008102\n",
      "val MSE loss: 0.11564446240663528\n",
      "val MSE loss: 0.11506722122430801\n",
      "train D loss: 1.4111772775650024 G loss: 60.56217956542969\n",
      "val MSE loss: 0.11461004614830017\n",
      "val MSE loss: 0.11428334563970566\n",
      "val MSE loss: 0.1136532872915268\n",
      "train D loss: 1.412893295288086 G loss: 60.04215621948242\n",
      "val MSE loss: 0.11323817074298859\n",
      "val MSE loss: 0.11342063546180725\n",
      "val MSE loss: 0.1126345843076706\n",
      "train D loss: 1.4124295711517334 G loss: 62.79342269897461\n",
      "val MSE loss: 0.11158351600170135\n",
      "val MSE loss: 0.11108982563018799\n",
      "val MSE loss: 0.1105637401342392\n",
      "train D loss: 1.4151281118392944 G loss: 63.014198303222656\n",
      "val MSE loss: 0.11022991687059402\n",
      "val MSE loss: 0.10996313393115997\n",
      "val MSE loss: 0.10926537960767746\n",
      "train D loss: 1.4130635261535645 G loss: 61.5651969909668\n",
      "val MSE loss: 0.10879921168088913\n",
      "val MSE loss: 0.10822685062885284\n",
      "val MSE loss: 0.10735142230987549\n",
      "train D loss: 1.411214828491211 G loss: 62.00515365600586\n",
      "val MSE loss: 0.10751587152481079\n",
      "val MSE loss: 0.10735190659761429\n",
      "val MSE loss: 0.10611115396022797\n",
      "train D loss: 1.4102156162261963 G loss: 60.38519287109375\n",
      "val MSE loss: 0.10598404705524445\n",
      "val MSE loss: 0.10534893721342087\n",
      "val MSE loss: 0.10457981377840042\n",
      "train D loss: 1.41155207157135 G loss: 60.2227897644043\n",
      "val MSE loss: 0.10491782426834106\n",
      "val MSE loss: 0.10435747355222702\n",
      "val MSE loss: 0.10407084971666336\n",
      "train D loss: 1.4113869667053223 G loss: 61.36471176147461\n",
      "val MSE loss: 0.10348313301801682\n",
      "val MSE loss: 0.10378861427307129\n",
      "val MSE loss: 0.10310140252113342\n",
      "train D loss: 1.414487361907959 G loss: 59.9229621887207\n",
      "val MSE loss: 0.10269030928611755\n",
      "val MSE loss: 0.10169990360736847\n",
      "val MSE loss: 0.10217521339654922\n",
      "train D loss: 1.411350131034851 G loss: 59.17280197143555\n",
      "val MSE loss: 0.10147935152053833\n",
      "val MSE loss: 0.1008831262588501\n",
      "val MSE loss: 0.10066808760166168\n",
      "train D loss: 1.4121743440628052 G loss: 60.049339294433594\n",
      "val MSE loss: 0.1005992516875267\n",
      "val MSE loss: 0.09976734220981598\n",
      "val MSE loss: 0.0996723622083664\n",
      "train D loss: 1.412269115447998 G loss: 33.9473991394043\n",
      "epoch: 3\n",
      "val MSE loss: 0.09967700392007828\n",
      "val MSE loss: 0.09917548298835754\n",
      "val MSE loss: 0.09901153296232224\n",
      "train D loss: 1.4113713502883911 G loss: 60.93579864501953\n",
      "val MSE loss: 0.09908686578273773\n",
      "val MSE loss: 0.09815604984760284\n",
      "val MSE loss: 0.09822998195886612\n",
      "train D loss: 1.4122138023376465 G loss: 60.341758728027344\n",
      "val MSE loss: 0.09858622401952744\n",
      "val MSE loss: 0.09748253226280212\n",
      "val MSE loss: 0.09730708599090576\n",
      "train D loss: 1.4126789569854736 G loss: 60.387638092041016\n",
      "val MSE loss: 0.09721352905035019\n",
      "val MSE loss: 0.09672356396913528\n",
      "val MSE loss: 0.09641734510660172\n",
      "train D loss: 1.4121390581130981 G loss: 58.26944351196289\n",
      "val MSE loss: 0.09626199305057526\n",
      "val MSE loss: 0.09536764770746231\n",
      "val MSE loss: 0.09510329365730286\n",
      "train D loss: 1.409848928451538 G loss: 57.826927185058594\n",
      "val MSE loss: 0.09542179107666016\n",
      "val MSE loss: 0.09499914199113846\n",
      "val MSE loss: 0.0940871387720108\n",
      "train D loss: 1.4125471115112305 G loss: 59.18988800048828\n",
      "val MSE loss: 0.09435339272022247\n",
      "val MSE loss: 0.09324612468481064\n",
      "val MSE loss: 0.09344366192817688\n",
      "train D loss: 1.4101932048797607 G loss: 58.73734664916992\n",
      "val MSE loss: 0.09271438419818878\n",
      "val MSE loss: 0.09212655574083328\n",
      "val MSE loss: 0.0909704864025116\n",
      "train D loss: 1.4118093252182007 G loss: 59.85041427612305\n",
      "val MSE loss: 0.09045516699552536\n",
      "val MSE loss: 0.09049908816814423\n",
      "val MSE loss: 0.08987445384263992\n",
      "train D loss: 1.4121747016906738 G loss: 56.93977737426758\n",
      "val MSE loss: 0.08936063945293427\n",
      "val MSE loss: 0.08855117112398148\n",
      "val MSE loss: 0.08812892436981201\n",
      "train D loss: 1.4121896028518677 G loss: 58.744972229003906\n",
      "val MSE loss: 0.08839402347803116\n",
      "val MSE loss: 0.08724134415388107\n",
      "val MSE loss: 0.08651827275753021\n",
      "train D loss: 1.4128345251083374 G loss: 59.15636444091797\n",
      "val MSE loss: 0.08662743121385574\n",
      "val MSE loss: 0.0860801488161087\n",
      "val MSE loss: 0.08547203242778778\n",
      "train D loss: 1.4117112159729004 G loss: 58.54331970214844\n",
      "val MSE loss: 0.08480725437402725\n",
      "val MSE loss: 0.0846463218331337\n",
      "val MSE loss: 0.08404850214719772\n",
      "train D loss: 1.4126946926116943 G loss: 57.08055114746094\n",
      "val MSE loss: 0.08382222801446915\n",
      "val MSE loss: 0.08326330780982971\n",
      "val MSE loss: 0.08292301744222641\n",
      "train D loss: 1.4124090671539307 G loss: 55.83041000366211\n",
      "val MSE loss: 0.08301636576652527\n",
      "val MSE loss: 0.08256325125694275\n",
      "val MSE loss: 0.08269066363573074\n",
      "train D loss: 1.4136035442352295 G loss: 58.769813537597656\n",
      "val MSE loss: 0.0818474218249321\n",
      "val MSE loss: 0.08183068782091141\n",
      "val MSE loss: 0.08185981959104538\n",
      "train D loss: 1.4106547832489014 G loss: 59.15180587768555\n",
      "val MSE loss: 0.08152991533279419\n",
      "val MSE loss: 0.08094127476215363\n",
      "val MSE loss: 0.08059102296829224\n",
      "train D loss: 1.4106581211090088 G loss: 54.42068099975586\n",
      "val MSE loss: 0.08085877448320389\n",
      "val MSE loss: 0.08018527925014496\n",
      "val MSE loss: 0.07985981553792953\n",
      "train D loss: 1.4127205610275269 G loss: 56.92176055908203\n",
      "val MSE loss: 0.07954265177249908\n",
      "val MSE loss: 0.0792294591665268\n",
      "val MSE loss: 0.07887496054172516\n",
      "train D loss: 1.413888931274414 G loss: 58.396846771240234\n",
      "val MSE loss: 0.07849185168743134\n",
      "val MSE loss: 0.07831434905529022\n",
      "val MSE loss: 0.07765491306781769\n",
      "train D loss: 1.4138747453689575 G loss: 55.371849060058594\n",
      "val MSE loss: 0.07725751399993896\n",
      "val MSE loss: 0.07675894349813461\n",
      "val MSE loss: 0.07667648792266846\n",
      "train D loss: 1.4134612083435059 G loss: 54.95415496826172\n",
      "val MSE loss: 0.07637156546115875\n",
      "val MSE loss: 0.07579108327627182\n",
      "val MSE loss: 0.0749676376581192\n",
      "train D loss: 1.4132035970687866 G loss: 56.5953369140625\n",
      "val MSE loss: 0.07523821294307709\n",
      "val MSE loss: 0.07488318532705307\n",
      "val MSE loss: 0.07415745407342911\n",
      "train D loss: 1.4156259298324585 G loss: 54.05535888671875\n",
      "val MSE loss: 0.07450126111507416\n",
      "val MSE loss: 0.07355349510908127\n",
      "val MSE loss: 0.07319658994674683\n",
      "train D loss: 1.4144692420959473 G loss: 31.506160736083984\n",
      "epoch: 4\n",
      "val MSE loss: 0.0726272463798523\n",
      "val MSE loss: 0.07233366370201111\n",
      "val MSE loss: 0.07221096754074097\n",
      "train D loss: 1.413542628288269 G loss: 57.1159553527832\n",
      "val MSE loss: 0.07212977856397629\n",
      "val MSE loss: 0.07202500104904175\n",
      "val MSE loss: 0.07167679071426392\n",
      "train D loss: 1.4148712158203125 G loss: 57.069454193115234\n",
      "val MSE loss: 0.07105325162410736\n",
      "val MSE loss: 0.07059327512979507\n",
      "val MSE loss: 0.07060737907886505\n",
      "train D loss: 1.4130949974060059 G loss: 54.34798812866211\n",
      "val MSE loss: 0.06963733583688736\n",
      "val MSE loss: 0.06965195387601852\n",
      "val MSE loss: 0.06958192586898804\n",
      "train D loss: 1.4140167236328125 G loss: 56.39345932006836\n",
      "val MSE loss: 0.06903412938117981\n",
      "val MSE loss: 0.06939833611249924\n",
      "val MSE loss: 0.06861073523759842\n",
      "train D loss: 1.4166245460510254 G loss: 56.43684387207031\n",
      "val MSE loss: 0.06859986484050751\n",
      "val MSE loss: 0.06857024878263474\n",
      "val MSE loss: 0.06854383647441864\n",
      "train D loss: 1.4139842987060547 G loss: 53.322025299072266\n",
      "val MSE loss: 0.06785337626934052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val MSE loss: 0.06785819679498672\n",
      "val MSE loss: 0.06740575283765793\n",
      "train D loss: 1.4146966934204102 G loss: 55.08308410644531\n",
      "val MSE loss: 0.06759887933731079\n",
      "val MSE loss: 0.06724801659584045\n",
      "val MSE loss: 0.06702128797769547\n",
      "train D loss: 1.4154908657073975 G loss: 54.66410446166992\n",
      "val MSE loss: 0.0668184831738472\n",
      "val MSE loss: 0.0665091872215271\n",
      "val MSE loss: 0.06624428182840347\n",
      "train D loss: 1.4127521514892578 G loss: 55.24779510498047\n",
      "val MSE loss: 0.06604351103305817\n",
      "val MSE loss: 0.06596056371927261\n",
      "val MSE loss: 0.06547468155622482\n",
      "train D loss: 1.4144699573516846 G loss: 54.10057067871094\n",
      "val MSE loss: 0.06541287899017334\n",
      "val MSE loss: 0.06475550681352615\n",
      "val MSE loss: 0.06499890983104706\n",
      "train D loss: 1.413578987121582 G loss: 55.931304931640625\n",
      "val MSE loss: 0.0646093413233757\n",
      "val MSE loss: 0.06412123888731003\n",
      "val MSE loss: 0.06348903477191925\n",
      "train D loss: 1.417458415031433 G loss: 51.91891860961914\n",
      "val MSE loss: 0.06376224011182785\n",
      "val MSE loss: 0.0636594370007515\n",
      "val MSE loss: 0.06305930018424988\n",
      "train D loss: 1.4161884784698486 G loss: 55.2824592590332\n",
      "val MSE loss: 0.06301869451999664\n",
      "val MSE loss: 0.062450237572193146\n",
      "val MSE loss: 0.062436483800411224\n",
      "train D loss: 1.4182908535003662 G loss: 56.58395004272461\n",
      "val MSE loss: 0.06219802424311638\n",
      "val MSE loss: 0.06218275800347328\n",
      "val MSE loss: 0.06188518926501274\n",
      "train D loss: 1.4155430793762207 G loss: 53.254486083984375\n",
      "val MSE loss: 0.06157476827502251\n",
      "val MSE loss: 0.06156643107533455\n",
      "val MSE loss: 0.0612502247095108\n",
      "train D loss: 1.416184425354004 G loss: 52.728248596191406\n",
      "val MSE loss: 0.06087302789092064\n",
      "val MSE loss: 0.06083415821194649\n",
      "val MSE loss: 0.06082157418131828\n",
      "train D loss: 1.4165115356445312 G loss: 54.02126693725586\n",
      "val MSE loss: 0.06075332686305046\n",
      "val MSE loss: 0.05969667062163353\n",
      "val MSE loss: 0.05964542552828789\n",
      "train D loss: 1.41880202293396 G loss: 49.732933044433594\n",
      "val MSE loss: 0.05923886224627495\n",
      "val MSE loss: 0.05935683101415634\n",
      "val MSE loss: 0.058540765196084976\n",
      "train D loss: 1.418182134628296 G loss: 54.00244140625\n",
      "val MSE loss: 0.05879995599389076\n",
      "val MSE loss: 0.058388371020555496\n",
      "val MSE loss: 0.058073434978723526\n",
      "train D loss: 1.4162254333496094 G loss: 53.72098922729492\n",
      "val MSE loss: 0.058033350855112076\n",
      "val MSE loss: 0.057943955063819885\n",
      "val MSE loss: 0.05754448100924492\n",
      "train D loss: 1.4155734777450562 G loss: 55.48648452758789\n",
      "val MSE loss: 0.05757467821240425\n",
      "val MSE loss: 0.05765395238995552\n",
      "val MSE loss: 0.05694989114999771\n",
      "train D loss: 1.4137907028198242 G loss: 52.5902099609375\n",
      "val MSE loss: 0.05689354985952377\n",
      "val MSE loss: 0.05683675780892372\n",
      "val MSE loss: 0.05638730525970459\n",
      "train D loss: 1.416325330734253 G loss: 52.32102966308594\n",
      "val MSE loss: 0.056150518357753754\n",
      "val MSE loss: 0.05644344910979271\n",
      "val MSE loss: 0.05592232942581177\n",
      "train D loss: 1.4155020713806152 G loss: 30.253206253051758\n",
      "epoch: 5\n",
      "val MSE loss: 0.05564190819859505\n",
      "val MSE loss: 0.055424463003873825\n",
      "val MSE loss: 0.055164884775877\n",
      "train D loss: 1.4184330701828003 G loss: 48.69731521606445\n",
      "val MSE loss: 0.055048778653144836\n",
      "val MSE loss: 0.054806701838970184\n",
      "val MSE loss: 0.05477600172162056\n",
      "train D loss: 1.4208202362060547 G loss: 51.63376998901367\n",
      "val MSE loss: 0.0546780489385128\n",
      "val MSE loss: 0.05437254160642624\n",
      "val MSE loss: 0.05455852299928665\n",
      "train D loss: 1.418355941772461 G loss: 52.27660369873047\n",
      "val MSE loss: 0.05439242720603943\n",
      "val MSE loss: 0.05374234914779663\n",
      "val MSE loss: 0.05383831635117531\n",
      "train D loss: 1.4224255084991455 G loss: 53.780269622802734\n",
      "val MSE loss: 0.05393587425351143\n",
      "val MSE loss: 0.05419514328241348\n",
      "val MSE loss: 0.053009308874607086\n",
      "train D loss: 1.4193979501724243 G loss: 52.09235382080078\n",
      "val MSE loss: 0.053177446126937866\n",
      "val MSE loss: 0.05303015187382698\n",
      "val MSE loss: 0.05269907787442207\n",
      "train D loss: 1.4168342351913452 G loss: 51.5307502746582\n",
      "val MSE loss: 0.053189683705568314\n",
      "val MSE loss: 0.052580300718545914\n",
      "val MSE loss: 0.05263795331120491\n",
      "train D loss: 1.4165916442871094 G loss: 51.68992614746094\n",
      "val MSE loss: 0.05226787179708481\n",
      "val MSE loss: 0.052587736397981644\n",
      "val MSE loss: 0.05181534215807915\n",
      "train D loss: 1.418339729309082 G loss: 52.12623596191406\n",
      "val MSE loss: 0.05206143856048584\n",
      "val MSE loss: 0.052203476428985596\n",
      "val MSE loss: 0.05189569666981697\n",
      "train D loss: 1.4161416292190552 G loss: 51.624813079833984\n",
      "val MSE loss: 0.051975399255752563\n",
      "val MSE loss: 0.051885176450014114\n",
      "val MSE loss: 0.05177909880876541\n",
      "train D loss: 1.4181277751922607 G loss: 49.6080207824707\n",
      "val MSE loss: 0.05156196653842926\n",
      "val MSE loss: 0.05177265405654907\n",
      "val MSE loss: 0.051235802471637726\n",
      "train D loss: 1.4215366840362549 G loss: 49.223262786865234\n",
      "val MSE loss: 0.05140352249145508\n",
      "val MSE loss: 0.05141111835837364\n",
      "val MSE loss: 0.0512922927737236\n",
      "train D loss: 1.420326590538025 G loss: 48.80722427368164\n",
      "val MSE loss: 0.051217421889305115\n",
      "val MSE loss: 0.05097723752260208\n",
      "val MSE loss: 0.05094475671648979\n",
      "train D loss: 1.4221880435943604 G loss: 49.5633544921875\n",
      "val MSE loss: 0.05044092237949371\n",
      "val MSE loss: 0.05048501491546631\n",
      "val MSE loss: 0.0504583902657032\n",
      "train D loss: 1.4216774702072144 G loss: 49.350502014160156\n",
      "val MSE loss: 0.0499897263944149\n",
      "val MSE loss: 0.049815911799669266\n",
      "val MSE loss: 0.04975080490112305\n",
      "train D loss: 1.4212943315505981 G loss: 51.49730682373047\n",
      "val MSE loss: 0.049524590373039246\n",
      "val MSE loss: 0.04965813830494881\n",
      "val MSE loss: 0.049693625420331955\n",
      "train D loss: 1.4174824953079224 G loss: 47.60303497314453\n",
      "val MSE loss: 0.04967154935002327\n",
      "val MSE loss: 0.049225885421037674\n",
      "val MSE loss: 0.04948537051677704\n",
      "train D loss: 1.4182782173156738 G loss: 49.20545196533203\n",
      "val MSE loss: 0.04923323169350624\n",
      "val MSE loss: 0.04995983466506004\n",
      "val MSE loss: 0.049162689596414566\n",
      "train D loss: 1.421683430671692 G loss: 48.46300506591797\n",
      "val MSE loss: 0.04900737479329109\n",
      "val MSE loss: 0.04949512332677841\n",
      "val MSE loss: 0.04930426925420761\n",
      "train D loss: 1.4226295948028564 G loss: 48.02073287963867\n",
      "val MSE loss: 0.04882124811410904\n",
      "val MSE loss: 0.04888467490673065\n",
      "val MSE loss: 0.049105312675237656\n",
      "train D loss: 1.4219841957092285 G loss: 49.52573776245117\n",
      "val MSE loss: 0.04868724197149277\n",
      "val MSE loss: 0.04880710691213608\n",
      "val MSE loss: 0.048228342086076736\n",
      "train D loss: 1.4215068817138672 G loss: 45.815460205078125\n",
      "val MSE loss: 0.048783641308546066\n",
      "val MSE loss: 0.04850061610341072\n",
      "val MSE loss: 0.04856158792972565\n",
      "train D loss: 1.4177165031433105 G loss: 49.98181915283203\n",
      "val MSE loss: 0.04855136200785637\n",
      "val MSE loss: 0.048215851187705994\n",
      "val MSE loss: 0.04808039963245392\n",
      "train D loss: 1.4236104488372803 G loss: 47.78694152832031\n",
      "val MSE loss: 0.04822723567485809\n",
      "val MSE loss: 0.047892071306705475\n",
      "val MSE loss: 0.04831929877400398\n",
      "train D loss: 1.4217184782028198 G loss: 24.299633026123047\n",
      "epoch: 6\n",
      "val MSE loss: 0.048071738332509995\n",
      "val MSE loss: 0.04812227562069893\n",
      "val MSE loss: 0.04844560846686363\n",
      "train D loss: 1.4202675819396973 G loss: 44.18407440185547\n",
      "val MSE loss: 0.047798339277505875\n",
      "val MSE loss: 0.04790787771344185\n",
      "val MSE loss: 0.04801960662007332\n",
      "train D loss: 1.4212876558303833 G loss: 46.41538619995117\n",
      "val MSE loss: 0.04726310819387436\n",
      "val MSE loss: 0.047351762652397156\n",
      "val MSE loss: 0.04738137125968933\n",
      "train D loss: 1.4235575199127197 G loss: 45.53759765625\n",
      "val MSE loss: 0.04719773307442665\n",
      "val MSE loss: 0.047516632825136185\n",
      "val MSE loss: 0.04715894162654877\n",
      "train D loss: 1.4216346740722656 G loss: 46.74217224121094\n",
      "val MSE loss: 0.04751915857195854\n",
      "val MSE loss: 0.047312479466199875\n",
      "val MSE loss: 0.04701761528849602\n",
      "train D loss: 1.423002004623413 G loss: 44.28889465332031\n",
      "val MSE loss: 0.046942777931690216\n",
      "val MSE loss: 0.04689496383070946\n",
      "val MSE loss: 0.04743683710694313\n",
      "train D loss: 1.4235146045684814 G loss: 43.106605529785156\n",
      "val MSE loss: 0.04696659743785858\n",
      "val MSE loss: 0.047186173498630524\n",
      "val MSE loss: 0.04746088758111\n",
      "train D loss: 1.4235255718231201 G loss: 45.346961975097656\n",
      "val MSE loss: 0.04725642874836922\n",
      "val MSE loss: 0.047042734920978546\n",
      "val MSE loss: 0.0472119078040123\n",
      "train D loss: 1.4199739694595337 G loss: 44.85862731933594\n",
      "val MSE loss: 0.047426000237464905\n",
      "val MSE loss: 0.04735339432954788\n",
      "val MSE loss: 0.047242797911167145\n",
      "train D loss: 1.4227148294448853 G loss: 48.63803482055664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val MSE loss: 0.04714329168200493\n",
      "val MSE loss: 0.046994760632514954\n",
      "val MSE loss: 0.04706708714365959\n",
      "train D loss: 1.4242143630981445 G loss: 41.986080169677734\n",
      "val MSE loss: 0.046765267848968506\n",
      "val MSE loss: 0.04669083654880524\n",
      "val MSE loss: 0.04664085805416107\n",
      "train D loss: 1.4224684238433838 G loss: 44.705509185791016\n",
      "val MSE loss: 0.04680401086807251\n",
      "val MSE loss: 0.04646476358175278\n",
      "val MSE loss: 0.04611187055706978\n",
      "train D loss: 1.4207912683486938 G loss: 44.8004150390625\n",
      "val MSE loss: 0.046719085425138474\n",
      "val MSE loss: 0.04650105908513069\n",
      "val MSE loss: 0.04594433680176735\n",
      "train D loss: 1.4184770584106445 G loss: 45.00968551635742\n",
      "val MSE loss: 0.04622650146484375\n",
      "val MSE loss: 0.04661353677511215\n",
      "val MSE loss: 0.04613933712244034\n",
      "train D loss: 1.4216070175170898 G loss: 45.58412551879883\n",
      "val MSE loss: 0.0467674545943737\n",
      "val MSE loss: 0.046308424323797226\n",
      "val MSE loss: 0.046379782259464264\n",
      "train D loss: 1.4234561920166016 G loss: 43.953243255615234\n",
      "val MSE loss: 0.04632014036178589\n",
      "val MSE loss: 0.04633311554789543\n",
      "val MSE loss: 0.04676371440291405\n",
      "train D loss: 1.4229707717895508 G loss: 44.342864990234375\n",
      "val MSE loss: 0.04625028371810913\n",
      "val MSE loss: 0.0461273230612278\n",
      "val MSE loss: 0.04633829742670059\n",
      "train D loss: 1.4233028888702393 G loss: 42.38124084472656\n",
      "val MSE loss: 0.04636239632964134\n",
      "val MSE loss: 0.046377506107091904\n",
      "val MSE loss: 0.046081192791461945\n",
      "train D loss: 1.4229040145874023 G loss: 40.847412109375\n",
      "val MSE loss: 0.045932020992040634\n",
      "val MSE loss: 0.04568016529083252\n",
      "val MSE loss: 0.04588279128074646\n",
      "train D loss: 1.4234064817428589 G loss: 40.73048782348633\n",
      "val MSE loss: 0.04645879194140434\n",
      "val MSE loss: 0.04583142697811127\n",
      "val MSE loss: 0.045868050307035446\n",
      "train D loss: 1.4247910976409912 G loss: 41.57438278198242\n",
      "val MSE loss: 0.045071955770254135\n",
      "val MSE loss: 0.04544084519147873\n",
      "val MSE loss: 0.04540439695119858\n",
      "train D loss: 1.4215319156646729 G loss: 43.24692153930664\n",
      "val MSE loss: 0.04542669653892517\n",
      "val MSE loss: 0.04534601792693138\n",
      "val MSE loss: 0.04555654898285866\n",
      "train D loss: 1.4222111701965332 G loss: 39.992515563964844\n",
      "val MSE loss: 0.045181144028902054\n",
      "val MSE loss: 0.04534341022372246\n",
      "val MSE loss: 0.04512166976928711\n",
      "train D loss: 1.423705816268921 G loss: 42.32364273071289\n",
      "val MSE loss: 0.04508661851286888\n",
      "val MSE loss: 0.04499257728457451\n",
      "val MSE loss: 0.045030612498521805\n",
      "train D loss: 1.4216290712356567 G loss: 24.72051239013672\n",
      "epoch: 7\n",
      "val MSE loss: 0.04507875442504883\n",
      "val MSE loss: 0.04515164718031883\n",
      "val MSE loss: 0.044879455119371414\n",
      "train D loss: 1.4212985038757324 G loss: 41.73020935058594\n",
      "val MSE loss: 0.044679585844278336\n",
      "val MSE loss: 0.04512736573815346\n",
      "val MSE loss: 0.04517369717359543\n",
      "train D loss: 1.4228875637054443 G loss: 37.22087860107422\n",
      "val MSE loss: 0.04501562938094139\n",
      "val MSE loss: 0.04457423463463783\n",
      "val MSE loss: 0.044554468244314194\n",
      "train D loss: 1.4212977886199951 G loss: 39.99560546875\n",
      "val MSE loss: 0.04483969137072563\n",
      "val MSE loss: 0.04482611268758774\n",
      "val MSE loss: 0.044746723026037216\n",
      "train D loss: 1.424013614654541 G loss: 40.82451629638672\n",
      "val MSE loss: 0.04477927088737488\n",
      "val MSE loss: 0.04484604299068451\n",
      "val MSE loss: 0.04477519169449806\n",
      "train D loss: 1.4210796356201172 G loss: 42.315086364746094\n",
      "val MSE loss: 0.045299600809812546\n",
      "val MSE loss: 0.04450454190373421\n",
      "val MSE loss: 0.04440554231405258\n",
      "train D loss: 1.4254162311553955 G loss: 39.82576370239258\n",
      "val MSE loss: 0.04469013586640358\n",
      "val MSE loss: 0.04462822526693344\n",
      "val MSE loss: 0.04428666830062866\n",
      "train D loss: 1.4239155054092407 G loss: 40.79641342163086\n",
      "val MSE loss: 0.044326599687337875\n",
      "val MSE loss: 0.04463987052440643\n",
      "val MSE loss: 0.04416739568114281\n",
      "train D loss: 1.4233198165893555 G loss: 40.13938903808594\n",
      "val MSE loss: 0.044274624437093735\n",
      "val MSE loss: 0.04429351165890694\n",
      "val MSE loss: 0.04390786588191986\n",
      "train D loss: 1.4237408638000488 G loss: 39.13554382324219\n",
      "val MSE loss: 0.04393044114112854\n",
      "val MSE loss: 0.04385744780302048\n",
      "val MSE loss: 0.04390899837017059\n",
      "train D loss: 1.4231395721435547 G loss: 36.56209182739258\n",
      "val MSE loss: 0.04403208941221237\n",
      "val MSE loss: 0.0439131073653698\n",
      "val MSE loss: 0.04376120865345001\n",
      "train D loss: 1.4218919277191162 G loss: 42.41168212890625\n",
      "val MSE loss: 0.04374806210398674\n",
      "val MSE loss: 0.04371891915798187\n",
      "val MSE loss: 0.04364992305636406\n",
      "train D loss: 1.421651840209961 G loss: 39.705352783203125\n",
      "val MSE loss: 0.043740566819906235\n",
      "val MSE loss: 0.043673060834407806\n",
      "val MSE loss: 0.04386992007493973\n",
      "train D loss: 1.4219213724136353 G loss: 39.63642883300781\n",
      "val MSE loss: 0.04364921152591705\n",
      "val MSE loss: 0.0432845801115036\n",
      "val MSE loss: 0.043262381106615067\n",
      "train D loss: 1.420297384262085 G loss: 40.48359298706055\n",
      "val MSE loss: 0.04315841570496559\n",
      "val MSE loss: 0.04374551773071289\n",
      "val MSE loss: 0.04325832054018974\n",
      "train D loss: 1.4214508533477783 G loss: 37.03217697143555\n",
      "val MSE loss: 0.043472085148096085\n",
      "val MSE loss: 0.04353560507297516\n",
      "val MSE loss: 0.0434221550822258\n",
      "train D loss: 1.4220513105392456 G loss: 39.43134307861328\n",
      "val MSE loss: 0.04338483512401581\n",
      "val MSE loss: 0.0432906448841095\n",
      "val MSE loss: 0.04332449287176132\n",
      "train D loss: 1.422297477722168 G loss: 36.366580963134766\n",
      "val MSE loss: 0.0436406247317791\n",
      "val MSE loss: 0.04348371550440788\n",
      "val MSE loss: 0.043327342718839645\n",
      "train D loss: 1.425180435180664 G loss: 38.13459014892578\n",
      "val MSE loss: 0.04312290996313095\n",
      "val MSE loss: 0.04307412728667259\n",
      "val MSE loss: 0.04343527555465698\n",
      "train D loss: 1.4237347841262817 G loss: 37.32660675048828\n",
      "val MSE loss: 0.04284856095910072\n",
      "val MSE loss: 0.04324977099895477\n",
      "val MSE loss: 0.0429733581840992\n",
      "train D loss: 1.4235904216766357 G loss: 35.968502044677734\n",
      "val MSE loss: 0.0429505780339241\n",
      "val MSE loss: 0.04291316121816635\n",
      "val MSE loss: 0.043139535933732986\n",
      "train D loss: 1.4240097999572754 G loss: 37.85470962524414\n",
      "val MSE loss: 0.04267566278576851\n",
      "val MSE loss: 0.04253080487251282\n",
      "val MSE loss: 0.04260100796818733\n",
      "train D loss: 1.4225749969482422 G loss: 37.7874870300293\n",
      "val MSE loss: 0.04257681965827942\n",
      "val MSE loss: 0.04235883429646492\n",
      "val MSE loss: 0.042585697025060654\n",
      "train D loss: 1.4208894968032837 G loss: 35.41472244262695\n",
      "val MSE loss: 0.04270634055137634\n",
      "val MSE loss: 0.042002469301223755\n",
      "val MSE loss: 0.0420294851064682\n",
      "train D loss: 1.4227678775787354 G loss: 20.005016326904297\n",
      "epoch: 8\n",
      "val MSE loss: 0.0421086847782135\n",
      "val MSE loss: 0.04213646054267883\n",
      "val MSE loss: 0.04197130352258682\n",
      "train D loss: 1.4201024770736694 G loss: 38.46012496948242\n",
      "val MSE loss: 0.042110566049814224\n",
      "val MSE loss: 0.041785869747400284\n",
      "val MSE loss: 0.04217459633946419\n",
      "train D loss: 1.4219872951507568 G loss: 37.16753005981445\n",
      "val MSE loss: 0.041894324123859406\n",
      "val MSE loss: 0.0420793779194355\n",
      "val MSE loss: 0.0419604554772377\n",
      "train D loss: 1.42266845703125 G loss: 35.10934066772461\n",
      "val MSE loss: 0.041957009583711624\n",
      "val MSE loss: 0.04168308898806572\n",
      "val MSE loss: 0.04185410216450691\n",
      "train D loss: 1.4228202104568481 G loss: 37.47803497314453\n",
      "val MSE loss: 0.04173409566283226\n",
      "val MSE loss: 0.04202156141400337\n",
      "val MSE loss: 0.041674382984638214\n",
      "train D loss: 1.4212522506713867 G loss: 35.93063735961914\n",
      "val MSE loss: 0.04166927933692932\n",
      "val MSE loss: 0.041480548679828644\n",
      "val MSE loss: 0.04181477427482605\n",
      "train D loss: 1.4236559867858887 G loss: 36.35637283325195\n",
      "val MSE loss: 0.04136742651462555\n",
      "val MSE loss: 0.041145842522382736\n",
      "val MSE loss: 0.04132113233208656\n",
      "train D loss: 1.4246811866760254 G loss: 35.19808578491211\n",
      "val MSE loss: 0.04121715947985649\n",
      "val MSE loss: 0.04091859608888626\n",
      "val MSE loss: 0.04104185104370117\n",
      "train D loss: 1.422389268875122 G loss: 35.81878662109375\n",
      "val MSE loss: 0.04092324897646904\n",
      "val MSE loss: 0.040859147906303406\n",
      "val MSE loss: 0.04098238795995712\n",
      "train D loss: 1.4218015670776367 G loss: 35.995025634765625\n",
      "val MSE loss: 0.040824804455041885\n",
      "val MSE loss: 0.04074868559837341\n",
      "val MSE loss: 0.04096318408846855\n",
      "train D loss: 1.4213134050369263 G loss: 34.5499382019043\n",
      "val MSE loss: 0.04093313217163086\n",
      "val MSE loss: 0.04088658466935158\n",
      "val MSE loss: 0.0405828021466732\n",
      "train D loss: 1.4219452142715454 G loss: 34.08639907836914\n",
      "val MSE loss: 0.04080704227089882\n",
      "val MSE loss: 0.04059600830078125\n",
      "val MSE loss: 0.040557779371738434\n",
      "train D loss: 1.4225738048553467 G loss: 33.70689392089844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val MSE loss: 0.040934208780527115\n",
      "val MSE loss: 0.040640171617269516\n",
      "val MSE loss: 0.04044407978653908\n",
      "train D loss: 1.423866868019104 G loss: 35.75636291503906\n",
      "val MSE loss: 0.0406557060778141\n",
      "val MSE loss: 0.04060063883662224\n",
      "val MSE loss: 0.04069562628865242\n",
      "train D loss: 1.4217021465301514 G loss: 31.998456954956055\n",
      "val MSE loss: 0.040455203503370285\n",
      "val MSE loss: 0.0406210720539093\n",
      "val MSE loss: 0.04018067196011543\n",
      "train D loss: 1.421972393989563 G loss: 35.9853401184082\n",
      "val MSE loss: 0.0403856597840786\n",
      "val MSE loss: 0.04042508453130722\n",
      "val MSE loss: 0.04019646346569061\n",
      "train D loss: 1.4221103191375732 G loss: 33.228782653808594\n",
      "val MSE loss: 0.04028748348355293\n",
      "val MSE loss: 0.04041828215122223\n",
      "val MSE loss: 0.04036225751042366\n",
      "train D loss: 1.4203804731369019 G loss: 32.69038772583008\n",
      "val MSE loss: 0.04044921323657036\n",
      "val MSE loss: 0.04037968069314957\n",
      "val MSE loss: 0.040409598499536514\n",
      "train D loss: 1.4214246273040771 G loss: 30.152952194213867\n",
      "val MSE loss: 0.04042986407876015\n",
      "val MSE loss: 0.04038822278380394\n",
      "val MSE loss: 0.040225498378276825\n",
      "train D loss: 1.4227021932601929 G loss: 34.693878173828125\n",
      "val MSE loss: 0.04054073989391327\n",
      "val MSE loss: 0.04023243859410286\n",
      "val MSE loss: 0.04021480306982994\n",
      "train D loss: 1.4243147373199463 G loss: 34.07892990112305\n",
      "val MSE loss: 0.04014220088720322\n",
      "val MSE loss: 0.03988311439752579\n",
      "val MSE loss: 0.03968038782477379\n",
      "train D loss: 1.4215415716171265 G loss: 30.309608459472656\n",
      "val MSE loss: 0.04002305120229721\n",
      "val MSE loss: 0.040256477892398834\n",
      "val MSE loss: 0.0399492047727108\n",
      "train D loss: 1.4212181568145752 G loss: 31.889034271240234\n",
      "val MSE loss: 0.03966047614812851\n",
      "val MSE loss: 0.039654821157455444\n",
      "val MSE loss: 0.03970097377896309\n",
      "train D loss: 1.4196364879608154 G loss: 31.744625091552734\n",
      "val MSE loss: 0.03971178084611893\n",
      "val MSE loss: 0.03987951576709747\n",
      "val MSE loss: 0.04018476977944374\n",
      "train D loss: 1.4192754030227661 G loss: 18.715076446533203\n",
      "epoch: 9\n",
      "val MSE loss: 0.03984770178794861\n",
      "val MSE loss: 0.03986489400267601\n",
      "val MSE loss: 0.04006306827068329\n",
      "train D loss: 1.4228713512420654 G loss: 34.476646423339844\n",
      "val MSE loss: 0.04009599983692169\n",
      "val MSE loss: 0.0396803542971611\n",
      "val MSE loss: 0.03965750336647034\n",
      "train D loss: 1.4220194816589355 G loss: 29.48228645324707\n",
      "val MSE loss: 0.03972388803958893\n",
      "val MSE loss: 0.03965596482157707\n",
      "val MSE loss: 0.03968087211251259\n",
      "train D loss: 1.4208621978759766 G loss: 33.43478775024414\n",
      "val MSE loss: 0.03941396623849869\n",
      "val MSE loss: 0.03949621692299843\n",
      "val MSE loss: 0.03943542018532753\n",
      "train D loss: 1.4208409786224365 G loss: 31.582063674926758\n",
      "val MSE loss: 0.039496392011642456\n",
      "val MSE loss: 0.039536554366350174\n",
      "val MSE loss: 0.03947804123163223\n",
      "train D loss: 1.422252893447876 G loss: 30.752758026123047\n",
      "val MSE loss: 0.039355095475912094\n",
      "val MSE loss: 0.03925105184316635\n",
      "val MSE loss: 0.03937479481101036\n",
      "train D loss: 1.4194475412368774 G loss: 30.62349510192871\n",
      "val MSE loss: 0.039140354841947556\n",
      "val MSE loss: 0.03922249749302864\n",
      "val MSE loss: 0.039341237396001816\n",
      "train D loss: 1.4207100868225098 G loss: 30.519939422607422\n",
      "val MSE loss: 0.039221346378326416\n",
      "val MSE loss: 0.03938526287674904\n",
      "val MSE loss: 0.0394674688577652\n",
      "train D loss: 1.4219406843185425 G loss: 33.07020568847656\n",
      "val MSE loss: 0.03928834944963455\n",
      "val MSE loss: 0.03930665925145149\n",
      "val MSE loss: 0.039420027285814285\n",
      "train D loss: 1.4223294258117676 G loss: 32.05461883544922\n",
      "val MSE loss: 0.03920412063598633\n",
      "val MSE loss: 0.03930236026644707\n",
      "val MSE loss: 0.03896581009030342\n",
      "train D loss: 1.4243069887161255 G loss: 30.6248836517334\n",
      "val MSE loss: 0.038839615881443024\n",
      "val MSE loss: 0.039025600999593735\n",
      "val MSE loss: 0.039060112088918686\n",
      "train D loss: 1.4228267669677734 G loss: 29.251100540161133\n",
      "val MSE loss: 0.039030205458402634\n",
      "val MSE loss: 0.03905188664793968\n",
      "val MSE loss: 0.03903866931796074\n",
      "train D loss: 1.420626163482666 G loss: 30.04828643798828\n",
      "val MSE loss: 0.03867366909980774\n",
      "val MSE loss: 0.038551293313503265\n",
      "val MSE loss: 0.03894059360027313\n",
      "train D loss: 1.4206995964050293 G loss: 30.194416046142578\n",
      "val MSE loss: 0.038800738751888275\n",
      "val MSE loss: 0.038541022688150406\n",
      "val MSE loss: 0.038785457611083984\n",
      "train D loss: 1.4238948822021484 G loss: 27.91901397705078\n",
      "val MSE loss: 0.03873816505074501\n",
      "val MSE loss: 0.038692113012075424\n",
      "val MSE loss: 0.038738880306482315\n",
      "train D loss: 1.4236705303192139 G loss: 29.758209228515625\n",
      "val MSE loss: 0.038763027638196945\n",
      "val MSE loss: 0.03860596939921379\n",
      "val MSE loss: 0.038800425827503204\n",
      "train D loss: 1.4195878505706787 G loss: 30.378536224365234\n",
      "val MSE loss: 0.038663771003484726\n",
      "val MSE loss: 0.038872890174388885\n",
      "val MSE loss: 0.03855583071708679\n",
      "train D loss: 1.4196877479553223 G loss: 29.329309463500977\n",
      "val MSE loss: 0.038567956537008286\n",
      "val MSE loss: 0.03866036981344223\n",
      "val MSE loss: 0.038569483906030655\n",
      "train D loss: 1.4214690923690796 G loss: 29.337968826293945\n",
      "val MSE loss: 0.038609836250543594\n",
      "val MSE loss: 0.0386035293340683\n",
      "val MSE loss: 0.038436274975538254\n",
      "train D loss: 1.4194071292877197 G loss: 30.8990478515625\n",
      "val MSE loss: 0.038268912583589554\n",
      "val MSE loss: 0.038559604436159134\n",
      "val MSE loss: 0.0387568362057209\n",
      "train D loss: 1.4205913543701172 G loss: 29.153594970703125\n",
      "val MSE loss: 0.0382167212665081\n",
      "val MSE loss: 0.03859954699873924\n",
      "val MSE loss: 0.03840932622551918\n",
      "train D loss: 1.4209604263305664 G loss: 29.638948440551758\n",
      "val MSE loss: 0.038574498146772385\n",
      "val MSE loss: 0.03848104923963547\n",
      "val MSE loss: 0.03850356116890907\n",
      "train D loss: 1.4225611686706543 G loss: 26.683212280273438\n",
      "val MSE loss: 0.03820405155420303\n",
      "val MSE loss: 0.038190703839063644\n",
      "val MSE loss: 0.038204554468393326\n",
      "train D loss: 1.4200633764266968 G loss: 27.091018676757812\n",
      "val MSE loss: 0.03828299418091774\n",
      "val MSE loss: 0.03818897902965546\n",
      "val MSE loss: 0.038310032337903976\n",
      "train D loss: 1.4213958978652954 G loss: 16.09013557434082\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print('epoch:',epoch)\n",
    "    for step , (batch_x,batch_y) in enumerate(loader):\n",
    "        # 训练判别器D，目的是能够区分G的输出和真实标签\n",
    "        for d in range(1):\n",
    "            D.train()\n",
    "            G.eval()\n",
    "            # 前向传播\n",
    "            z=torch.from_numpy(np.random.randn(batch_x.shape[0], 10)).float()   # 随机噪声\n",
    "            d_real = D(batch_y)     #  （标签）输入判别器的结果\n",
    "            batch_y_pred = G(torch.cat([z, batch_x], dim=1))      #  （噪声+x）输入生成器，得到预测结果\n",
    "            d_gen = D(batch_y_pred)        #  （预测结果）输入判别器的结果\n",
    "            # 计算损失\n",
    "            Dloss_real = loss_func_bce(d_real, torch.ones((batch_x.shape[0],1))) # 对于（标签+x）组，判别器输出应趋向于全1\n",
    "            Dloss_gen = loss_func_bce(d_gen, torch.zeros((batch_x.shape[0],1)))  # 对于（预测结果+x）组，判别器输出应趋向于全0\n",
    "            Dloss = Dloss_real + Dloss_gen\n",
    "            # 反向传播（只对判别器参数进行更新）\n",
    "            Dloss.backward()\n",
    "            opt_d.step()\n",
    "            opt_d.zero_grad()\n",
    "            opt_g.zero_grad()\n",
    "        # 训练生成器G，目的是G的输出能够欺骗D，让D以为G的输出就是真实标签\n",
    "        for g in range(3):\n",
    "            D.eval()\n",
    "            G.train()\n",
    "            # 前向传播\n",
    "            z = torch.from_numpy(np.random.randn(batch_x.shape[0], 10)).float() # 随机噪声\n",
    "            batch_y_pred = G(torch.cat([z, batch_x], dim=1))\n",
    "            d_gen = D(batch_y_pred)  \n",
    "            # 计算损失函数\n",
    "            Gloss_adventure = 0.3 * loss_func_bce(d_gen, torch.ones((batch_x.shape[0],1)))   # G的目的是，让D以为它的输出就是真实标签，因此G趋向于让d_gen等于1\n",
    "            Gloss_regression = loss_func_reg(batch_y_pred,batch_y)\n",
    "            r_cty_pred, r_u1_pred, r_u2_pred = compute_resids(batch_x, batch_y_pred, basis_v, basis_p,scaler)\n",
    "            r_cty, r_u1, r_u2 = compute_resids(batch_x, batch_y, basis_v, basis_p,scaler)\n",
    "#             r_cty, r_u1, r_u2 = compute_resids(batch_y, batch_y_pred, basis_v, basis_p, scaler)\n",
    "            Gloss_res = torch.norm(r_cty_pred-r_cty)+torch.norm(r_u1_pred-r_u1)+torch.norm(r_u2_pred-r_u2)\n",
    "            Gloss = Gloss_regression + Gloss_adventure + lambda_res*Gloss_res\n",
    "            # 反向传播\n",
    "            Gloss.backward()\n",
    "            opt_g.step()\n",
    "            opt_g.zero_grad()\n",
    "            opt_d.zero_grad()\n",
    "            loss_train.append(Gloss_regression.item())\n",
    "            valid(G,x_test,loss_func_reg)\n",
    "        D.eval()\n",
    "        G.eval()\n",
    "        print('train D loss:', Dloss.detach().item(), 'G loss:', Gloss.detach().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce2b730f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAN_with_noise——测试集均方误差： 0.038310032337903976\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [i for i in range(len(loss_train))]    # 每一轮G都训练了3次，所以乘3\n",
    "plt.plot(x,loss_train, label='train')\n",
    "plt.plot(x,loss_val, label='val')\n",
    "plt.title('GAN_with_noise')\n",
    "plt.legend()\n",
    "plt.savefig(fname=\"results/GAN_with_noise.png\")\n",
    "np.save('results/GAN_with_noise.npy',loss_train)   # 保存为npy文件,供不同方法对比\n",
    "print('GAN_with_noise——测试集均方误差：',loss_val[-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a608e94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.eval()\n",
    "z = torch.from_numpy(np.random.randn(data.shape[0], 10)).float() # 随机噪声\n",
    "coeff_pred = G(torch.cat([z, data], dim=1))\n",
    "np.savetxt('./results/prediction_res_GAN_scaled.csv', coeff_pred.detach().numpy(), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc08c55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
