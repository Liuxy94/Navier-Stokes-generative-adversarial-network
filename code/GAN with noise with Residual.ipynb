{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdb2bb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch,copy,argparse,csv\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9ee220",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09df5500",
   "metadata": {},
   "outputs": [],
   "source": [
    "velocity_coeff = np.genfromtxt('./data/POD_coeffs_3900_new_grid_221_42_Velocity_1.csv', delimiter=',')\n",
    "pressure_coeff = np.genfromtxt('./data/POD_coeffs_3900_new_grid_221_42_Pressure_1.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f442139",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_all = torch.from_numpy(np.append(velocity_coeff,pressure_coeff,axis=0)).type(torch.FloatTensor)\n",
    "data = copy.deepcopy(coeff_all[:,:-1].T)\n",
    "label = copy.deepcopy(coeff_all[:,1:].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1e2f6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data,label, test_size=0.3 , random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb3d013",
   "metadata": {},
   "source": [
    "## Compute residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dde356cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "basis_v = torch.from_numpy(np.genfromtxt('./data/Velocity_basis.csv', delimiter=',')).type(torch.FloatTensor)\n",
    "basis_p = torch.from_numpy(np.genfromtxt('./data/Pressure_basis.csv', delimiter=',')).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bdfdb2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_p(coeff_p, basis_p):\n",
    "    length, number = coeff_p.shape\n",
    "    nx = 221\n",
    "    ny = 42\n",
    "    grid_p = torch.matmul(basis_p, coeff_p)\n",
    "    return grid_p.reshape(nx,ny,number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e47773e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_v(coeff_v, basis_v):\n",
    "    length, number = coeff_v.shape\n",
    "    nx = 221\n",
    "    ny = 42\n",
    "    grid_v = torch.matmul(basis_v, coeff_v)\n",
    "    grid_v1 = grid_v[0:nx*ny].reshape(nx,ny,number)\n",
    "    grid_v2 = grid_v[nx*ny:].reshape(nx,ny,number)\n",
    "    return grid_v1, grid_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52f9839c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(coeff, basis_v, basis_p):\n",
    "    nPOD = 10\n",
    "    coeff_v = coeff[:nPOD,:]\n",
    "    coeff_p = coeff[nPOD:,:]\n",
    "    v1, v2 = backward_v(coeff_v, basis_v)\n",
    "    p = backward_p(coeff_p, basis_p)\n",
    "    return v1, v2, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8fc63b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_resids(coeff, coeff_pred, basis_v, basis_p):\n",
    "    u1_o1d, u2_o1d, p = backward(coeff, basis_v, basis_p)\n",
    "    u1, u2, p = backward(coeff_pred, basis_v, basis_p)\n",
    "    nx, ny = 221,42\n",
    "    dx, dy = 0.01, 0.01\n",
    "    dt = 1.\n",
    "    sigma = 0.\n",
    "    rho = 1.\n",
    "    mu = 1./300\n",
    "    # first order term of u1\n",
    "    u1x = (u1[2:,1:-1] - u1[:-2,1:-1])/(2.0*dx)\n",
    "    u1y = (u1[1:-1,2:] - u1[1:-1,:-2])/(2.0*dy)\n",
    "#     u1t = (u1_new[1:-1, 1:-1] - u1 [1:-1, 1:-1] )/dt \n",
    "    u1t = (u1[1:-1, 1:-1] - u1_o1d[1:-1, 1:-1])/dt \n",
    "    # second order term of u1 \n",
    "    u1xx = (u1[2:,1:-1] - 2. *u1[1:-1, 1:-1] + u1[0: -2, 1:-1])/(dx**2) \n",
    "    u1yy = (u1[1:-1,2:] - 2. *u1[1:-1, 1:-1] + u1[1:-1, 0:-2])/(dy**2) \n",
    "    # first order term of u2\n",
    "    u2x = (u2[2:,1:-1] - u2[0:-2, 1:-1])/(2.0*dx) \n",
    "    u2y = (u2[1:-1, 2: ] - u2 [1:-1, 0:-2])/(2.0*dy) \n",
    "#     u2t = (u2_new[1:-1, 1:-1] - u2 [1:-1, 1:-1] )/dt \n",
    "    u2t = (u2[1:-1, 1:-1] - u2_o1d[1:-1, 1:-1] )/dt \n",
    "    # second order term of u2 \n",
    "    u2xx = (u2[2:,1:-1] - 2.*u2[1:-1, 1:-1] + u2[0:-2, 1:-1])/(dx**2) \n",
    "    u2yy = (u2[1:-1,2:] - 2.*u2[1:-1, 1:-1] + u2[1:-1, 0:-2])/(dy**2) \n",
    "    # first order of p \n",
    "    px = (p[2:,1:-1] - p[0:-2,1:-1])/(2.* dx)\n",
    "    py = (p[1:-1,2:] - p[1:-1,0:-2])/(2.* dy)\n",
    "    r_cty = u1x + u2y \n",
    "    r_u1 = rho*u1t + sigma*u1 [1:-1, 1:-1]+ rho* (u1 [1:-1, 1:-1] *u1x+u2 [1:-1, 1:-1] *u1y) - mu* (u1xx+u1yy) + px\n",
    "    r_u2 = rho*u2t + sigma*u2 [1: -1, 1: -1]+ rho* (u1 [1:-1, 1:-1] *u2x+u2 [1:-1, 1:-1] *u2y) - mu* (u2xx+u2yy) + py\n",
    "    return r_cty, r_u1, r_u2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20171daf",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b95939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ========================模型验证======================\n",
    "def valid(model,data,criterion):\n",
    "    model.eval()\n",
    "    z = torch.from_numpy(np.random.randn(data.shape[0], 10)).float() # 随机噪声\n",
    "    with torch.no_grad():   # 不记录梯度信息\n",
    "        y_pred = model(torch.cat([z, data], dim=1))\n",
    "        loss = criterion(y_pred, y_test)\n",
    "        loss_val.append(loss.item())\n",
    "        print('val MSE loss:',loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1d59bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 网络G，用于预测下一时刻的特征向量\n",
    "G = torch.nn.Sequential(\n",
    "    torch.nn.Linear(20+10,30),   # 特征向量+噪声向量\n",
    "    torch.nn.ReLU(),   \n",
    "    torch.nn.Linear(30,15), \n",
    "    torch.nn.ReLU(),   \n",
    "    torch.nn.Linear(15,20),   \n",
    ")\n",
    "# 网络D，用于一组特征向量是网络G制造的还是真实标签，促使G的输出接近于真实标签\n",
    "D = torch.nn.Sequential(\n",
    "    torch.nn.Linear(20,10),   \n",
    "    torch.nn.ReLU(),   \n",
    "    torch.nn.Linear(10,5), \n",
    "    torch.nn.ReLU(),   \n",
    "    torch.nn.Linear(5,1),\n",
    "    nn.Sigmoid()   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dae7ea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func_bce = nn.BCELoss()    # 用于鉴别器模型D（对抗损失），D的任务判断是否为标签，属于二分类任务，因此用交叉熵损失\n",
    "loss_func_reg = nn.MSELoss()    # 用于生成器G，计算模型预测和标签的差异，因为是回归任务所以用均方根损失\n",
    "opt_g = torch.optim.Adam(G.parameters(), lr=3e-4)   \n",
    "opt_d = torch.optim.Adam(D.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d9073166",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = []\n",
    "loss_val = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa147d7f",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c86370b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = Data.TensorDataset(x_train , y_train)\n",
    "batch_size = 60\n",
    "loader = Data.DataLoader(\n",
    "    dataset = dataset_train,\n",
    "    batch_size = batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers = 8\n",
    ")\n",
    "epochs = 10\n",
    "lambda_res = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "47f16616",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "val MSE loss: 48.88916778564453\n",
      "val MSE loss: 48.839942932128906\n",
      "val MSE loss: 48.79334259033203\n",
      "train D loss: 1.9866406917572021 G loss: 145.09384155273438\n",
      "val MSE loss: 48.72427749633789\n",
      "val MSE loss: 48.673274993896484\n",
      "val MSE loss: 48.62242889404297\n",
      "train D loss: 2.0287790298461914 G loss: 142.08331298828125\n",
      "val MSE loss: 48.55815505981445\n",
      "val MSE loss: 48.49900436401367\n",
      "val MSE loss: 48.44989013671875\n",
      "train D loss: 1.9576349258422852 G loss: 142.30642700195312\n",
      "val MSE loss: 48.383087158203125\n",
      "val MSE loss: 48.32743835449219\n",
      "val MSE loss: 48.270118713378906\n",
      "train D loss: 1.9688491821289062 G loss: 144.52455139160156\n",
      "val MSE loss: 48.19900894165039\n",
      "val MSE loss: 48.15056228637695\n",
      "val MSE loss: 48.10066604614258\n",
      "train D loss: 2.017115592956543 G loss: 142.1493682861328\n",
      "val MSE loss: 48.03609848022461\n",
      "val MSE loss: 47.9707145690918\n",
      "val MSE loss: 47.91036605834961\n",
      "train D loss: 1.9763200283050537 G loss: 141.29212951660156\n",
      "val MSE loss: 47.85422897338867\n",
      "val MSE loss: 47.808284759521484\n",
      "val MSE loss: 47.7416877746582\n",
      "train D loss: 1.9467151165008545 G loss: 140.89712524414062\n",
      "val MSE loss: 47.67113494873047\n",
      "val MSE loss: 47.615234375\n",
      "val MSE loss: 47.558990478515625\n",
      "train D loss: 1.9696184396743774 G loss: 140.67306518554688\n",
      "val MSE loss: 47.48493576049805\n",
      "val MSE loss: 47.42915725708008\n",
      "val MSE loss: 47.37284469604492\n",
      "train D loss: 1.9827005863189697 G loss: 141.03240966796875\n",
      "val MSE loss: 47.306522369384766\n",
      "val MSE loss: 47.241146087646484\n",
      "val MSE loss: 47.180416107177734\n",
      "train D loss: 1.9764554500579834 G loss: 139.1802978515625\n",
      "val MSE loss: 47.11762619018555\n",
      "val MSE loss: 47.048160552978516\n",
      "val MSE loss: 46.98951721191406\n",
      "train D loss: 2.0377306938171387 G loss: 137.36346435546875\n",
      "val MSE loss: 46.9223747253418\n",
      "val MSE loss: 46.856666564941406\n",
      "val MSE loss: 46.79486846923828\n",
      "train D loss: 2.001349925994873 G loss: 139.85484313964844\n",
      "val MSE loss: 46.725589752197266\n",
      "val MSE loss: 46.66952133178711\n",
      "val MSE loss: 46.60121536254883\n",
      "train D loss: 2.011911153793335 G loss: 140.11753845214844\n",
      "val MSE loss: 46.54145431518555\n",
      "val MSE loss: 46.471229553222656\n",
      "val MSE loss: 46.401206970214844\n",
      "train D loss: 1.939212679862976 G loss: 137.65972900390625\n",
      "val MSE loss: 46.33905029296875\n",
      "val MSE loss: 46.27115249633789\n",
      "val MSE loss: 46.20806121826172\n",
      "train D loss: 1.952099323272705 G loss: 137.17721557617188\n",
      "val MSE loss: 46.13844299316406\n",
      "val MSE loss: 46.073211669921875\n",
      "val MSE loss: 46.009761810302734\n",
      "train D loss: 1.9799673557281494 G loss: 137.64035034179688\n",
      "val MSE loss: 45.93138885498047\n",
      "val MSE loss: 45.86144256591797\n",
      "val MSE loss: 45.79519271850586\n",
      "train D loss: 1.95168137550354 G loss: 138.11599731445312\n",
      "val MSE loss: 45.71176528930664\n",
      "val MSE loss: 45.65309524536133\n",
      "val MSE loss: 45.57439422607422\n",
      "train D loss: 1.9590352773666382 G loss: 140.2649383544922\n",
      "val MSE loss: 45.50666809082031\n",
      "val MSE loss: 45.4556770324707\n",
      "val MSE loss: 45.36648941040039\n",
      "train D loss: 1.9624555110931396 G loss: 137.67547607421875\n",
      "val MSE loss: 45.30853271484375\n",
      "val MSE loss: 45.227603912353516\n",
      "val MSE loss: 45.16816711425781\n",
      "train D loss: 1.920815348625183 G loss: 137.00665283203125\n",
      "val MSE loss: 45.09215545654297\n",
      "val MSE loss: 45.019718170166016\n",
      "val MSE loss: 44.9333610534668\n",
      "train D loss: 1.9114561080932617 G loss: 133.42498779296875\n",
      "val MSE loss: 44.866233825683594\n",
      "val MSE loss: 44.78960418701172\n",
      "val MSE loss: 44.71529006958008\n",
      "train D loss: 1.9766255617141724 G loss: 136.98512268066406\n",
      "val MSE loss: 44.649139404296875\n",
      "val MSE loss: 44.549495697021484\n",
      "val MSE loss: 44.47432327270508\n",
      "train D loss: 1.9921588897705078 G loss: 135.5838623046875\n",
      "val MSE loss: 44.40322494506836\n",
      "val MSE loss: 44.323490142822266\n",
      "val MSE loss: 44.248199462890625\n",
      "train D loss: 1.9925336837768555 G loss: 92.56668090820312\n",
      "epoch: 1\n",
      "val MSE loss: 44.15470886230469\n",
      "val MSE loss: 44.09531784057617\n",
      "val MSE loss: 44.00299835205078\n",
      "train D loss: 1.946045160293579 G loss: 133.04710388183594\n",
      "val MSE loss: 43.92180633544922\n",
      "val MSE loss: 43.818504333496094\n",
      "val MSE loss: 43.738983154296875\n",
      "train D loss: 1.925663948059082 G loss: 131.28277587890625\n",
      "val MSE loss: 43.6352424621582\n",
      "val MSE loss: 43.550926208496094\n",
      "val MSE loss: 43.463260650634766\n",
      "train D loss: 1.915130615234375 G loss: 135.64584350585938\n",
      "val MSE loss: 43.37427520751953\n",
      "val MSE loss: 43.282310485839844\n",
      "val MSE loss: 43.19222640991211\n",
      "train D loss: 1.9711426496505737 G loss: 134.66375732421875\n",
      "val MSE loss: 43.11381912231445\n",
      "val MSE loss: 42.99248123168945\n",
      "val MSE loss: 42.89176940917969\n",
      "train D loss: 1.9153821468353271 G loss: 132.24185180664062\n",
      "val MSE loss: 42.785030364990234\n",
      "val MSE loss: 42.69269943237305\n",
      "val MSE loss: 42.5970458984375\n",
      "train D loss: 1.9286359548568726 G loss: 134.12753295898438\n",
      "val MSE loss: 42.49244689941406\n",
      "val MSE loss: 42.38432693481445\n",
      "val MSE loss: 42.28987121582031\n",
      "train D loss: 1.9384312629699707 G loss: 132.2903289794922\n",
      "val MSE loss: 42.1717643737793\n",
      "val MSE loss: 42.071964263916016\n",
      "val MSE loss: 41.975990295410156\n",
      "train D loss: 1.9422156810760498 G loss: 129.90377807617188\n",
      "val MSE loss: 41.84431076049805\n",
      "val MSE loss: 41.74842071533203\n",
      "val MSE loss: 41.62380599975586\n",
      "train D loss: 1.8988010883331299 G loss: 125.02874755859375\n",
      "val MSE loss: 41.52585220336914\n",
      "val MSE loss: 41.399471282958984\n",
      "val MSE loss: 41.280548095703125\n",
      "train D loss: 1.9102134704589844 G loss: 131.24778747558594\n",
      "val MSE loss: 41.16196823120117\n",
      "val MSE loss: 41.042667388916016\n",
      "val MSE loss: 40.9269905090332\n",
      "train D loss: 1.8666359186172485 G loss: 127.73468017578125\n",
      "val MSE loss: 40.81889724731445\n",
      "val MSE loss: 40.69011306762695\n",
      "val MSE loss: 40.5627326965332\n",
      "train D loss: 1.9599255323410034 G loss: 129.07931518554688\n",
      "val MSE loss: 40.425636291503906\n",
      "val MSE loss: 40.31657409667969\n",
      "val MSE loss: 40.2058219909668\n",
      "train D loss: 1.932004690170288 G loss: 127.7481689453125\n",
      "val MSE loss: 40.04948806762695\n",
      "val MSE loss: 39.92744445800781\n",
      "val MSE loss: 39.78830337524414\n",
      "train D loss: 1.8651998043060303 G loss: 124.20540618896484\n",
      "val MSE loss: 39.66584777832031\n",
      "val MSE loss: 39.52433776855469\n",
      "val MSE loss: 39.39567184448242\n",
      "train D loss: 1.9046053886413574 G loss: 125.38546752929688\n",
      "val MSE loss: 39.23596954345703\n",
      "val MSE loss: 39.10206985473633\n",
      "val MSE loss: 38.95354080200195\n",
      "train D loss: 1.9025518894195557 G loss: 124.08445739746094\n",
      "val MSE loss: 38.82081985473633\n",
      "val MSE loss: 38.64814376831055\n",
      "val MSE loss: 38.52607727050781\n",
      "train D loss: 1.8895182609558105 G loss: 122.81126403808594\n",
      "val MSE loss: 38.36045455932617\n",
      "val MSE loss: 38.21691131591797\n",
      "val MSE loss: 38.06116485595703\n",
      "train D loss: 1.8934847116470337 G loss: 119.88583374023438\n",
      "val MSE loss: 37.92630386352539\n",
      "val MSE loss: 37.780635833740234\n",
      "val MSE loss: 37.612125396728516\n",
      "train D loss: 1.8895009756088257 G loss: 124.51783752441406\n",
      "val MSE loss: 37.440067291259766\n",
      "val MSE loss: 37.308528900146484\n",
      "val MSE loss: 37.139888763427734\n",
      "train D loss: 1.866041898727417 G loss: 119.64662170410156\n",
      "val MSE loss: 36.95864486694336\n",
      "val MSE loss: 36.79256820678711\n",
      "val MSE loss: 36.6363410949707\n",
      "train D loss: 1.848901391029358 G loss: 117.79385375976562\n",
      "val MSE loss: 36.449432373046875\n",
      "val MSE loss: 36.29313659667969\n",
      "val MSE loss: 36.13222885131836\n",
      "train D loss: 1.8136954307556152 G loss: 118.24755859375\n",
      "val MSE loss: 35.942813873291016\n",
      "val MSE loss: 35.79536437988281\n",
      "val MSE loss: 35.63401412963867\n",
      "train D loss: 1.864350438117981 G loss: 117.17079162597656\n",
      "val MSE loss: 35.44822311401367\n",
      "val MSE loss: 35.29859924316406\n",
      "val MSE loss: 35.097373962402344\n",
      "train D loss: 1.7440005540847778 G loss: 81.15935516357422\n",
      "epoch: 2\n",
      "val MSE loss: 34.945655822753906\n",
      "val MSE loss: 34.78799819946289\n",
      "val MSE loss: 34.606327056884766\n",
      "train D loss: 1.7869141101837158 G loss: 112.79194641113281\n",
      "val MSE loss: 34.454959869384766\n",
      "val MSE loss: 34.27299118041992\n",
      "val MSE loss: 34.07330322265625\n",
      "train D loss: 1.8077855110168457 G loss: 114.73625946044922\n",
      "val MSE loss: 33.926483154296875\n",
      "val MSE loss: 33.75333786010742\n",
      "val MSE loss: 33.53327941894531\n",
      "train D loss: 1.8100234270095825 G loss: 115.39010620117188\n",
      "val MSE loss: 33.37044906616211\n",
      "val MSE loss: 33.186988830566406\n",
      "val MSE loss: 32.997901916503906\n",
      "train D loss: 1.8258087635040283 G loss: 114.35678100585938\n",
      "val MSE loss: 32.8007926940918\n",
      "val MSE loss: 32.61964416503906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val MSE loss: 32.39981460571289\n",
      "train D loss: 1.82481050491333 G loss: 111.17138671875\n",
      "val MSE loss: 32.23309326171875\n",
      "val MSE loss: 32.063846588134766\n",
      "val MSE loss: 31.851449966430664\n",
      "train D loss: 1.7635325193405151 G loss: 108.20721435546875\n",
      "val MSE loss: 31.639211654663086\n",
      "val MSE loss: 31.433963775634766\n",
      "val MSE loss: 31.227947235107422\n",
      "train D loss: 1.8159176111221313 G loss: 108.95518493652344\n",
      "val MSE loss: 31.044973373413086\n",
      "val MSE loss: 30.82695770263672\n",
      "val MSE loss: 30.64451789855957\n",
      "train D loss: 1.789698839187622 G loss: 108.8724365234375\n",
      "val MSE loss: 30.40166473388672\n",
      "val MSE loss: 30.214406967163086\n",
      "val MSE loss: 29.993589401245117\n",
      "train D loss: 1.7337510585784912 G loss: 107.19657135009766\n",
      "val MSE loss: 29.795425415039062\n",
      "val MSE loss: 29.615970611572266\n",
      "val MSE loss: 29.403377532958984\n",
      "train D loss: 1.823392629623413 G loss: 105.04534149169922\n",
      "val MSE loss: 29.179773330688477\n",
      "val MSE loss: 28.970882415771484\n",
      "val MSE loss: 28.745500564575195\n",
      "train D loss: 1.8196096420288086 G loss: 100.49180603027344\n",
      "val MSE loss: 28.540407180786133\n",
      "val MSE loss: 28.30037498474121\n",
      "val MSE loss: 28.088594436645508\n",
      "train D loss: 1.7497196197509766 G loss: 98.74239349365234\n",
      "val MSE loss: 27.888259887695312\n",
      "val MSE loss: 27.647558212280273\n",
      "val MSE loss: 27.437400817871094\n",
      "train D loss: 1.7230892181396484 G loss: 101.97919464111328\n",
      "val MSE loss: 27.197052001953125\n",
      "val MSE loss: 26.988615036010742\n",
      "val MSE loss: 26.77278709411621\n",
      "train D loss: 1.807100534439087 G loss: 97.88143920898438\n",
      "val MSE loss: 26.53533935546875\n",
      "val MSE loss: 26.352346420288086\n",
      "val MSE loss: 26.10689926147461\n",
      "train D loss: 1.8130114078521729 G loss: 96.67176055908203\n",
      "val MSE loss: 25.891324996948242\n",
      "val MSE loss: 25.669795989990234\n",
      "val MSE loss: 25.429901123046875\n",
      "train D loss: 1.7444374561309814 G loss: 96.717041015625\n",
      "val MSE loss: 25.224010467529297\n",
      "val MSE loss: 24.966398239135742\n",
      "val MSE loss: 24.774198532104492\n",
      "train D loss: 1.7293169498443604 G loss: 92.37507629394531\n",
      "val MSE loss: 24.559162139892578\n",
      "val MSE loss: 24.332988739013672\n",
      "val MSE loss: 24.1145076751709\n",
      "train D loss: 1.7250795364379883 G loss: 95.80241394042969\n",
      "val MSE loss: 23.88907241821289\n",
      "val MSE loss: 23.663101196289062\n",
      "val MSE loss: 23.4373779296875\n",
      "train D loss: 1.7161192893981934 G loss: 90.19660186767578\n",
      "val MSE loss: 23.20424461364746\n",
      "val MSE loss: 22.991374969482422\n",
      "val MSE loss: 22.779911041259766\n",
      "train D loss: 1.7281787395477295 G loss: 84.727783203125\n",
      "val MSE loss: 22.531917572021484\n",
      "val MSE loss: 22.321557998657227\n",
      "val MSE loss: 22.096508026123047\n",
      "train D loss: 1.7116079330444336 G loss: 89.28495788574219\n",
      "val MSE loss: 21.851348876953125\n",
      "val MSE loss: 21.652067184448242\n",
      "val MSE loss: 21.426382064819336\n",
      "train D loss: 1.745659351348877 G loss: 86.77902221679688\n",
      "val MSE loss: 21.195295333862305\n",
      "val MSE loss: 20.98938751220703\n",
      "val MSE loss: 20.75374412536621\n",
      "train D loss: 1.678553581237793 G loss: 86.44644165039062\n",
      "val MSE loss: 20.56658935546875\n",
      "val MSE loss: 20.333148956298828\n",
      "val MSE loss: 20.135921478271484\n",
      "train D loss: 1.7373698949813843 G loss: 54.53144073486328\n",
      "epoch: 3\n",
      "val MSE loss: 19.921695709228516\n",
      "val MSE loss: 19.724578857421875\n",
      "val MSE loss: 19.505220413208008\n",
      "train D loss: 1.656104326248169 G loss: 82.60628509521484\n",
      "val MSE loss: 19.318134307861328\n",
      "val MSE loss: 19.100343704223633\n",
      "val MSE loss: 18.88449478149414\n",
      "train D loss: 1.7284119129180908 G loss: 79.867431640625\n",
      "val MSE loss: 18.69090461730957\n",
      "val MSE loss: 18.45433235168457\n",
      "val MSE loss: 18.262399673461914\n",
      "train D loss: 1.690737247467041 G loss: 79.73341369628906\n",
      "val MSE loss: 18.084787368774414\n",
      "val MSE loss: 17.86758041381836\n",
      "val MSE loss: 17.673677444458008\n",
      "train D loss: 1.685861349105835 G loss: 79.78992462158203\n",
      "val MSE loss: 17.46787452697754\n",
      "val MSE loss: 17.277681350708008\n",
      "val MSE loss: 17.077260971069336\n",
      "train D loss: 1.7440513372421265 G loss: 77.96995544433594\n",
      "val MSE loss: 16.883533477783203\n",
      "val MSE loss: 16.71004295349121\n",
      "val MSE loss: 16.51360511779785\n",
      "train D loss: 1.6378567218780518 G loss: 73.06642150878906\n",
      "val MSE loss: 16.326553344726562\n",
      "val MSE loss: 16.12952995300293\n",
      "val MSE loss: 15.953095436096191\n",
      "train D loss: 1.7248847484588623 G loss: 74.47689056396484\n",
      "val MSE loss: 15.785794258117676\n",
      "val MSE loss: 15.565825462341309\n",
      "val MSE loss: 15.399510383605957\n",
      "train D loss: 1.664491891860962 G loss: 72.7254638671875\n",
      "val MSE loss: 15.20937728881836\n",
      "val MSE loss: 15.059957504272461\n",
      "val MSE loss: 14.876689910888672\n",
      "train D loss: 1.6444334983825684 G loss: 71.94021606445312\n",
      "val MSE loss: 14.705705642700195\n",
      "val MSE loss: 14.56168270111084\n",
      "val MSE loss: 14.386358261108398\n",
      "train D loss: 1.6698226928710938 G loss: 67.91410827636719\n",
      "val MSE loss: 14.233518600463867\n",
      "val MSE loss: 14.058611869812012\n",
      "val MSE loss: 13.90283203125\n",
      "train D loss: 1.6624348163604736 G loss: 69.36643981933594\n",
      "val MSE loss: 13.741732597351074\n",
      "val MSE loss: 13.600722312927246\n",
      "val MSE loss: 13.439677238464355\n",
      "train D loss: 1.6691370010375977 G loss: 65.08110046386719\n",
      "val MSE loss: 13.297640800476074\n",
      "val MSE loss: 13.132192611694336\n",
      "val MSE loss: 13.002933502197266\n",
      "train D loss: 1.6080763339996338 G loss: 64.4931869506836\n",
      "val MSE loss: 12.840874671936035\n",
      "val MSE loss: 12.702515602111816\n",
      "val MSE loss: 12.575467109680176\n",
      "train D loss: 1.6191608905792236 G loss: 65.01603698730469\n",
      "val MSE loss: 12.42467212677002\n",
      "val MSE loss: 12.297636985778809\n",
      "val MSE loss: 12.177149772644043\n",
      "train D loss: 1.6428067684173584 G loss: 61.44723129272461\n",
      "val MSE loss: 12.046557426452637\n",
      "val MSE loss: 11.916150093078613\n",
      "val MSE loss: 11.797330856323242\n",
      "train D loss: 1.6273548603057861 G loss: 65.62967681884766\n",
      "val MSE loss: 11.675944328308105\n",
      "val MSE loss: 11.586421966552734\n",
      "val MSE loss: 11.472039222717285\n",
      "train D loss: 1.6250303983688354 G loss: 61.74132537841797\n",
      "val MSE loss: 11.355047225952148\n",
      "val MSE loss: 11.247493743896484\n",
      "val MSE loss: 11.137663841247559\n",
      "train D loss: 1.6034187078475952 G loss: 58.693119049072266\n",
      "val MSE loss: 11.024834632873535\n",
      "val MSE loss: 10.92377758026123\n",
      "val MSE loss: 10.83770751953125\n",
      "train D loss: 1.6213462352752686 G loss: 58.18046188354492\n",
      "val MSE loss: 10.735268592834473\n",
      "val MSE loss: 10.650300979614258\n",
      "val MSE loss: 10.552921295166016\n",
      "train D loss: 1.564955711364746 G loss: 56.06667709350586\n",
      "val MSE loss: 10.46084213256836\n",
      "val MSE loss: 10.386476516723633\n",
      "val MSE loss: 10.293800354003906\n",
      "train D loss: 1.5974656343460083 G loss: 56.30954360961914\n",
      "val MSE loss: 10.21448040008545\n",
      "val MSE loss: 10.138077735900879\n",
      "val MSE loss: 10.062005043029785\n",
      "train D loss: 1.5720458030700684 G loss: 54.88859176635742\n",
      "val MSE loss: 9.993093490600586\n",
      "val MSE loss: 9.92820930480957\n",
      "val MSE loss: 9.86106014251709\n",
      "train D loss: 1.5420103073120117 G loss: 52.231685638427734\n",
      "val MSE loss: 9.794984817504883\n",
      "val MSE loss: 9.743545532226562\n",
      "val MSE loss: 9.690042495727539\n",
      "train D loss: 1.507524013519287 G loss: 34.634605407714844\n",
      "epoch: 4\n",
      "val MSE loss: 9.633121490478516\n",
      "val MSE loss: 9.581509590148926\n",
      "val MSE loss: 9.529552459716797\n",
      "train D loss: 1.5757639408111572 G loss: 54.65853500366211\n",
      "val MSE loss: 9.49089241027832\n",
      "val MSE loss: 9.445694923400879\n",
      "val MSE loss: 9.390933990478516\n",
      "train D loss: 1.5758817195892334 G loss: 57.69586944580078\n",
      "val MSE loss: 9.343283653259277\n",
      "val MSE loss: 9.299840927124023\n",
      "val MSE loss: 9.256625175476074\n",
      "train D loss: 1.5557060241699219 G loss: 53.256080627441406\n",
      "val MSE loss: 9.201862335205078\n",
      "val MSE loss: 9.16596794128418\n",
      "val MSE loss: 9.118538856506348\n",
      "train D loss: 1.5763516426086426 G loss: 52.45820236206055\n",
      "val MSE loss: 9.084656715393066\n",
      "val MSE loss: 9.035069465637207\n",
      "val MSE loss: 9.005279541015625\n",
      "train D loss: 1.5467970371246338 G loss: 51.43233871459961\n",
      "val MSE loss: 8.971394538879395\n",
      "val MSE loss: 8.937004089355469\n",
      "val MSE loss: 8.899876594543457\n",
      "train D loss: 1.5583972930908203 G loss: 50.722450256347656\n",
      "val MSE loss: 8.872467041015625\n",
      "val MSE loss: 8.845913887023926\n",
      "val MSE loss: 8.810554504394531\n",
      "train D loss: 1.546494722366333 G loss: 50.60441589355469\n",
      "val MSE loss: 8.786609649658203\n",
      "val MSE loss: 8.757973670959473\n",
      "val MSE loss: 8.734453201293945\n",
      "train D loss: 1.5591826438903809 G loss: 50.84419250488281\n",
      "val MSE loss: 8.696840286254883\n",
      "val MSE loss: 8.674175262451172\n",
      "val MSE loss: 8.651222229003906\n",
      "train D loss: 1.530321478843689 G loss: 51.257713317871094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val MSE loss: 8.625471115112305\n",
      "val MSE loss: 8.60733413696289\n",
      "val MSE loss: 8.584226608276367\n",
      "train D loss: 1.5275557041168213 G loss: 49.28558349609375\n",
      "val MSE loss: 8.565585136413574\n",
      "val MSE loss: 8.53689956665039\n",
      "val MSE loss: 8.518411636352539\n",
      "train D loss: 1.5180991888046265 G loss: 49.990596771240234\n",
      "val MSE loss: 8.49451732635498\n",
      "val MSE loss: 8.471102714538574\n",
      "val MSE loss: 8.450739860534668\n",
      "train D loss: 1.5512096881866455 G loss: 49.493019104003906\n",
      "val MSE loss: 8.430989265441895\n",
      "val MSE loss: 8.414006233215332\n",
      "val MSE loss: 8.388055801391602\n",
      "train D loss: 1.5273196697235107 G loss: 48.88264083862305\n",
      "val MSE loss: 8.371856689453125\n",
      "val MSE loss: 8.357686042785645\n",
      "val MSE loss: 8.330459594726562\n",
      "train D loss: 1.512798547744751 G loss: 48.39128875732422\n",
      "val MSE loss: 8.309852600097656\n",
      "val MSE loss: 8.28766918182373\n",
      "val MSE loss: 8.26291275024414\n",
      "train D loss: 1.5035947561264038 G loss: 49.022544860839844\n",
      "val MSE loss: 8.247940063476562\n",
      "val MSE loss: 8.228592872619629\n",
      "val MSE loss: 8.202986717224121\n",
      "train D loss: 1.5745036602020264 G loss: 49.71588134765625\n",
      "val MSE loss: 8.182577133178711\n",
      "val MSE loss: 8.16262149810791\n",
      "val MSE loss: 8.140931129455566\n",
      "train D loss: 1.539616346359253 G loss: 49.42723846435547\n",
      "val MSE loss: 8.12611198425293\n",
      "val MSE loss: 8.110240936279297\n",
      "val MSE loss: 8.081732749938965\n",
      "train D loss: 1.502853274345398 G loss: 47.61423873901367\n",
      "val MSE loss: 8.068839073181152\n",
      "val MSE loss: 8.047260284423828\n",
      "val MSE loss: 8.029691696166992\n",
      "train D loss: 1.5283366441726685 G loss: 51.037967681884766\n",
      "val MSE loss: 8.016386985778809\n",
      "val MSE loss: 7.992393970489502\n",
      "val MSE loss: 7.984358787536621\n",
      "train D loss: 1.5466312170028687 G loss: 45.138031005859375\n",
      "val MSE loss: 7.971585273742676\n",
      "val MSE loss: 7.948869705200195\n",
      "val MSE loss: 7.941760540008545\n",
      "train D loss: 1.544564127922058 G loss: 48.945743560791016\n",
      "val MSE loss: 7.922773361206055\n",
      "val MSE loss: 7.916959762573242\n",
      "val MSE loss: 7.8956170082092285\n",
      "train D loss: 1.5280348062515259 G loss: 49.95378875732422\n",
      "val MSE loss: 7.878109455108643\n",
      "val MSE loss: 7.864744186401367\n",
      "val MSE loss: 7.844531059265137\n",
      "train D loss: 1.5279382467269897 G loss: 45.887447357177734\n",
      "val MSE loss: 7.834062576293945\n",
      "val MSE loss: 7.821692943572998\n",
      "val MSE loss: 7.813335418701172\n",
      "train D loss: 1.610419511795044 G loss: 26.850475311279297\n",
      "epoch: 5\n",
      "val MSE loss: 7.805072784423828\n",
      "val MSE loss: 7.792040824890137\n",
      "val MSE loss: 7.782491683959961\n",
      "train D loss: 1.5592604875564575 G loss: 46.84920120239258\n",
      "val MSE loss: 7.773257732391357\n",
      "val MSE loss: 7.759707450866699\n",
      "val MSE loss: 7.744429588317871\n",
      "train D loss: 1.5050175189971924 G loss: 46.0140495300293\n",
      "val MSE loss: 7.721813678741455\n",
      "val MSE loss: 7.709003925323486\n",
      "val MSE loss: 7.692735195159912\n",
      "train D loss: 1.4858980178833008 G loss: 43.66899490356445\n",
      "val MSE loss: 7.665550708770752\n",
      "val MSE loss: 7.648921012878418\n",
      "val MSE loss: 7.626799583435059\n",
      "train D loss: 1.5164142847061157 G loss: 43.89739990234375\n",
      "val MSE loss: 7.61176061630249\n",
      "val MSE loss: 7.588126182556152\n",
      "val MSE loss: 7.576064586639404\n",
      "train D loss: 1.506151556968689 G loss: 45.61901092529297\n",
      "val MSE loss: 7.559789180755615\n",
      "val MSE loss: 7.530394554138184\n",
      "val MSE loss: 7.526386737823486\n",
      "train D loss: 1.5303555727005005 G loss: 48.596343994140625\n",
      "val MSE loss: 7.505834102630615\n",
      "val MSE loss: 7.4919633865356445\n",
      "val MSE loss: 7.467832088470459\n",
      "train D loss: 1.5024240016937256 G loss: 43.57122802734375\n",
      "val MSE loss: 7.456341743469238\n",
      "val MSE loss: 7.440820217132568\n",
      "val MSE loss: 7.423293590545654\n",
      "train D loss: 1.5085535049438477 G loss: 43.857093811035156\n",
      "val MSE loss: 7.40858793258667\n",
      "val MSE loss: 7.394261837005615\n",
      "val MSE loss: 7.376524925231934\n",
      "train D loss: 1.4934672117233276 G loss: 42.397056579589844\n",
      "val MSE loss: 7.362265110015869\n",
      "val MSE loss: 7.344548225402832\n",
      "val MSE loss: 7.322079658508301\n",
      "train D loss: 1.5013477802276611 G loss: 45.45802307128906\n",
      "val MSE loss: 7.30398416519165\n",
      "val MSE loss: 7.290430545806885\n",
      "val MSE loss: 7.275045394897461\n",
      "train D loss: 1.5074222087860107 G loss: 43.707767486572266\n",
      "val MSE loss: 7.255739688873291\n",
      "val MSE loss: 7.241684913635254\n",
      "val MSE loss: 7.218479156494141\n",
      "train D loss: 1.5238773822784424 G loss: 45.65976333618164\n",
      "val MSE loss: 7.210779190063477\n",
      "val MSE loss: 7.194114685058594\n",
      "val MSE loss: 7.183648586273193\n",
      "train D loss: 1.5078896284103394 G loss: 47.38525390625\n",
      "val MSE loss: 7.159952640533447\n",
      "val MSE loss: 7.143916130065918\n",
      "val MSE loss: 7.1275434494018555\n",
      "train D loss: 1.503157138824463 G loss: 41.439090728759766\n",
      "val MSE loss: 7.114418029785156\n",
      "val MSE loss: 7.090307235717773\n",
      "val MSE loss: 7.079761028289795\n",
      "train D loss: 1.4763636589050293 G loss: 42.31067657470703\n",
      "val MSE loss: 7.0547871589660645\n",
      "val MSE loss: 7.046575546264648\n",
      "val MSE loss: 7.034215927124023\n",
      "train D loss: 1.5039260387420654 G loss: 42.45329284667969\n",
      "val MSE loss: 7.022678852081299\n",
      "val MSE loss: 6.9959588050842285\n",
      "val MSE loss: 6.983638763427734\n",
      "train D loss: 1.526879906654358 G loss: 40.58114242553711\n",
      "val MSE loss: 6.969272613525391\n",
      "val MSE loss: 6.947515487670898\n",
      "val MSE loss: 6.93717622756958\n",
      "train D loss: 1.5009649991989136 G loss: 40.14597702026367\n",
      "val MSE loss: 6.907814025878906\n",
      "val MSE loss: 6.892382621765137\n",
      "val MSE loss: 6.876707077026367\n",
      "train D loss: 1.5023558139801025 G loss: 39.254459381103516\n",
      "val MSE loss: 6.8542070388793945\n",
      "val MSE loss: 6.831602096557617\n",
      "val MSE loss: 6.81723165512085\n",
      "train D loss: 1.4961605072021484 G loss: 40.87038803100586\n",
      "val MSE loss: 6.796392440795898\n",
      "val MSE loss: 6.780662536621094\n",
      "val MSE loss: 6.7630743980407715\n",
      "train D loss: 1.5400488376617432 G loss: 42.48588562011719\n",
      "val MSE loss: 6.737773895263672\n",
      "val MSE loss: 6.719725131988525\n",
      "val MSE loss: 6.692104339599609\n",
      "train D loss: 1.4893951416015625 G loss: 39.69294357299805\n",
      "val MSE loss: 6.680049419403076\n",
      "val MSE loss: 6.663259983062744\n",
      "val MSE loss: 6.644376277923584\n",
      "train D loss: 1.4948958158493042 G loss: 39.91170883178711\n",
      "val MSE loss: 6.618157386779785\n",
      "val MSE loss: 6.601835250854492\n",
      "val MSE loss: 6.579850673675537\n",
      "train D loss: 1.4042433500289917 G loss: 24.758834838867188\n",
      "epoch: 6\n",
      "val MSE loss: 6.567770957946777\n",
      "val MSE loss: 6.552412033081055\n",
      "val MSE loss: 6.538691520690918\n",
      "train D loss: 1.4917488098144531 G loss: 38.698570251464844\n",
      "val MSE loss: 6.520157337188721\n",
      "val MSE loss: 6.508304595947266\n",
      "val MSE loss: 6.490523338317871\n",
      "train D loss: 1.4958381652832031 G loss: 38.44947052001953\n",
      "val MSE loss: 6.475678443908691\n",
      "val MSE loss: 6.455941200256348\n",
      "val MSE loss: 6.442496299743652\n",
      "train D loss: 1.475679636001587 G loss: 39.478458404541016\n",
      "val MSE loss: 6.420114040374756\n",
      "val MSE loss: 6.3992462158203125\n",
      "val MSE loss: 6.382981300354004\n",
      "train D loss: 1.5541456937789917 G loss: 39.13015365600586\n",
      "val MSE loss: 6.360796928405762\n",
      "val MSE loss: 6.3423686027526855\n",
      "val MSE loss: 6.326402187347412\n",
      "train D loss: 1.4736274480819702 G loss: 36.35920333862305\n",
      "val MSE loss: 6.312610149383545\n",
      "val MSE loss: 6.287210941314697\n",
      "val MSE loss: 6.278571128845215\n",
      "train D loss: 1.495223879814148 G loss: 39.52512741088867\n",
      "val MSE loss: 6.251657485961914\n",
      "val MSE loss: 6.234825611114502\n",
      "val MSE loss: 6.209693908691406\n",
      "train D loss: 1.4715772867202759 G loss: 38.678321838378906\n",
      "val MSE loss: 6.189548015594482\n",
      "val MSE loss: 6.178491592407227\n",
      "val MSE loss: 6.163422584533691\n",
      "train D loss: 1.4852015972137451 G loss: 36.7614631652832\n",
      "val MSE loss: 6.1434855461120605\n",
      "val MSE loss: 6.1207404136657715\n",
      "val MSE loss: 6.101689338684082\n",
      "train D loss: 1.480417013168335 G loss: 36.69324493408203\n",
      "val MSE loss: 6.080910682678223\n",
      "val MSE loss: 6.059051513671875\n",
      "val MSE loss: 6.0453996658325195\n",
      "train D loss: 1.490944504737854 G loss: 37.27867126464844\n",
      "val MSE loss: 6.017184257507324\n",
      "val MSE loss: 6.004730701446533\n",
      "val MSE loss: 5.983877658843994\n",
      "train D loss: 1.4976712465286255 G loss: 35.07202911376953\n",
      "val MSE loss: 5.97391939163208\n",
      "val MSE loss: 5.9457244873046875\n",
      "val MSE loss: 5.932412624359131\n",
      "train D loss: 1.489935278892517 G loss: 36.72690963745117\n",
      "val MSE loss: 5.914901256561279\n",
      "val MSE loss: 5.889612674713135\n",
      "val MSE loss: 5.877921104431152\n",
      "train D loss: 1.4758925437927246 G loss: 34.47185134887695\n",
      "val MSE loss: 5.851443290710449\n",
      "val MSE loss: 5.843544960021973\n",
      "val MSE loss: 5.816551208496094\n",
      "train D loss: 1.494555950164795 G loss: 32.75034713745117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val MSE loss: 5.807284355163574\n",
      "val MSE loss: 5.785455703735352\n",
      "val MSE loss: 5.77497673034668\n",
      "train D loss: 1.4802299737930298 G loss: 33.47673034667969\n",
      "val MSE loss: 5.753509998321533\n",
      "val MSE loss: 5.735325336456299\n",
      "val MSE loss: 5.724841117858887\n",
      "train D loss: 1.4844355583190918 G loss: 34.148765563964844\n",
      "val MSE loss: 5.703293800354004\n",
      "val MSE loss: 5.678819179534912\n",
      "val MSE loss: 5.668346881866455\n",
      "train D loss: 1.479168176651001 G loss: 32.527740478515625\n",
      "val MSE loss: 5.644989490509033\n",
      "val MSE loss: 5.624570846557617\n",
      "val MSE loss: 5.618903636932373\n",
      "train D loss: 1.474243402481079 G loss: 32.91834259033203\n",
      "val MSE loss: 5.597048759460449\n",
      "val MSE loss: 5.579452991485596\n",
      "val MSE loss: 5.56696891784668\n",
      "train D loss: 1.5067881345748901 G loss: 35.29404830932617\n",
      "val MSE loss: 5.548500061035156\n",
      "val MSE loss: 5.529661655426025\n",
      "val MSE loss: 5.51669979095459\n",
      "train D loss: 1.4780304431915283 G loss: 33.73849105834961\n",
      "val MSE loss: 5.495203018188477\n",
      "val MSE loss: 5.479863166809082\n",
      "val MSE loss: 5.464050769805908\n",
      "train D loss: 1.4782466888427734 G loss: 32.16728210449219\n",
      "val MSE loss: 5.45157527923584\n",
      "val MSE loss: 5.433716297149658\n",
      "val MSE loss: 5.412460803985596\n",
      "train D loss: 1.4706956148147583 G loss: 31.233951568603516\n",
      "val MSE loss: 5.396515846252441\n",
      "val MSE loss: 5.376495838165283\n",
      "val MSE loss: 5.3719635009765625\n",
      "train D loss: 1.4965757131576538 G loss: 32.94799041748047\n",
      "val MSE loss: 5.3505072593688965\n",
      "val MSE loss: 5.334007740020752\n",
      "val MSE loss: 5.32254695892334\n",
      "train D loss: 1.465811014175415 G loss: 20.894794464111328\n",
      "epoch: 7\n",
      "val MSE loss: 5.298555850982666\n",
      "val MSE loss: 5.289391994476318\n",
      "val MSE loss: 5.270065784454346\n",
      "train D loss: 1.4971156120300293 G loss: 32.33186721801758\n",
      "val MSE loss: 5.259573459625244\n",
      "val MSE loss: 5.24760627746582\n",
      "val MSE loss: 5.22666597366333\n",
      "train D loss: 1.4883792400360107 G loss: 31.20745849609375\n",
      "val MSE loss: 5.213035583496094\n",
      "val MSE loss: 5.191673278808594\n",
      "val MSE loss: 5.183953762054443\n",
      "train D loss: 1.4661121368408203 G loss: 28.471378326416016\n",
      "val MSE loss: 5.165389537811279\n",
      "val MSE loss: 5.146338939666748\n",
      "val MSE loss: 5.137635231018066\n",
      "train D loss: 1.4690680503845215 G loss: 32.574764251708984\n",
      "val MSE loss: 5.1190924644470215\n",
      "val MSE loss: 5.107132911682129\n",
      "val MSE loss: 5.088274002075195\n",
      "train D loss: 1.4563841819763184 G loss: 28.861225128173828\n",
      "val MSE loss: 5.077072620391846\n",
      "val MSE loss: 5.072047710418701\n",
      "val MSE loss: 5.0486345291137695\n",
      "train D loss: 1.4605262279510498 G loss: 29.283126831054688\n",
      "val MSE loss: 5.042238235473633\n",
      "val MSE loss: 5.027822971343994\n",
      "val MSE loss: 5.016932010650635\n",
      "train D loss: 1.50537109375 G loss: 31.406919479370117\n",
      "val MSE loss: 4.999921798706055\n",
      "val MSE loss: 4.992956161499023\n",
      "val MSE loss: 4.980754375457764\n",
      "train D loss: 1.4997371435165405 G loss: 31.091812133789062\n",
      "val MSE loss: 4.968690872192383\n",
      "val MSE loss: 4.956792831420898\n",
      "val MSE loss: 4.94735860824585\n",
      "train D loss: 1.4665731191635132 G loss: 29.607364654541016\n",
      "val MSE loss: 4.938295364379883\n",
      "val MSE loss: 4.9253387451171875\n",
      "val MSE loss: 4.915893077850342\n",
      "train D loss: 1.4852094650268555 G loss: 27.049182891845703\n",
      "val MSE loss: 4.901003360748291\n",
      "val MSE loss: 4.8942036628723145\n",
      "val MSE loss: 4.8817291259765625\n",
      "train D loss: 1.4863176345825195 G loss: 30.127294540405273\n",
      "val MSE loss: 4.871397495269775\n",
      "val MSE loss: 4.860441207885742\n",
      "val MSE loss: 4.848503112792969\n",
      "train D loss: 1.5059808492660522 G loss: 28.997146606445312\n",
      "val MSE loss: 4.8396077156066895\n",
      "val MSE loss: 4.835842132568359\n",
      "val MSE loss: 4.821841716766357\n",
      "train D loss: 1.4652605056762695 G loss: 28.702634811401367\n",
      "val MSE loss: 4.813669204711914\n",
      "val MSE loss: 4.804015159606934\n",
      "val MSE loss: 4.785777568817139\n",
      "train D loss: 1.4475123882293701 G loss: 27.576473236083984\n",
      "val MSE loss: 4.770443439483643\n",
      "val MSE loss: 4.752996921539307\n",
      "val MSE loss: 4.736014366149902\n",
      "train D loss: 1.4510389566421509 G loss: 26.717519760131836\n",
      "val MSE loss: 4.716669082641602\n",
      "val MSE loss: 4.699977397918701\n",
      "val MSE loss: 4.6938934326171875\n",
      "train D loss: 1.468142032623291 G loss: 27.695310592651367\n",
      "val MSE loss: 4.67666482925415\n",
      "val MSE loss: 4.657029151916504\n",
      "val MSE loss: 4.650089263916016\n",
      "train D loss: 1.460207223892212 G loss: 26.488082885742188\n",
      "val MSE loss: 4.632658958435059\n",
      "val MSE loss: 4.622857570648193\n",
      "val MSE loss: 4.610573768615723\n",
      "train D loss: 1.4581440687179565 G loss: 25.284608840942383\n",
      "val MSE loss: 4.603899955749512\n",
      "val MSE loss: 4.594203948974609\n",
      "val MSE loss: 4.5838189125061035\n",
      "train D loss: 1.4618115425109863 G loss: 26.391769409179688\n",
      "val MSE loss: 4.56436014175415\n",
      "val MSE loss: 4.564110279083252\n",
      "val MSE loss: 4.557224750518799\n",
      "train D loss: 1.475081205368042 G loss: 28.191486358642578\n",
      "val MSE loss: 4.546319007873535\n",
      "val MSE loss: 4.530158042907715\n",
      "val MSE loss: 4.517165184020996\n",
      "train D loss: 1.4717189073562622 G loss: 26.73720932006836\n",
      "val MSE loss: 4.502949237823486\n",
      "val MSE loss: 4.4966936111450195\n",
      "val MSE loss: 4.4788360595703125\n",
      "train D loss: 1.4656010866165161 G loss: 25.538158416748047\n",
      "val MSE loss: 4.459977149963379\n",
      "val MSE loss: 4.456212520599365\n",
      "val MSE loss: 4.446476459503174\n",
      "train D loss: 1.4818990230560303 G loss: 24.315156936645508\n",
      "val MSE loss: 4.438774585723877\n",
      "val MSE loss: 4.424297332763672\n",
      "val MSE loss: 4.415290832519531\n",
      "train D loss: 1.455175757408142 G loss: 18.60018539428711\n",
      "epoch: 8\n",
      "val MSE loss: 4.408604145050049\n",
      "val MSE loss: 4.400119304656982\n",
      "val MSE loss: 4.3882880210876465\n",
      "train D loss: 1.4940237998962402 G loss: 26.754837036132812\n",
      "val MSE loss: 4.385704517364502\n",
      "val MSE loss: 4.372966289520264\n",
      "val MSE loss: 4.366414546966553\n",
      "train D loss: 1.4656797647476196 G loss: 25.477434158325195\n",
      "val MSE loss: 4.353882789611816\n",
      "val MSE loss: 4.347909450531006\n",
      "val MSE loss: 4.340294361114502\n",
      "train D loss: 1.4521284103393555 G loss: 23.187286376953125\n",
      "val MSE loss: 4.331489086151123\n",
      "val MSE loss: 4.3222737312316895\n",
      "val MSE loss: 4.318943977355957\n",
      "train D loss: 1.4844872951507568 G loss: 27.154069900512695\n",
      "val MSE loss: 4.3029632568359375\n",
      "val MSE loss: 4.302963733673096\n",
      "val MSE loss: 4.2893781661987305\n",
      "train D loss: 1.435718059539795 G loss: 23.112056732177734\n",
      "val MSE loss: 4.280023574829102\n",
      "val MSE loss: 4.270757675170898\n",
      "val MSE loss: 4.2578582763671875\n",
      "train D loss: 1.4655976295471191 G loss: 22.024499893188477\n",
      "val MSE loss: 4.244636058807373\n",
      "val MSE loss: 4.241740703582764\n",
      "val MSE loss: 4.228785037994385\n",
      "train D loss: 1.4677311182022095 G loss: 25.959426879882812\n",
      "val MSE loss: 4.218027114868164\n",
      "val MSE loss: 4.208296298980713\n",
      "val MSE loss: 4.202995300292969\n",
      "train D loss: 1.4582936763763428 G loss: 22.443714141845703\n",
      "val MSE loss: 4.202949047088623\n",
      "val MSE loss: 4.186800003051758\n",
      "val MSE loss: 4.184020042419434\n",
      "train D loss: 1.4709872007369995 G loss: 25.89537811279297\n",
      "val MSE loss: 4.171831130981445\n",
      "val MSE loss: 4.159451961517334\n",
      "val MSE loss: 4.143186092376709\n",
      "train D loss: 1.4456552267074585 G loss: 25.76165008544922\n",
      "val MSE loss: 4.139511585235596\n",
      "val MSE loss: 4.123713493347168\n",
      "val MSE loss: 4.106310844421387\n",
      "train D loss: 1.4726669788360596 G loss: 23.830333709716797\n",
      "val MSE loss: 4.0974297523498535\n",
      "val MSE loss: 4.091990947723389\n",
      "val MSE loss: 4.079648494720459\n",
      "train D loss: 1.4526450634002686 G loss: 24.676647186279297\n",
      "val MSE loss: 4.065566539764404\n",
      "val MSE loss: 4.058700084686279\n",
      "val MSE loss: 4.0476861000061035\n",
      "train D loss: 1.4615204334259033 G loss: 23.925945281982422\n",
      "val MSE loss: 4.03556489944458\n",
      "val MSE loss: 4.020322799682617\n",
      "val MSE loss: 4.00910758972168\n",
      "train D loss: 1.4507009983062744 G loss: 21.980009078979492\n",
      "val MSE loss: 3.9954915046691895\n",
      "val MSE loss: 3.983938455581665\n",
      "val MSE loss: 3.980717420578003\n",
      "train D loss: 1.457870364189148 G loss: 23.669832229614258\n",
      "val MSE loss: 3.9662673473358154\n",
      "val MSE loss: 3.964989185333252\n",
      "val MSE loss: 3.951646566390991\n",
      "train D loss: 1.4783068895339966 G loss: 25.4240779876709\n",
      "val MSE loss: 3.9374895095825195\n",
      "val MSE loss: 3.9296038150787354\n",
      "val MSE loss: 3.923935651779175\n",
      "train D loss: 1.4511022567749023 G loss: 22.388099670410156\n",
      "val MSE loss: 3.9159576892852783\n",
      "val MSE loss: 3.900071620941162\n",
      "val MSE loss: 3.8934218883514404\n",
      "train D loss: 1.471814751625061 G loss: 23.39755630493164\n",
      "val MSE loss: 3.8855113983154297\n",
      "val MSE loss: 3.8725247383117676\n",
      "val MSE loss: 3.8629605770111084\n",
      "train D loss: 1.4534785747528076 G loss: 22.559764862060547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val MSE loss: 3.855400800704956\n",
      "val MSE loss: 3.842719793319702\n",
      "val MSE loss: 3.834057569503784\n",
      "train D loss: 1.467455506324768 G loss: 21.595788955688477\n",
      "val MSE loss: 3.8224997520446777\n",
      "val MSE loss: 3.8191096782684326\n",
      "val MSE loss: 3.810004949569702\n",
      "train D loss: 1.4650137424468994 G loss: 21.495201110839844\n",
      "val MSE loss: 3.8028085231781006\n",
      "val MSE loss: 3.7956597805023193\n",
      "val MSE loss: 3.789846897125244\n",
      "train D loss: 1.467280387878418 G loss: 22.873451232910156\n",
      "val MSE loss: 3.7779221534729004\n",
      "val MSE loss: 3.7685065269470215\n",
      "val MSE loss: 3.7675840854644775\n",
      "train D loss: 1.4673879146575928 G loss: 23.399993896484375\n",
      "val MSE loss: 3.761155605316162\n",
      "val MSE loss: 3.7512450218200684\n",
      "val MSE loss: 3.744197368621826\n",
      "train D loss: 1.4431612491607666 G loss: 14.252433776855469\n",
      "epoch: 9\n",
      "val MSE loss: 3.7374799251556396\n",
      "val MSE loss: 3.7296836376190186\n",
      "val MSE loss: 3.718184232711792\n",
      "train D loss: 1.4392657279968262 G loss: 20.26156997680664\n",
      "val MSE loss: 3.7087242603302\n",
      "val MSE loss: 3.696889638900757\n",
      "val MSE loss: 3.6797943115234375\n",
      "train D loss: 1.4484219551086426 G loss: 22.366031646728516\n",
      "val MSE loss: 3.6813340187072754\n",
      "val MSE loss: 3.6715803146362305\n",
      "val MSE loss: 3.6638569831848145\n",
      "train D loss: 1.4524834156036377 G loss: 23.57060432434082\n",
      "val MSE loss: 3.65071964263916\n",
      "val MSE loss: 3.642709970474243\n",
      "val MSE loss: 3.633021116256714\n",
      "train D loss: 1.4779154062271118 G loss: 21.674945831298828\n",
      "val MSE loss: 3.6276767253875732\n",
      "val MSE loss: 3.619774341583252\n",
      "val MSE loss: 3.6085705757141113\n",
      "train D loss: 1.4600850343704224 G loss: 22.08169174194336\n",
      "val MSE loss: 3.5983128547668457\n",
      "val MSE loss: 3.587833881378174\n",
      "val MSE loss: 3.576237916946411\n",
      "train D loss: 1.4600117206573486 G loss: 21.276683807373047\n",
      "val MSE loss: 3.569831609725952\n",
      "val MSE loss: 3.5598788261413574\n",
      "val MSE loss: 3.553133726119995\n",
      "train D loss: 1.4577577114105225 G loss: 22.99849510192871\n",
      "val MSE loss: 3.5460305213928223\n",
      "val MSE loss: 3.5376803874969482\n",
      "val MSE loss: 3.5323987007141113\n",
      "train D loss: 1.460855484008789 G loss: 23.018138885498047\n",
      "val MSE loss: 3.5247135162353516\n",
      "val MSE loss: 3.512655258178711\n",
      "val MSE loss: 3.510468006134033\n",
      "train D loss: 1.4502499103546143 G loss: 21.639312744140625\n",
      "val MSE loss: 3.501595973968506\n",
      "val MSE loss: 3.4911983013153076\n",
      "val MSE loss: 3.4865517616271973\n",
      "train D loss: 1.4745097160339355 G loss: 22.93191146850586\n",
      "val MSE loss: 3.475983142852783\n",
      "val MSE loss: 3.465224504470825\n",
      "val MSE loss: 3.458932876586914\n",
      "train D loss: 1.4474568367004395 G loss: 20.319107055664062\n",
      "val MSE loss: 3.4471256732940674\n",
      "val MSE loss: 3.432509183883667\n",
      "val MSE loss: 3.430861711502075\n",
      "train D loss: 1.4580425024032593 G loss: 19.410934448242188\n",
      "val MSE loss: 3.420888662338257\n",
      "val MSE loss: 3.4138638973236084\n",
      "val MSE loss: 3.4040110111236572\n",
      "train D loss: 1.4650310277938843 G loss: 20.900840759277344\n",
      "val MSE loss: 3.395991563796997\n",
      "val MSE loss: 3.386300563812256\n",
      "val MSE loss: 3.3775200843811035\n",
      "train D loss: 1.4576904773712158 G loss: 19.416545867919922\n",
      "val MSE loss: 3.373236656188965\n",
      "val MSE loss: 3.356491804122925\n",
      "val MSE loss: 3.348051071166992\n",
      "train D loss: 1.4587247371673584 G loss: 21.91849136352539\n",
      "val MSE loss: 3.335155963897705\n",
      "val MSE loss: 3.3285677433013916\n",
      "val MSE loss: 3.315870523452759\n",
      "train D loss: 1.4228155612945557 G loss: 18.462112426757812\n",
      "val MSE loss: 3.310673713684082\n",
      "val MSE loss: 3.303077459335327\n",
      "val MSE loss: 3.2903928756713867\n",
      "train D loss: 1.4415040016174316 G loss: 19.649782180786133\n",
      "val MSE loss: 3.278994083404541\n",
      "val MSE loss: 3.266021728515625\n",
      "val MSE loss: 3.2559447288513184\n",
      "train D loss: 1.4701944589614868 G loss: 18.597204208374023\n",
      "val MSE loss: 3.245147466659546\n",
      "val MSE loss: 3.2351040840148926\n",
      "val MSE loss: 3.2205862998962402\n",
      "train D loss: 1.4454407691955566 G loss: 19.41743278503418\n",
      "val MSE loss: 3.2127792835235596\n",
      "val MSE loss: 3.19962477684021\n",
      "val MSE loss: 3.1898558139801025\n",
      "train D loss: 1.4481713771820068 G loss: 20.42034149169922\n",
      "val MSE loss: 3.1814393997192383\n",
      "val MSE loss: 3.1666975021362305\n",
      "val MSE loss: 3.155609130859375\n",
      "train D loss: 1.4670906066894531 G loss: 20.748367309570312\n",
      "val MSE loss: 3.148500442504883\n",
      "val MSE loss: 3.1369149684906006\n",
      "val MSE loss: 3.1234827041625977\n",
      "train D loss: 1.4652059078216553 G loss: 19.807905197143555\n",
      "val MSE loss: 3.1185858249664307\n",
      "val MSE loss: 3.103919267654419\n",
      "val MSE loss: 3.1018903255462646\n",
      "train D loss: 1.4605021476745605 G loss: 22.002456665039062\n",
      "val MSE loss: 3.0934579372406006\n",
      "val MSE loss: 3.0904624462127686\n",
      "val MSE loss: 3.0823421478271484\n",
      "train D loss: 1.4333148002624512 G loss: 11.087054252624512\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print('epoch:',epoch)\n",
    "    for step , (batch_x,batch_y) in enumerate(loader):\n",
    "        # 训练判别器D，目的是能够区分G的输出和真实标签\n",
    "        for d in range(1):\n",
    "            D.train()\n",
    "            G.eval()\n",
    "            # 前向传播\n",
    "            z=torch.from_numpy(np.random.randn(batch_x.shape[0], 10)).float()   # 随机噪声\n",
    "            d_real = D(batch_y)     #  （标签）输入判别器的结果\n",
    "            batch_y_pred = G(torch.cat([z, batch_x], dim=1))      #  （噪声+x）输入生成器，得到预测结果\n",
    "            d_gen = D(batch_y_pred)        #  （预测结果）输入判别器的结果\n",
    "            # 计算损失\n",
    "            Dloss_real = loss_func_bce(d_real, torch.ones((batch_x.shape[0],1))) # 对于（标签+x）组，判别器输出应趋向于全1\n",
    "            Dloss_gen = loss_func_bce(d_gen, torch.zeros((batch_x.shape[0],1)))  # 对于（预测结果+x）组，判别器输出应趋向于全0\n",
    "            Dloss = Dloss_real + Dloss_gen\n",
    "            # 反向传播（只对判别器参数进行更新）\n",
    "            Dloss.backward()\n",
    "            opt_d.step()\n",
    "            opt_d.zero_grad()\n",
    "            opt_g.zero_grad()\n",
    "        # 训练生成器G，目的是G的输出能够欺骗D，让D以为G的输出就是真实标签\n",
    "        for g in range(3):\n",
    "            D.eval()\n",
    "            G.train()\n",
    "            # 前向传播\n",
    "            z = torch.from_numpy(np.random.randn(batch_x.shape[0], 10)).float() # 随机噪声\n",
    "            batch_y_pred = G(torch.cat([z, batch_x], dim=1))\n",
    "            d_gen = D(batch_y_pred)  \n",
    "            # 计算损失函数\n",
    "            Gloss_adventure = 0.3 * loss_func_bce(d_gen, torch.ones((batch_x.shape[0],1)))   # G的目的是，让D以为它的输出就是真实标签，因此G趋向于让d_gen等于1\n",
    "            Gloss_regression = loss_func_reg(batch_y_pred,batch_y)\n",
    "            r_cty_pred, r_u1_pred, r_u2_pred = compute_resids(batch_x.T, batch_y_pred.T, basis_v, basis_p)\n",
    "            r_cty, r_u1, r_u2 = compute_resids(batch_x.T, batch_y.T, basis_v, basis_p)\n",
    "#             r_cty, r_u1, r_u2 = compute_resids(batch_y, batch_y_pred, basis_v, basis_p, scaler)\n",
    "            Gloss_res = torch.norm(r_cty_pred-r_cty)+torch.norm(r_u1_pred-r_u1)+torch.norm(r_u2_pred-r_u2)\n",
    "            Gloss = Gloss_regression + Gloss_adventure + lambda_res*Gloss_res\n",
    "            # 反向传播\n",
    "            Gloss.backward()\n",
    "            opt_g.step()\n",
    "            opt_g.zero_grad()\n",
    "            opt_d.zero_grad()\n",
    "            loss_train.append(Gloss_regression.item())\n",
    "            valid(G,x_test,loss_func_reg)\n",
    "        D.eval()\n",
    "        G.eval()\n",
    "        print('train D loss:', Dloss.detach().item(), 'G loss:', Gloss.detach().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ce2b730f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAN_with_noise——测试集均方误差： 3.0823421478271484\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [i for i in range(len(loss_train))]    # 每一轮G都训练了3次，所以乘3\n",
    "plt.plot(x,loss_train, label='train')\n",
    "plt.plot(x,loss_val, label='val')\n",
    "plt.title('GAN_with_noise')\n",
    "plt.legend()\n",
    "plt.savefig(fname=\"results/GAN_with_noise.png\")\n",
    "np.save('results/GAN_with_noise.npy',loss_train)   # 保存为npy文件,供不同方法对比\n",
    "print('GAN_with_noise——测试集均方误差：',loss_val[-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a608e94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.eval()\n",
    "z = torch.from_numpy(np.random.randn(data.shape[0], 10)).float() # 随机噪声\n",
    "coeff_pred = G(torch.cat([z, data], dim=1))\n",
    "np.savetxt('./results/prediction_res_GAN.csv', coeff_pred.detach().numpy(), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc08c55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
